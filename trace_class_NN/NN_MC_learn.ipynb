{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import gym\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "from scipy.integrate import quad\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import geom\n",
    "from torch import nn\n",
    "from copy import deepcopy as dc\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['text.usetex'] = True\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "torch.autograd.set_detect_anomaly(False) #makes code very slow but one can find errors in backward()\n",
    "\n",
    "N_a = 3                             # number of possible actions (actions are -1, 0, and 1)\n",
    "sigma = 0.1                         # noise\n",
    "dim = 2                             # dimensionality of the space (v: R^d --> R)\n",
    "obs_min = np.array([-1.2,-0.07])\n",
    "obs_max = np.array([ 0.6, 0.07])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "initialise NN\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Hyperparameters for our network\n",
    "input_size = dim\n",
    "output_size = 1\n",
    "n_layer = 3\n",
    "\n",
    "# hidden_sizes = [32,32,32]\n",
    "\n",
    "# # Store hyperparameters in string\n",
    "# hyps = str(hidden_sizes[0])\n",
    "# for i in range(1,n_layer):\n",
    "#     hyps = hyps+'_'+str(hidden_sizes[i])\n",
    "\n",
    "# # Build a feed-forward network\n",
    "# model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "#                       nn.Tanh(),\n",
    "#                       nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "#                       nn.Tanh(),\n",
    "#                       nn.Linear(hidden_sizes[1], hidden_sizes[2]),\n",
    "#                       nn.Tanh(),\n",
    "#                       nn.Linear(hidden_sizes[-1], output_size))\n",
    "\n",
    "\n",
    "# # Get weights\n",
    "# weights = []\n",
    "# biases = []\n",
    "    \n",
    "# for l in range(n_layer+1):\n",
    "#     weights.append(np.zeros(model[2*l].weight.shape))\n",
    "#     biases.append(np.zeros(model[2*l].bias.shape))\n",
    "\n",
    "# # Set all weights to 0\n",
    "# for l in range(n_layer+1):\n",
    "#     model[2*l].weight = torch.nn.Parameter(torch.from_numpy(0*weights[l]).float(), requires_grad=True)\n",
    "#     model[2*l].bias = torch.nn.Parameter(torch.from_numpy(0*biases[l]).float(), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "FUNCTIONS - PRIOR\n",
    "\n",
    "\"\"\" \n",
    "\n",
    "\n",
    "''' Set sigma_w and sigma_b uniformly '''    \n",
    "alpha = 1.5\n",
    "sigma_w_sq = np.ones(n_layer+1)*2\n",
    "sigma_b_sq = np.ones(n_layer+1)*2\n",
    "    \n",
    "    \n",
    "    \n",
    "# ''' define scaling/covariance matrix '''\n",
    "# C = [[],[]]\n",
    "# C_root = [[],[]]\n",
    "# C_arr = np.ones(model[0].weight.shape)\n",
    "# for t in range(hidden_sizes[0]):\n",
    "#     for s in range(input_size):\n",
    "#         C_arr[t][s] = sigma_w_sq[0]/np.power(t+1,alpha)\n",
    "# C[0].append(C_arr)\n",
    "# C_root[0].append(C_arr**(1/2))\n",
    "\n",
    "# for l in range(1,n_layer):\n",
    "#     C_arr = np.ones(model[2*l].weight.shape)\n",
    "#     for t in range(hidden_sizes[l]):\n",
    "#         for s in range(hidden_sizes[l-1]):\n",
    "#             C_arr[t][s] = sigma_w_sq[l]/np.power(s+1,alpha)/np.power(t+1,alpha)\n",
    "#     C[0].append(C_arr)\n",
    "#     C_root[0].append(C_arr**(1/2))\n",
    "    \n",
    "# C_arr = np.ones(model[2*n_layer].weight.shape)   \n",
    "# for t in range(output_size):\n",
    "#     for s in range(hidden_sizes[n_layer-1]):\n",
    "#         C_arr[t][s] = sigma_w_sq[n_layer]/np.power(s+1,alpha)\n",
    "# C[0].append(C_arr)   \n",
    "# C_root[0].append(C_arr**(1/2))\n",
    "\n",
    "# C_arr = np.ones(model[0].bias.shape)\n",
    "# for t in range(hidden_sizes[0]):\n",
    "#     C_arr[t] = sigma_b_sq[0]/np.power(t+1,alpha)\n",
    "# C[1].append(C_arr)\n",
    "# C_root[1].append(C_arr**(1/2))\n",
    "\n",
    "# for l in range(1,n_layer):\n",
    "#     C_arr = np.ones(model[2*l].bias.shape)\n",
    "#     for t in range(hidden_sizes[l]):\n",
    "#         C_arr[t] = sigma_b_sq[l]/np.power(t+1,alpha)\n",
    "#     C[1].append(C_arr)\n",
    "#     C_root[1].append(C_arr**(1/2))\n",
    "\n",
    "# C_arr = np.ones(model[2*n_layer].bias.shape)\n",
    "# for t in range(output_size):\n",
    "#     C_arr[t] = sigma_b_sq[n_layer]/np.power(t+1,alpha)\n",
    "# C[1].append(C_arr)\n",
    "# C_root[1].append(C_arr**(1/2))\n",
    "\n",
    "\n",
    "''' prior samples are represented by array of coefficients '''\n",
    "def sample_prior(c=1):\n",
    "    w = []\n",
    "    b = []\n",
    "    \n",
    "    ''' weights '''\n",
    "    w_layer = np.random.randn(model[0].weight.shape[0],model[0].weight.shape[1])*C_root[0][0]\n",
    "    w.append(c*w_layer)\n",
    "    \n",
    "    for l in range(1,n_layer):\n",
    "        w_layer = np.random.randn(model[2*l].weight.shape[0],model[2*l].weight.shape[1])*C_root[0][l]\n",
    "        w.append(c*w_layer)\n",
    "     \n",
    "    w_layer = np.random.randn(model[2*n_layer].weight.shape[0],model[2*n_layer].weight.shape[1])*C_root[0][-1]\n",
    "    w.append(c*w_layer)\n",
    "                \n",
    "    ''' biases '''\n",
    "    b_layer = np.random.randn(model[0].bias.shape[0])*C_root[1][0]\n",
    "    b.append(c*b_layer)\n",
    "    \n",
    "    for l in range(1,n_layer):\n",
    "        b_layer = np.random.randn(model[2*l].bias.shape[0])*C_root[1][l]\n",
    "        b.append(c*b_layer)\n",
    "     \n",
    "    b_layer = np.random.randn(model[2*n_layer].bias.shape[0])*C_root[1][-1]\n",
    "    b.append(c*b_layer)\n",
    "    \n",
    "    return [w,b]\n",
    "\n",
    "\n",
    "''' evaluate the log_prior up to a constant '''\n",
    "def logprior(xi):\n",
    "    log_prior = 0\n",
    "    w = xi[0]\n",
    "    b = xi[1]\n",
    "    \n",
    "    ''' weights '''\n",
    "    for l in range(n_layer+1):\n",
    "        log_prior += -0.5*np.sum(w[l]**2/C[0][l])\n",
    "        \n",
    "    ''' biases '''\n",
    "    for l in range(n_layer+1):\n",
    "        log_prior += -0.5*np.sum(b[l]**2/C[1][l])\n",
    "        \n",
    "    return log_prior\n",
    "\n",
    "\n",
    "def prior_proposal(xi):\n",
    "    log_prior_xi = logprior(xi)\n",
    "    \n",
    "    w = xi[0]\n",
    "    b = xi[1]\n",
    "    \n",
    "    ''' Firstly make a cpoy of the current state '''\n",
    "    w_proposal = []\n",
    "    b_proposal = []\n",
    "    for l in range(n_layer+1):\n",
    "        w_proposal.append(dc(w[l]))\n",
    "        b_proposal.append(dc(b[l]))\n",
    "    \n",
    "    random_l = np.random.randint(n_layer)\n",
    "    \n",
    "    ''' Sample index of function to be swapped '''\n",
    "    random_i = geom.rvs(1/alpha)\n",
    "    while random_i>=model[2*random_l].weight.shape[0]-1:  # -1???\n",
    "        random_i = geom.rvs(1/alpha)\n",
    "        \n",
    "    ''' swap f^l_i with f^l_{i+1}'''\n",
    "    w_proposal[random_l][random_i,:] = w[random_l][random_i+1,:]\n",
    "    w_proposal[random_l][random_i+1,:] = w[random_l][random_i,:]\n",
    "    w_proposal[random_l+1][:,random_i] = w[random_l+1][:,random_i+1]\n",
    "    w_proposal[random_l+1][:,random_i+1] = w[random_l+1][:,random_i]\n",
    "    b_proposal[random_l][random_i] = b[random_l][random_i+1]\n",
    "    b_proposal[random_l][random_i+1] = b[random_l][random_i]\n",
    "    \n",
    "    xi_proposal = [w_proposal,b_proposal]\n",
    "    log_prior_proposal = logprior(xi_proposal)\n",
    "    \n",
    "    a = np.exp(log_prior_proposal-log_prior_xi)\n",
    "    if np.random.uniform() < a:\n",
    "        counts[0] += 1\n",
    "        return xi_proposal\n",
    "    else:\n",
    "        counts[1] += 1\n",
    "        return xi\n",
    "    \n",
    "counts = np.zeros(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "FUNCTIONS - LIKELIHOOD \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "''' Function to integrate within likelihood '''\n",
    "def f(x,v_a):\n",
    "    value = norm._pdf((x-v_a[0])/sigma)/sigma\n",
    "    for i in range(1,N_a):\n",
    "        value = value*norm._cdf((x-v_a[i])/sigma)\n",
    "    return value\n",
    "\n",
    "def likelihood(pair):\n",
    "    ''' Check where agent's action would take us '''\n",
    "    pos_curr = pair[0]\n",
    "    speed_curr = pair[1]\n",
    "    action = int(pair[2])\n",
    "    \n",
    "    data_state = pair[0:dim]\n",
    "    x = np.zeros((N_a,dim))\n",
    "    v = np.zeros(N_a)\n",
    "    for j in range(N_a):\n",
    "        env.state = data_state\n",
    "        x[j,:] = env.step(j)[0]\n",
    "        v[j] = u(x[j,:]).detach().numpy()[0]\n",
    "        \n",
    "    # sort v such that the first entry is the taken action\n",
    "    v_a = np.zeros(N_a)\n",
    "    if action!=0:\n",
    "        v_a[0] = v[action]\n",
    "        v_a[1:action+1] = v[0:action]\n",
    "        v_a[action+1:] = v[action+1:]\n",
    "    else:\n",
    "        v_a = v\n",
    "    \n",
    "    ''' Integrate over pdf of chosen action and cdf of other actions '''\n",
    "    lklhd = quad(f,v_a[0]-3*sigma,v_a[0]+3*sigma,args=v_a,limit=200)[0]\n",
    "    return lklhd\n",
    "\n",
    "def loglikelihood(data):\n",
    "    loglikelihood = 0\n",
    "    for j in range(data.shape[0]):\n",
    "        lh = likelihood(data[j,:])\n",
    "        loglikelihood += np.log(lh)\n",
    "    return loglikelihood\n",
    "\n",
    "''' function which is integrated in likelihood gradient '''\n",
    "def f_grad(t,args):\n",
    "    v_a = args[0]\n",
    "    j = args[1]\n",
    "    if j==0:\n",
    "        value = (t-v_a[0])/(sigma**2)*norm._pdf((t-v_a[0])/sigma)/sigma\n",
    "    else:\n",
    "        value = -norm._pdf((v_a[0]-v_a[j])/(np.sqrt(2)*sigma))/(np.sqrt(2)*sigma)*norm._pdf((t-(v_a[0]+v_a[j])/2)/(sigma/np.sqrt(2)))/(sigma/np.sqrt(2))\n",
    "    for i in range(1,N_a):\n",
    "        if i!=j:\n",
    "            value = value*norm._cdf((t-v_a[i])/sigma)\n",
    "    return value\n",
    "\n",
    "''' partial derivative dl/dv '''\n",
    "def grad_ll(v_a,j):\n",
    "    if j==0:\n",
    "        return quad(f_grad,v_a[0]-3*sigma,v_a[0]+3*sigma,args=[v_a,j],limit=200)[0]\n",
    "    else:\n",
    "        return quad(f_grad,(v_a[0]+v_a[j])/2-3*(sigma/np.sqrt(2)),(v_a[0]+v_a[j])/2+3*(sigma/np.sqrt(2)),args=[v_a,j],limit=200)[0]\n",
    "\n",
    "''' partial derivative dl/dw, dl/db'''\n",
    "def diff_ll(data):\n",
    "    diff_w = []\n",
    "    diff_b = []\n",
    "        \n",
    "    l(x)=l(f(x))    \n",
    "    f ist das nn\n",
    "    l ist die loss-function\n",
    "    dl/dx=dl/df*df/dx\n",
    "    \n",
    "    l ist das nn\n",
    "    K ist das gewicht\n",
    "    t ist der Zeitparam.\n",
    "    u ist das Bild\n",
    "    dl/dt = dl/dK*dK/dt\n",
    "    \n",
    "    zerograd\n",
    "    out = model(u_1)\n",
    "    out.backward()\n",
    "    A = model[2*l].weight.grad.data.clone().detach().numpy()\n",
    "    out = model(u_2)\n",
    "    out.backward()\n",
    "    B = model[2*l].weight.grad.data.clone().detach().numpy()\n",
    "    B==A\n",
    "    \n",
    "    loss = L^2(out_t+out_xx)\n",
    "    dloss/dbias to optimise bias\n",
    "    \n",
    "    zerograd\n",
    "    optimiere irgendwas\n",
    "    berechne wieder dl/dK\n",
    "    \n",
    "        \n",
    "    x = np.zeros((N_a,dim))\n",
    "    v = np.zeros(N_a)\n",
    "    x_a = np.zeros((N_a,dim))\n",
    "    v_a = np.zeros(N_a)\n",
    "    grad_a = np.zeros(N_a)\n",
    "    data_state = np.zeros(dim)\n",
    "    \n",
    "    ''' Iterate through all or a subset of the data points, and compute the respective gradients '''\n",
    "    if stochastic_gradients and unadjusted:\n",
    "        range_i = random.sample(range(data.shape[0]),10)\n",
    "    else:\n",
    "        range_i = range(data.shape[0])\n",
    "    for i in range_i:\n",
    "        lh = likelihood(data[i,:])\n",
    "        data_state = data[i,0:dim]\n",
    "        data_action = int(data[i,-1])\n",
    "        \n",
    "        ''' compute locations the actions would take us to and the values of the value function at those points '''\n",
    "        for j in range(N_a):\n",
    "            env.state = data_state\n",
    "            x[j,:] = env.step(j)[0]\n",
    "            v[j] = u(x[j,:]).detach().numpy()[0]\n",
    "        \n",
    "        ''' sort v and x such that the first entry is the taken action '''\n",
    "        if data_action!=0:\n",
    "            v_a[0] = v[data_action]\n",
    "            v_a[1:data_action+1] = v[0:data_action]\n",
    "            v_a[data_action+1:] = v[data_action+1:]\n",
    "            x_a[0,:] = x[data_action,:]\n",
    "            x_a[1:data_action+1,:] = x[0:data_action,:]\n",
    "            x_a[data_action+1:,:] = x[data_action+1:,:]\n",
    "        else:\n",
    "            v_a = v\n",
    "            x_a = x\n",
    "            \n",
    "        ''' Calculate gradient at the v_i the action i would give us '''\n",
    "        for j in range(N_a):\n",
    "            grad_a[j] = grad_ll(v_a,j)/lh\n",
    "        mean_grad = np.mean(grad_a)\n",
    "        grad_a -= mean_grad\n",
    "            \n",
    "        for j in range(N_a):\n",
    "            out = u(x_a[j,:])\n",
    "            out.backward(torch.from_numpy(np.array([grad_a[j]])).float())\n",
    "\n",
    "    for l in range(n_layer+1):\n",
    "        diff_w.append(model[2*l].weight.grad.data.clone().detach().numpy())\n",
    "        diff_b.append(model[2*l].bias.grad.data.clone().detach().numpy())\n",
    "        \n",
    "    return [diff_w, diff_b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "FUNCTIONS - VALUE FUNCTION AND POLICIES\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "''' u(x), which evaluates the function u at x=(pos,speed) with the currently set xi '''\n",
    "def u(x):\n",
    "    x_transformed = np.zeros(dim)\n",
    "    for i in range(dim):\n",
    "        x_transformed[i] = ((x[i]-obs_min[i])/(obs_max[i]-obs_min[i])-1/2)*2\n",
    "        \n",
    "    value = model(torch.from_numpy(x_transformed).float())\n",
    "    return value\n",
    "\n",
    "''' Policy from \"Reinforcement Learning: Theory and {Python} Implementation\" '''\n",
    "def policy(position,velocity):\n",
    "        lb = min(-0.09 * (position + 0.25) ** 2 + 0.03,\n",
    "                0.3 * (position + 0.9) ** 4 - 0.008)\n",
    "        ub = -0.07 * (position + 0.38) ** 2 + 0.07\n",
    "        if lb < velocity < ub:\n",
    "            action = 2 # push right\n",
    "        else:\n",
    "            action = 0 # push left\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "FUNCTIONS - ANALYTICS\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "''' Progress bar to know how much longer one has to wait '''\n",
    "def progressBar(t,value, t_max, acceptances, bar_length=40):\n",
    "    percent = float(t) / t_max\n",
    "    arrow = '-' * int(round(percent * bar_length)-1) + '>'\n",
    "    spaces = ' ' * (bar_length - len(arrow))\n",
    "    sys.stdout.write(\"\\rIteration: {0}    Acceptance ratio: {1}    Percent: [{2}] {3}%  \".format(value,round(acceptances/value,3),arrow + spaces, int(round(percent * 100))))\n",
    "    sys.stdout.flush()    \n",
    "        \n",
    "''' Plotting a value function '''    \n",
    "def func_plot(xi,name):\n",
    "    x = np.arange(-1.2,0.6,0.02)\n",
    "    y = np.arange(-0.07,0.07,0.002)\n",
    "    X,Y = np.meshgrid(x,y)\n",
    "    Z = np.zeros(X.shape)\n",
    "    \n",
    "    for l in range(n_layer+1):\n",
    "        model[2*l].weight = torch.nn.Parameter(torch.from_numpy(xi[0][l]).float(), requires_grad=False)\n",
    "        model[2*l].bias = torch.nn.Parameter(torch.from_numpy(xi[1][l]).float(), requires_grad=False)\n",
    "        \n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(X.shape[1]):\n",
    "            Z[i,j] = u((X[i,j],Y[i,j]))[0]\n",
    "            \n",
    "    fig = plt.figure()\n",
    "    ax = fig.gca(projection='3d')\n",
    "    surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=cm.RdBu,linewidth=0, antialiased=False)\n",
    "    ax.zaxis.set_major_locator(LinearLocator(10))\n",
    "    ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n",
    "    ax.set_xlabel('x-axis')\n",
    "    ax.set_ylabel('y-axis')\n",
    "    ax.set_zlabel('z-axis')\n",
    "    ax.view_init(elev=25, azim=-120)\n",
    "    fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "    fig.savefig(name + '.png', bbox_inches='tight',format='png')\n",
    "    plt.close(fig)\n",
    "    \n",
    "''' Plot a trajectory '''  \n",
    "def trajectory_plot(xi,name):\n",
    "    x = np.arange(len(xi))\n",
    "    fig = plt.figure()\n",
    "    plt.plot(x,xi)\n",
    "    fig.savefig(name + '.png', bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "''' Compute the autocorrelations '''    \n",
    "def autocorr(x,lags):\n",
    "    mean=np.mean(x)\n",
    "    var=np.var(x)\n",
    "    xp=x-mean\n",
    "    corr=[1. if l==0 else np.sum(xp[l:]*xp[:-l])/len(x)/var for l in lags]\n",
    "    return np.array(corr)\n",
    "\n",
    "''' Calculate the Effective Sample Size, assumes algorithm already burned in '''\n",
    "def ESS(logposterior,name):\n",
    "    fig, ax = plt.subplots()\n",
    "    N = len(logposterior)\n",
    "    ax.stem(autocorr(logposterior, range(int(N*0.1))),use_line_collection=True) \n",
    "    ESS = N/(1+2*sum(autocorr(logposterior, range(int(N*0.1)))))\n",
    "    print('Effective Sample Size:', round(ESS))\n",
    "    print('Samples required to generate 1 independent sample:', round(N/ESS,2))\n",
    "    fig.savefig(name + '.png', bbox_inches='tight')\n",
    "    plt.close(fig)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Uncertainty Quantification Initialisation\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "x_test = np.load('MC_x_test.npy')\n",
    "a_test = np.asarray(np.load('MC_a_test.npy'),dtype=int)\n",
    "    \n",
    "v_test = [[],[],[],[],[]]\n",
    "for j in range(5):\n",
    "    for i in range(N_a):\n",
    "        v_test[j].append([])\n",
    "\n",
    "def test_value_fn():\n",
    "    for j in range(5):\n",
    "        ''' Evaluate value function at test points '''\n",
    "        for i in range(N_a):\n",
    "            v = u(x_test[i,:,j]).detach().numpy()[0]\n",
    "            v_test[j][i].append(v)\n",
    "        ''' substract value at optimal test point for normalisation purposes'''\n",
    "        for i in range(N_a):\n",
    "            if i!=a_test[j]:\n",
    "                v_test[j][i][-1]=v_test[j][i][-1]-v_test[j][a_test[j]][-1]\n",
    "        v_test[j][a_test[j]][-1] = 0\n",
    "        \n",
    "def boxplot_value_fn():\n",
    "    global v_test\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title('UQ of relative value function evaluation')\n",
    "    ax.boxplot(v_test[0], positions = [0,1,2])\n",
    "    ax.boxplot(v_test[1], positions = [4,5,6])\n",
    "    ax.boxplot(v_test[2], positions = [8,9,10])\n",
    "    ax.boxplot(v_test[3], positions = [12,13,14])\n",
    "    ax.boxplot(v_test[4], positions = [16,17,18])\n",
    "    ax.set_xticklabels(['L','0','R','L','0','R','L','0','R','L','0','R','L','0','R'])\n",
    "\n",
    "    fig.savefig('figs/MC/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_UQ.pdf', dpi=300)\n",
    "    plt.close(fig) \n",
    "    \n",
    "    v_test = [[],[],[],[],[]]\n",
    "    for j in range(5):\n",
    "        for i in range(N_a):\n",
    "            v_test[j].append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "FUNCTIONS - MCMC (pCN/pCNL)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def acceptance_prop(xi_u, xi_v,data,ll_u,diff_u=False):\n",
    "    accept_prop = -ll_u\n",
    "    ll_v = 0\n",
    "    \n",
    "    ''' both pCN and pCNL '''\n",
    "    for l in range(n_layer+1):\n",
    "        model[2*l].weight = torch.nn.Parameter(torch.from_numpy(xi_v[0][l]).float(), requires_grad=True)\n",
    "        model[2*l].bias = torch.nn.Parameter(torch.from_numpy(xi_v[1][l]).float(), requires_grad=True)\n",
    "        \n",
    "    for j in range(data.shape[0]):\n",
    "        lh = likelihood(data[j,:])\n",
    "        ll_v += np.log(lh)\n",
    "    accept_prop += ll_v\n",
    "    \n",
    "    ''' unadjusted? '''\n",
    "    if unadjusted:\n",
    "        diff_v = diff_ll(data)\n",
    "        return 1,ll_v,diff_v\n",
    "    \n",
    "    elif method=='pCNL':\n",
    "        diff_v = diff_ll(data)\n",
    "        for l in range(n_layer+1):\n",
    "            accept_prop += -np.sum((xi_v[0][l]-xi_u[0][l])*diff_u[0][l])/2 - delta*np.sum((xi_u[0][l]+xi_v[0][l])*diff_u[0][l])/4 + delta*np.linalg.norm(diff_u[0][l]*C_root[0][l])**2/4\n",
    "            accept_prop += -np.sum((xi_v[1][l]-xi_u[1][l])*diff_u[1][l])/2 - delta*np.sum((xi_u[1][l]+xi_v[1][l])*diff_u[1][l])/4 + delta*np.linalg.norm(diff_u[1][l]*C_root[1][l])**2/4\n",
    "\n",
    "            accept_prop -= -np.sum((xi_u[0][l]-xi_v[0][l])*diff_v[0][l])/2 - delta*np.sum((xi_v[0][l]+xi_u[0][l])*diff_v[0][l])/4 + delta*np.linalg.norm(diff_v[0][l]*C_root[0][l])**2/4  \n",
    "            accept_prop -= -np.sum((xi_u[1][l]-xi_v[1][l])*diff_v[1][l])/2 - delta*np.sum((xi_v[1][l]+xi_u[1][l])*diff_v[1][l])/4 + delta*np.linalg.norm(diff_v[1][l]*C_root[1][l])**2/4      \n",
    "        return min(1, np.exp(accept_prop)),ll_v,diff_v\n",
    "#     elif method=='CNL':\n",
    "#         diff_v = diff_ll(data)\n",
    "#         for l in range(n_layer+1):\n",
    "#             accept_prop += -np.sum((xi_v[0][l]-xi_u[0][l])*diff_u[0][l])/2 - delta*np.sum((xi_u[0][l]+xi_v[0][l])*diff_u[0][l]/C[0][l])/4 + delta*np.linalg.norm(diff_u[0][l])**2/4\n",
    "#             accept_prop += -np.sum((xi_v[1][l]-xi_u[1][l])*diff_u[1][l])/2 - delta*np.sum((xi_u[1][l]+xi_v[1][l])*diff_u[1][l]/C[1][l])/4 + delta*np.linalg.norm(diff_u[1][l])**2/4\n",
    "\n",
    "#             accept_prop -= -np.sum((xi_u[0][l]-xi_v[0][l])*diff_v[0][l])/2 - delta*np.sum((xi_v[0][l]+xi_u[0][l])*diff_v[0][l]/C[0][l])/4 + delta*np.linalg.norm(diff_v[0][l])**2/4  \n",
    "#             accept_prop -= -np.sum((xi_u[1][l]-xi_v[1][l])*diff_v[1][l])/2 - delta*np.sum((xi_v[1][l]+xi_u[1][l])*diff_v[1][l]/C[1][l])/4 + delta*np.linalg.norm(diff_v[1][l])**2/4      \n",
    "#         return min(1, np.exp(accept_prop)),ll_v,diff_v\n",
    "    else:\n",
    "        return min(1, np.exp(accept_prop)),ll_v\n",
    "    \n",
    "def propose(xi,diff=False):\n",
    "    w = xi[0]\n",
    "    b = xi[1]\n",
    "    \n",
    "    noise = sample_prior()\n",
    "    w_noise = noise[0]\n",
    "    b_noise = noise[1]\n",
    "    \n",
    "    w_proposal = []\n",
    "    b_proposal = []\n",
    "    for l in range(n_layer+1):\n",
    "        w_proposal.append(np.zeros(model[2*l].weight.shape))\n",
    "        b_proposal.append(np.zeros(model[2*l].bias.shape))\n",
    "    \n",
    "    if method=='pCNL':\n",
    "        diff_w = diff[0]\n",
    "        diff_b = diff[1]\n",
    "        for l in range(n_layer+1):\n",
    "            w_proposal[l] = ((2-delta)*w[l] + 2*delta*C[0][l]*diff_w[l] + np.sqrt(8*delta)*w_noise[l])/(2+delta)\n",
    "            b_proposal[l] = ((2-delta)*b[l] + 2*delta*C[1][l]*diff_b[l] + np.sqrt(8*delta)*b_noise[l])/(2+delta)\n",
    "#     elif method=='CNL':\n",
    "#         diff_w = diff[0]\n",
    "#         diff_b = diff[1]\n",
    "#         for l in range(n_layer+1):\n",
    "#             w_proposal[l] = ((2*C[0][l]-delta)*w[l] + 2*delta*C[0][l]*diff_w[l] + np.sqrt(8*delta*C[0][l])*w_noise[l])/(2*C[0][l]+delta)\n",
    "#             b_proposal[l] = ((2*C[1][l]-delta)*b[l] + 2*delta*C[1][l]*diff_b[l] + np.sqrt(8*delta*C[1][l])*b_noise[l])/(2*C[1][l]+delta)\n",
    "    else:\n",
    "        for l in range(n_layer+1):\n",
    "            w_proposal[l] = np.sqrt(1-beta*beta)*w[l]+beta*w_noise[l]\n",
    "            b_proposal[l] = np.sqrt(1-beta*beta)*b[l]+beta*b_noise[l]\n",
    "    return [w_proposal,b_proposal]\n",
    "\n",
    "def MCMC(xi,N_data,data,max_time):   \n",
    "    print('\\nMCMC algorithm ('+method + ', N_data=' + str(N_data) + ', ' + str(max_time) + ' seconds) was started: ' + str(time.ctime()))\n",
    "      \n",
    "    acc_ratio = 0\n",
    "    logposterior = []\n",
    "    logp = []\n",
    "    logl = []\n",
    "    \n",
    "    ''' Set model weights and biases to current iterate '''\n",
    "    for l in range(n_layer+1):\n",
    "        model[2*l].weight = torch.nn.Parameter(torch.from_numpy(xi[0][l]).float(), requires_grad=True)\n",
    "        model[2*l].bias = torch.nn.Parameter(torch.from_numpy(xi[1][l]).float(), requires_grad=True)\n",
    "        \n",
    "    ''' Initialise likelihood and gradient '''    \n",
    "    ll = loglikelihood(data)\n",
    "    print('Initial loglikelihood: ',ll)\n",
    "    if method=='CNL' or method=='pCNL':\n",
    "        diff = diff_ll(data)\n",
    "        func_plot(diff,'figs/MC/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_diff')\n",
    "        \n",
    "    ''' Run MCMC '''\n",
    "    start = time.time() \n",
    "    j = 0\n",
    "    it = 0\n",
    "    while(time.time()-start<max_time):\n",
    "        \n",
    "        ''' Swap functions around to get mode switching '''\n",
    "        for _ in range(hidden_sizes[0]):\n",
    "            xi = prior_proposal(xi)\n",
    "        \n",
    "        ''' Propose and calculate acceptance probability '''\n",
    "        if method=='pCNL' or method=='CNL':\n",
    "            xi_proposal = propose(xi,diff)  \n",
    "            a,ll_proposal,diff_proposal = acceptance_prop(xi,xi_proposal,data,ll,diff)\n",
    "        else:\n",
    "            xi_proposal = propose(xi)  \n",
    "            a,ll_proposal = acceptance_prop(xi,xi_proposal,data,ll)\n",
    "        \n",
    "        ''' Accept or reject proposal '''\n",
    "        uni = np.random.uniform()\n",
    "        if uni < a or unadjusted:\n",
    "            if method=='pCNL' or method=='CNL':\n",
    "                diff = diff_proposal\n",
    "            xi = xi_proposal    \n",
    "            ll = ll_proposal\n",
    "            acc_ratio = acc_ratio + 1\n",
    "\n",
    "        ''' prior, likelihood, and posterior traceplots are appended '''\n",
    "        lp = logprior(xi)\n",
    "        logposterior.append(lp+ll)\n",
    "        logp.append(lp)\n",
    "        logl.append(ll)\n",
    "        \n",
    "        if prior_compare and j%10==0:\n",
    "            ''' store value function evaluations for uncertainty estimates '''\n",
    "            test_value_fn()\n",
    "        elif policy_compare and (time.time()-start)>it*t_max/1000 and it<1000:\n",
    "            ''' store sample for future use '''\n",
    "            for l in range(n_layer+1):\n",
    "                np.save('np_saved/MC/samples_policy_learning/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_w'+str(l)+'_sampleNo'+str(it)+'.npy',xi[0][l])\n",
    "                np.save('np_saved/MC/samples_policy_learning/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_b'+str(l)+'_sampleNo'+str(it)+'.npy',xi[1][l])\n",
    "            it += 1\n",
    "        \n",
    "        if (j+1)%100==0:\n",
    "            progressBar(time.time()-start,j+1,max_time,acc_ratio)\n",
    "        j+=1\n",
    "        \n",
    "    progressBar(max_time,j,max_time,acc_ratio)\n",
    "    \n",
    "    acc_ratio = acc_ratio/(j)\n",
    "    print('\\nMCMC algorithm terminated: ' + str(time.ctime()) + '. \\nRuntime = ' + str(time.time()-start) + '\\nSteps: ' + str(j))\n",
    "    print('Final loglikelihood: ',ll)\n",
    "    print('Acceptance ratio is ',acc_ratio)\n",
    "    if dim_robust_test:\n",
    "        global all_acceptances\n",
    "        all_acceptances.append(acc_ratio)\n",
    "    \n",
    "    trajectory_plot(logposterior[1:],'figs/MC/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_logposterior')\n",
    "    trajectory_plot(logp[1:],'figs/MC/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_logprior')\n",
    "    trajectory_plot(logl[1:],'figs/MC/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_loglikelihood')\n",
    "    if prior_compare:\n",
    "        boxplot_value_fn()\n",
    "    for l in range(n_layer+1):\n",
    "        np.save('np_saved/MC/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_lastSample_w'+str(l)+'.npy',xi[0][l])\n",
    "        np.save('np_saved/MC/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_lastSample_b'+str(l)+'.npy',xi[1][l])\n",
    "    func_plot(xi,'figs/MC/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_lastSample')\n",
    "    \n",
    "    ESS(logposterior,'figs/MC/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_autocorr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN PROGRAMMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "MAIN PROGRAMME 1 - compare NN prior to KL prior (large number of parameters)\n",
    "\n",
    "\"\"\"    \n",
    "\n",
    "prior_compare = True\n",
    "policy_compare = False\n",
    "dim_robust_test = False\n",
    "\n",
    "# set maximal runtime\n",
    "t_max = 3600*10\n",
    "\n",
    "''' Initialise network network, see second block for detailed comments '''\n",
    "input_size = dim\n",
    "output_size = 1\n",
    "n_layer = 3\n",
    "hidden_sizes = [100,100,100]\n",
    "hyps = str(hidden_sizes[0])\n",
    "for i in range(1,n_layer):\n",
    "    hyps = hyps+'_'+str(hidden_sizes[i])\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.Tanh(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.Tanh(),\n",
    "                      nn.Linear(hidden_sizes[1], hidden_sizes[2]),\n",
    "                      nn.Tanh(),\n",
    "                      nn.Linear(hidden_sizes[-1], output_size))\n",
    "weights = []\n",
    "biases = []\n",
    "for l in range(n_layer+1):\n",
    "    weights.append(np.zeros(model[2*l].weight.shape))\n",
    "    biases.append(np.zeros(model[2*l].bias.shape))\n",
    "for l in range(n_layer+1):\n",
    "    model[2*l].weight = torch.nn.Parameter(torch.from_numpy(0*weights[l]).float(), requires_grad=False)\n",
    "    model[2*l].bias = torch.nn.Parameter(torch.from_numpy(0*biases[l]).float(), requires_grad=False)\n",
    "''' Initialise covariance operator, see prior block for detailed comments '''\n",
    "C = [[],[]]\n",
    "C_root = [[],[]]\n",
    "C_arr = np.ones(model[0].weight.shape)\n",
    "for t in range(hidden_sizes[0]):\n",
    "    for s in range(input_size):\n",
    "        C_arr[t][s] = sigma_w_sq[0]/np.power(t+1,alpha)\n",
    "C[0].append(C_arr)\n",
    "C_root[0].append(C_arr**(1/2))\n",
    "for l in range(1,n_layer):\n",
    "    C_arr = np.ones(model[2*l].weight.shape)\n",
    "    for t in range(hidden_sizes[l]):\n",
    "        for s in range(hidden_sizes[l-1]):\n",
    "            C_arr[t][s] = sigma_w_sq[l]/np.power(s+1,alpha)/np.power(t+1,alpha)\n",
    "    C[0].append(C_arr)\n",
    "    C_root[0].append(C_arr**(1/2))\n",
    "C_arr = np.ones(model[2*n_layer].weight.shape)   \n",
    "for t in range(output_size):\n",
    "    for s in range(hidden_sizes[n_layer-1]):\n",
    "        C_arr[t][s] = sigma_w_sq[n_layer]/np.power(s+1,alpha)\n",
    "C[0].append(C_arr)   \n",
    "C_root[0].append(C_arr**(1/2))\n",
    "C_arr = np.ones(model[0].bias.shape)\n",
    "for t in range(hidden_sizes[0]):\n",
    "    C_arr[t] = sigma_b_sq[0]/np.power(t+1,alpha)\n",
    "C[1].append(C_arr)\n",
    "C_root[1].append(C_arr**(1/2))\n",
    "for l in range(1,n_layer):\n",
    "    C_arr = np.ones(model[2*l].bias.shape)\n",
    "    for t in range(hidden_sizes[l]):\n",
    "        C_arr[t] = sigma_b_sq[l]/np.power(t+1,alpha)\n",
    "    C[1].append(C_arr)\n",
    "    C_root[1].append(C_arr**(1/2))\n",
    "C_arr = np.ones(model[2*n_layer].bias.shape)\n",
    "for t in range(output_size):\n",
    "    C_arr[t] = sigma_b_sq[n_layer]/np.power(t+1,alpha)\n",
    "C[1].append(C_arr)\n",
    "C_root[1].append(C_arr**(1/2))\n",
    "\n",
    "# Sample from the prior to see what a sample looks like\n",
    "xi = sample_prior()\n",
    "func_plot(xi,'figs/MC/NN_'+hyps+'_a_prior_sample')\n",
    "\n",
    "# Create environment \n",
    "env = gym.make('MountainCar-v0')\n",
    "env = env.unwrapped\n",
    "data = np.load('MC_data.npy')\n",
    "N_data = 50\n",
    "\n",
    "''' run pCN '''\n",
    "method = 'pCN'\n",
    "stochastic_gradients = False\n",
    "unadjusted = False\n",
    "beta =  1/10.5\n",
    "counts = np.zeros(2)\n",
    "try:\n",
    "    xi = [[],[]]\n",
    "    for l in range(n_layer+1):\n",
    "        xi[0].append(np.load('np_saved/MC/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_lastSample_w'+str(l)+'.npy'))\n",
    "        xi[1].append(np.load('np_saved/MC/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_lastSample_b'+str(l)+'.npy'))\n",
    "except FileNotFoundError:\n",
    "    print('Starting from close to 0')\n",
    "    xi = sample_prior(0.1)\n",
    "MCMC(xi,N_data,data[0:N_data,:],t_max) \n",
    "print('Counts: '+str(int(counts[0]))+' accepted prior moves, '+str(int(counts[1]))+' rejected ones.')\n",
    "                                          \n",
    "env.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "MAIN PROGRAMME 2 - LEARN policy, and store samples for future use (small number of parameters)\n",
    "\n",
    "\"\"\"    \n",
    "\n",
    "prior_compare = False\n",
    "policy_compare = True\n",
    "dim_robust_test = False\n",
    "\n",
    "# set maximal runtime\n",
    "t_max = 3600*10\n",
    "\n",
    "''' Initialise network network, see second block for detailed comments '''\n",
    "input_size = dim\n",
    "output_size = 1\n",
    "n_layer = 3\n",
    "hidden_sizes = [10,10,10]\n",
    "hyps = str(hidden_sizes[0])\n",
    "for i in range(1,n_layer):\n",
    "    hyps = hyps+'_'+str(hidden_sizes[i])\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.Tanh(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.Tanh(),\n",
    "                      nn.Linear(hidden_sizes[1], hidden_sizes[2]),\n",
    "                      nn.Tanh(),\n",
    "                      nn.Linear(hidden_sizes[-1], output_size))\n",
    "weights = []\n",
    "biases = []\n",
    "for l in range(n_layer+1):\n",
    "    weights.append(np.zeros(model[2*l].weight.shape))\n",
    "    biases.append(np.zeros(model[2*l].bias.shape))\n",
    "for l in range(n_layer+1):\n",
    "    model[2*l].weight = torch.nn.Parameter(torch.from_numpy(0*weights[l]).float(), requires_grad=False)\n",
    "    model[2*l].bias = torch.nn.Parameter(torch.from_numpy(0*biases[l]).float(), requires_grad=False)\n",
    "''' Initialise covariance operator, see prior block for detailed comments '''\n",
    "C = [[],[]]\n",
    "C_root = [[],[]]\n",
    "C_arr = np.ones(model[0].weight.shape)\n",
    "for t in range(hidden_sizes[0]):\n",
    "    for s in range(input_size):\n",
    "        C_arr[t][s] = sigma_w_sq[0]/np.power(t+1,alpha)\n",
    "C[0].append(C_arr)\n",
    "C_root[0].append(C_arr**(1/2))\n",
    "for l in range(1,n_layer):\n",
    "    C_arr = np.ones(model[2*l].weight.shape)\n",
    "    for t in range(hidden_sizes[l]):\n",
    "        for s in range(hidden_sizes[l-1]):\n",
    "            C_arr[t][s] = sigma_w_sq[l]/np.power(s+1,alpha)/np.power(t+1,alpha)\n",
    "    C[0].append(C_arr)\n",
    "    C_root[0].append(C_arr**(1/2))\n",
    "C_arr = np.ones(model[2*n_layer].weight.shape)   \n",
    "for t in range(output_size):\n",
    "    for s in range(hidden_sizes[n_layer-1]):\n",
    "        C_arr[t][s] = sigma_w_sq[n_layer]/np.power(s+1,alpha)\n",
    "C[0].append(C_arr)   \n",
    "C_root[0].append(C_arr**(1/2))\n",
    "C_arr = np.ones(model[0].bias.shape)\n",
    "for t in range(hidden_sizes[0]):\n",
    "    C_arr[t] = sigma_b_sq[0]/np.power(t+1,alpha)\n",
    "C[1].append(C_arr)\n",
    "C_root[1].append(C_arr**(1/2))\n",
    "for l in range(1,n_layer):\n",
    "    C_arr = np.ones(model[2*l].bias.shape)\n",
    "    for t in range(hidden_sizes[l]):\n",
    "        C_arr[t] = sigma_b_sq[l]/np.power(t+1,alpha)\n",
    "    C[1].append(C_arr)\n",
    "    C_root[1].append(C_arr**(1/2))\n",
    "C_arr = np.ones(model[2*n_layer].bias.shape)\n",
    "for t in range(output_size):\n",
    "    C_arr[t] = sigma_b_sq[n_layer]/np.power(t+1,alpha)\n",
    "C[1].append(C_arr)\n",
    "C_root[1].append(C_arr**(1/2))\n",
    "\n",
    "# Sample from the prior to see what a sample looks like\n",
    "xi = sample_prior()\n",
    "func_plot(xi,'figs/MC/NN_'+hyps+'_a_prior_sample')\n",
    "\n",
    "# Create environment \n",
    "env = gym.make('MountainCar-v0')\n",
    "env = env.unwrapped\n",
    "data = np.load('MC_data.npy')\n",
    "N_data = 50 \n",
    "\n",
    "''' run pCN '''\n",
    "method = 'pCN'\n",
    "stochastic_gradients = False\n",
    "unadjusted = False\n",
    "beta =  1/10.5\n",
    "counts = np.zeros(2)\n",
    "try:\n",
    "    xi = [[],[]]\n",
    "    for l in range(n_layer+1):\n",
    "        xi[0].append(np.load('np_saved/MC/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_lastSample_w'+str(l)+'.npy'))\n",
    "        xi[1].append(np.load('np_saved/MC/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_lastSample_b'+str(l)+'.npy'))\n",
    "except FileNotFoundError:\n",
    "    print('Starting from close to 0')\n",
    "    xi = sample_prior(0.1)\n",
    "MCMC(xi,N_data,data[0:N_data,:],t_max) \n",
    "print('Counts: '+str(int(counts[0]))+' accepted prior moves, '+str(int(counts[1]))+' rejected ones.')\n",
    "\n",
    "''' run pCNL ''' # can be modified to CNL by just replacing the method to 'CNL'\n",
    "method = 'pCNL'\n",
    "stochastic_gradients = False # if true then unadjusted needs to be true too\n",
    "unadjusted = False\n",
    "delta = 1/2500\n",
    "counts = np.zeros(2)\n",
    "try:\n",
    "    xi = [[],[]]\n",
    "    for l in range(n_layer+1):\n",
    "        xi[0].append(np.load('np_saved/MC/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_lastSample_w'+str(l)+'.npy'))\n",
    "        xi[1].append(np.load('np_saved/MC/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_lastSample_b'+str(l)+'.npy'))\n",
    "except FileNotFoundError:\n",
    "    print('\\nStarting from close to 0')\n",
    "    xi = sample_prior(0.1)\n",
    "MCMC(xi,N_data,data[0:N_data,:],t_max) \n",
    "print('Counts: '+str(int(counts[0]))+' accepted prior moves, '+str(int(counts[1]))+' rejected ones.')\n",
    "                                          \n",
    "env.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MCMC algorithm (pCN, N_data=50, 18000 seconds) was started: Wed Jul  8 16:12:05 2020\n",
      "Initial loglikelihood:  -20.72449147401812\n",
      "Iteration: 131100    Acceptance ratio: 0.224    Percent: [------------------>                     ] 48%  "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "MAIN PROGRAMME 3 - check dimension-robustness of the proposed Neural Network prior\n",
    "\n",
    "\"\"\"    \n",
    "\n",
    "prior_compare = False\n",
    "policy_compare = False\n",
    "dim_robust_test = True\n",
    "\n",
    "# set maximal runtime\n",
    "t_max = 3600*5\n",
    "\n",
    "all_acceptances = []\n",
    "\n",
    "for hidden_size in [10,20,30,40,50,60,70,80,90,100]:\n",
    "    ''' Initialise network network, see second block for detailed comments '''\n",
    "    input_size = dim\n",
    "    output_size = 1\n",
    "    n_layer = 3\n",
    "    hidden_sizes = [hidden_size,hidden_size,hidden_size]\n",
    "    hyps = str(hidden_sizes[0])\n",
    "    for i in range(1,n_layer):\n",
    "        hyps = hyps+'_'+str(hidden_sizes[i])\n",
    "    model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                          nn.Tanh(),\n",
    "                          nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                          nn.Tanh(),\n",
    "                          nn.Linear(hidden_sizes[1], hidden_sizes[2]),\n",
    "                          nn.Tanh(),\n",
    "                          nn.Linear(hidden_sizes[-1], output_size))\n",
    "    weights = []\n",
    "    biases = []\n",
    "    for l in range(n_layer+1):\n",
    "        weights.append(np.zeros(model[2*l].weight.shape))\n",
    "        biases.append(np.zeros(model[2*l].bias.shape))\n",
    "    for l in range(n_layer+1):\n",
    "        model[2*l].weight = torch.nn.Parameter(torch.from_numpy(0*weights[l]).float(), requires_grad=False)\n",
    "        model[2*l].bias = torch.nn.Parameter(torch.from_numpy(0*biases[l]).float(), requires_grad=False)\n",
    "    ''' Initialise covariance operator, see prior block for detailed comments '''\n",
    "    C = [[],[]]\n",
    "    C_root = [[],[]]\n",
    "    C_arr = np.ones(model[0].weight.shape)\n",
    "    for t in range(hidden_sizes[0]):\n",
    "        for s in range(input_size):\n",
    "            C_arr[t][s] = sigma_w_sq[0]/np.power(t+1,alpha)\n",
    "    C[0].append(C_arr)\n",
    "    C_root[0].append(C_arr**(1/2))\n",
    "    for l in range(1,n_layer):\n",
    "        C_arr = np.ones(model[2*l].weight.shape)\n",
    "        for t in range(hidden_sizes[l]):\n",
    "            for s in range(hidden_sizes[l-1]):\n",
    "                C_arr[t][s] = sigma_w_sq[l]/np.power(s+1,alpha)/np.power(t+1,alpha)\n",
    "        C[0].append(C_arr)\n",
    "        C_root[0].append(C_arr**(1/2))\n",
    "    C_arr = np.ones(model[2*n_layer].weight.shape)   \n",
    "    for t in range(output_size):\n",
    "        for s in range(hidden_sizes[n_layer-1]):\n",
    "            C_arr[t][s] = sigma_w_sq[n_layer]/np.power(s+1,alpha)\n",
    "    C[0].append(C_arr)   \n",
    "    C_root[0].append(C_arr**(1/2))\n",
    "    C_arr = np.ones(model[0].bias.shape)\n",
    "    for t in range(hidden_sizes[0]):\n",
    "        C_arr[t] = sigma_b_sq[0]/np.power(t+1,alpha)\n",
    "    C[1].append(C_arr)\n",
    "    C_root[1].append(C_arr**(1/2))\n",
    "    for l in range(1,n_layer):\n",
    "        C_arr = np.ones(model[2*l].bias.shape)\n",
    "        for t in range(hidden_sizes[l]):\n",
    "            C_arr[t] = sigma_b_sq[l]/np.power(t+1,alpha)\n",
    "        C[1].append(C_arr)\n",
    "        C_root[1].append(C_arr**(1/2))\n",
    "    C_arr = np.ones(model[2*n_layer].bias.shape)\n",
    "    for t in range(output_size):\n",
    "        C_arr[t] = sigma_b_sq[n_layer]/np.power(t+1,alpha)\n",
    "    C[1].append(C_arr)\n",
    "    C_root[1].append(C_arr**(1/2))\n",
    "\n",
    "    # Sample from the prior to see what a sample looks like\n",
    "    xi = sample_prior()\n",
    "    func_plot(xi,'figs/MC/NN_'+hyps+'_a_prior_sample')\n",
    "\n",
    "    # Create environment \n",
    "    env = gym.make('MountainCar-v0')\n",
    "    env = env.unwrapped\n",
    "\n",
    "    data = np.load('MC_data.npy')\n",
    "\n",
    "    for N_data in [50]: \n",
    "\n",
    "        ''' run pCN '''\n",
    "        method = 'pCN'\n",
    "        stochastic_gradients = False\n",
    "        unadjusted = False\n",
    "        beta =  1/10.5\n",
    "        counts = np.zeros(2)\n",
    "        try:\n",
    "            xi = [[],[]]\n",
    "            for l in range(n_layer+1):\n",
    "                xi[0].append(np.load('np_saved/MC/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_lastSample_w'+str(l)+'.npy'))\n",
    "                xi[1].append(np.load('np_saved/MC/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_lastSample_b'+str(l)+'.npy'))\n",
    "        except FileNotFoundError:\n",
    "            print('\\nStarting from close to 0')\n",
    "            xi = sample_prior(0.1)\n",
    "        MCMC(xi,N_data,data[0:N_data,:],t_max) \n",
    "        print('Counts: '+str(int(counts[0]))+' accepted prior moves, '+str(int(counts[1]))+' rejected ones.')\n",
    "        print('Number of layers: '+str(int(n_layer))+', number of nodes per layer: '+str(int(hidden_size)))\n",
    "\n",
    "    env.close()   \n",
    "\n",
    "np.save('MC_dim_independence_acceptances.npy',all_acceptances)\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "ax.set_title('Acceptance ratio by layer width')\n",
    "ax.plot([10,20,30,40,50,60,70,80,90,100],all_acceptances)\n",
    "\n",
    "fig.savefig('figs/NN_dim_independence.pdf', dpi=300)\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Respective acceptances and total number of parameters for the different layer widths:\n",
      "10:   22.86%     261\n",
      "20:   24.0%     921\n",
      "30:   23.48%     1981\n",
      "40:   22.1%     3441\n",
      "50:   22.19%     5301\n",
      "60:   23.14%     7561\n",
      "70:   23.94%     10221\n",
      "80:   23.37%     13281\n",
      "90:   22.98%     16741\n",
      "100:  23.85%     20601\n"
     ]
    }
   ],
   "source": [
    "all_acceptances = np.load('MC_dim_independence_acceptances.npy')\n",
    "print('\\nRespective acceptances and total number of parameters for the different layer widths:')\n",
    "for N_l in [10,20,30,40,50,60,70,80,90,100]:\n",
    "    number_w = N_l+2*(N_l**2)+2*N_l\n",
    "    number_b = 1+3*N_l\n",
    "    if N_l==100:\n",
    "        print(str(N_l)+':  '+str(round(all_acceptances[int(N_l/10-1)]*100,2))+'%'+'     '+str(number_w+number_b))\n",
    "    else:\n",
    "        print(str(N_l)+':   '+str(round(all_acceptances[int(N_l/10-1)]*100,2))+'%'+'     '+str(number_w+number_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
