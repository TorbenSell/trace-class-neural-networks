{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import time\n",
    "import numba\n",
    "\n",
    "sigma = 0.01                        # noise\n",
    "dim = 2                             # dimensionality of the space (v: R^d --> R)\n",
    "obs_min = np.array([0,0])\n",
    "obs_max = np.array([1,1])\n",
    "\n",
    "no_samples_for_mean = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "FUNCTIONS - LIKELIHOOD \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "m = 21\n",
    "\n",
    "# Define h where it is fixed (when x_2=0 or x_2=1).\n",
    "h_first_row = np.zeros(m)\n",
    "h_last_row = np.zeros(m)\n",
    "for i in range(m): # Points on the bottom, where x_2=0 (h=x_1)\n",
    "    h_first_row[i] = i/(m-1)\n",
    "for i in range(m): # Points on top, where x_2=1 (h=1-x_1)\n",
    "    h_last_row[i] = 1-i/(m-1)\n",
    "    \n",
    "\"\"\"\n",
    "Initialise the finite difference scheme.\n",
    "\"\"\"\n",
    "@numba.njit()\n",
    "def A_init(k):   \n",
    "    A = np.zeros((m**2,m**2))\n",
    "    for j in range(m-2): # central points\n",
    "        for i in range(m-2):\n",
    "            A[(i+1)+m*(j+1),(i+1)+m*(j+1)] = 2*k[(i+1)+m*(j+1)]+k[(i)+m*(j+1)]+k[(i+1)+m*(j)]\n",
    "            A[(i+1)+m*(j+1),(i)+m*(j+1)] = -k[(i)+m*(j+1)]\n",
    "            A[(i+1)+m*(j+1),(i+1)+m*(j)] = -k[(i+1)+m*(j)]\n",
    "            A[(i+1)+m*(j+1),(i+2)+m*(j+1)] = -k[(i+1)+m*(j+1)]\n",
    "            A[(i+1)+m*(j+1),(i+1)+m*(j+2)] = -k[(i+1)+m*(j+1)]\n",
    "    i = m-1 # Points on the right, where x=6 (h_x=0)\n",
    "    for j in range(m-2):\n",
    "        A[i+m*(j+1),i+m*(j+1)] = k[i+m*(j+1)]+k[i+m*(j)]+k[i-1+m*(j+1)]\n",
    "        A[i+m*(j+1),i+m*(j)] = -k[i+m*(j)]\n",
    "        A[i+m*(j+1),i+m*(j+2)] = -k[i+m*(j+1)]\n",
    "        A[i+m*(j+1),i-1+m*(j+1)] = -k[i-1+m*(j+1)]\n",
    "    i = 0 # Points on the left, where x=0 (h_x=0)\n",
    "    for j in range(m-2):\n",
    "        A[i+m*(j+1),i+m*(j+1)] = 2*k[i+m*(j+1)]+k[i+m*(j)]\n",
    "        A[i+m*(j+1),i+m*(j)] = -k[i+m*(j)]\n",
    "        A[i+m*(j+1),(i+1)+m*(j+1)] = -k[i+m*(j+1)]\n",
    "        A[i+m*(j+1),i+m*(j+2)] = -k[i+m*(j+1)]       \n",
    "    # Corner points are already considered in the cases where x_2=0 and x_2=1.'''   \n",
    "    A = 1/((6/(m-1))**2)*A   # Multiply by 1/Delta^2\n",
    "    return A\n",
    "\n",
    "\"\"\"\n",
    "G maps a function u to the output of a PDE, h. It makes use of the finite \n",
    "difference approximation given by A.k and b_k. It's specific for the problem\n",
    "setup.\n",
    "\"\"\"\n",
    "@numba.njit()\n",
    "def G(k):    \n",
    "    A_k = A_init(k)\n",
    "    h = np.zeros(m**2)\n",
    "    b = np.zeros(m**2)\n",
    "    for i in range(m):\n",
    "        b[i+m] -= A_k[i+m,i]*h_first_row[i]\n",
    "        b[i+m*(m-2)] -= A_k[i+m*(m-2),i+m*(m-1)]*h_last_row[i]        \n",
    "        \n",
    "    # As the first and last column are fixed, we only consider a subset of the problem for the linalg solver.\n",
    "    h_sub = np.zeros(m*(m-2))     \n",
    "    A_sub = A_k[:,m:m*(m-1)]\n",
    "    A_sub = A_sub[m:m*(m-1),:]\n",
    "    h_sub = np.linalg.solve(A_sub,b[m:m*(m-1)])\n",
    "\n",
    "    h[m:m*(m-1)] = h_sub\n",
    "    h[0:m] = h_first_row\n",
    "    h[m*(m-1):] = h_last_row    \n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "FUNCTIONS - VALUE FUNCTION AND POLICIES\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "''' Progress bar to know how much longer one has to wait '''\n",
    "def progressBar(value, endvalue, bar_length=40):\n",
    "    percent = float(value) / endvalue\n",
    "    arrow = '-' * int(round(percent * bar_length)-1) + '>'\n",
    "    spaces = ' ' * (bar_length - len(arrow))\n",
    "    sys.stdout.write(\"\\rPercent: [{0}] {1}%\".format(arrow + spaces, int(round(percent * 100))))\n",
    "    sys.stdout.flush() \n",
    "    \n",
    "# ''' u(x), which evaluates the function u at x=(pos,speed) with the currently set xi '''\n",
    "# def u(x,it=0):\n",
    "#     x_transformed = np.zeros(dim)\n",
    "#     for i in range(dim):\n",
    "#         x_transformed[i] = ((x[i]-obs_min[i])/(obs_max[i]-obs_min[i])-1/2)*2\n",
    "        \n",
    "#     value = model_list[it](torch.from_numpy(x_transformed).float())\n",
    "#     return value\n",
    "\n",
    "''' u(x), which evaluates the function u at x '''\n",
    "def u(x,it=0):\n",
    "    x_transformed = np.zeros(dim)\n",
    "    for i in range(dim):\n",
    "        x_transformed[i] = ((x[i]-obs_min[i])/(obs_max[i]-obs_min[i])-1/2)*2\n",
    "        \n",
    "    x_transformed_again = np.zeros(2*dim)\n",
    "    for i in range(dim):\n",
    "        x_transformed_again[i] = x_transformed[i]\n",
    "        x_transformed_again[dim+i] = np.sin(x_transformed[i]*np.pi)\n",
    "        \n",
    "    value = model_list[it](torch.from_numpy(x_transformed_again).float())\n",
    "    return value\n",
    "\n",
    "''' Plotting a mean function exp(u) '''    \n",
    "def mean_func_plot(name):\n",
    "    x = np.arange(0,1,0.01)\n",
    "    y = np.arange(0,1,0.01)\n",
    "    X,Y = np.meshgrid(x,y)\n",
    "    Z = np.zeros(X.shape)\n",
    "    \n",
    "    for it in range(no_samples_for_mean):\n",
    "        progressBar(it,no_samples_for_mean)\n",
    "        for i in range(X.shape[0]):\n",
    "            for j in range(X.shape[1]):\n",
    "                x=np.zeros(dim)\n",
    "                x[0]=X[i,j]\n",
    "                x[1]=Y[i,j]\n",
    "                Z[i,j] += np.exp(u(x,it).detach().numpy()[0])/no_samples_for_mean\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    c = ax.pcolormesh(X, Y, Z, cmap='cool', vmin=Z.min(), vmax=Z.max())\n",
    "    \n",
    "    # set the limits of the plot to the limits of the data\n",
    "    ax.axis([X.min(), X.max(), Y.min(), Y.max()])\n",
    "    fig.colorbar(c, ax=ax)\n",
    "    fig.savefig(name + '.pdf', dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "''' Check the predicitve posterior '''\n",
    "def data_prediction_check(name):\n",
    "    \n",
    "    ''' load data and observation points '''\n",
    "    N_data = 33\n",
    "    data = np.load('GWF_data.npy')\n",
    "    obs_indices = np.load('GWF_obs_indices.npy')\n",
    "    \n",
    "    ''' Inititalise list for predicted data and for the labels '''\n",
    "    y_rep = []\n",
    "    labels = []\n",
    "    for i in range(N_data):\n",
    "        y_rep.append([])\n",
    "        x = (obs_indices[i]%m)/(m-1)\n",
    "        y = ((obs_indices[i]-obs_indices[i]%m)/m)/(m-1)\n",
    "        labels.append(str((x,y)))\n",
    "    \n",
    "    for it in range(no_samples_for_mean):\n",
    "        progressBar(it,no_samples_for_mean)\n",
    "    \n",
    "        k = np.zeros(m**2)\n",
    "        for j in range(m):\n",
    "            for i in range(m):\n",
    "                k[i+m*j] = np.exp(u((i/(m-1),j/(m-1)),it).detach().numpy()[0])\n",
    "        h = G(k)\n",
    "        y_h = h[obs_indices]\n",
    "        \n",
    "        ''' Create data predictions '''\n",
    "        for i in range(N_data):\n",
    "            if noise:\n",
    "                y_rep[i].append(y_h[i]+sigma*np.random.normal())\n",
    "            else:\n",
    "                y_rep[i].append(y_h[i])\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.boxplot(y_rep)\n",
    "    ax.set_xticklabels(labels, rotation=90)\n",
    "\n",
    "    ''' Print true data points '''\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(33):\n",
    "        x.append(i+1)\n",
    "        y.append(data[i])\n",
    "    ax.plot(x, y, \"o\")\n",
    "    \n",
    "    fig.savefig(name + '.pdf', dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN PROGRAMME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'pCN'\n",
    "N_data = 33\n",
    "\n",
    "\"\"\" initialise NNs - pCN \"\"\"\n",
    "\n",
    "model_list = []\n",
    "    \n",
    "# Hyperparameters for our network\n",
    "input_size = 2*dim\n",
    "output_size = 1\n",
    "n_layer = 3\n",
    "hidden_sizes = [10,10,10]\n",
    "\n",
    "# Store hyperparameters in string\n",
    "hyps = str(hidden_sizes[0])\n",
    "for i in range(1,n_layer):\n",
    "    hyps = hyps+'_'+str(hidden_sizes[i])\n",
    "\n",
    "for it in range(no_samples_for_mean):\n",
    "    # Build a feed-forward network\n",
    "    model_list.append(nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.Tanh(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.Tanh(),\n",
    "                      nn.Linear(hidden_sizes[1], hidden_sizes[2]),\n",
    "                      nn.Tanh(),\n",
    "                      nn.Linear(hidden_sizes[-1], output_size)))\n",
    "\n",
    "\n",
    "\n",
    "    # Get weights\n",
    "    weights = []\n",
    "    biases = []\n",
    "    for l in range(n_layer+1):\n",
    "        weights.append(np.load('np_saved/GWF/samples_policy_learning/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_w'+str(l)+'_sampleNo'+str(it)+'.npy'))\n",
    "        biases.append(np.load('np_saved/GWF/samples_policy_learning/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_b'+str(l)+'_sampleNo'+str(it)+'.npy'))\n",
    "        \n",
    "    # Set all weights to 0\n",
    "    for l in range(n_layer+1):\n",
    "        model_list[it][2*l].weight = torch.nn.Parameter(torch.from_numpy(weights[l]).float(), requires_grad=False)\n",
    "        model_list[it][2*l].bias = torch.nn.Parameter(torch.from_numpy(biases[l]).float(), requires_grad=False)\n",
    "\n",
    "noise = True\n",
    "data_prediction_check('figs/GWF/GWF_NN_prediction_check')\n",
    "noise = False\n",
    "data_prediction_check('figs/GWF/GWF_NN_prediction_check_no_noise')\n",
    "\n",
    "mean_func_plot('figs/GWF/GWF_NN_mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
