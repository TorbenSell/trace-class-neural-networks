{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/torbensell/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/torbensell/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/torbensell/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/torbensell/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/torbensell/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/torbensell/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import time, gym, sys, torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from scipy.integrate import quad\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import geom\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import matplotlib.pyplot as plt\n",
    "import homework.hw1.load_policy as load_policy\n",
    "import homework.hw1.tf_util as tf_util\n",
    "import tensorflow as tf\n",
    "from copy import deepcopy as dc\n",
    "\n",
    "sigma = 0.1\n",
    "\n",
    "dim = 17\n",
    "dim_act = 6\n",
    "obs_min = np.array([-0.25,-0.8 ,-0.7 ,-0.7,-0.65,-0.75,-0.95,-0.6 ,-1.5,-3.1,-7.1,-20,-24,-27,-27,-30,-20])\n",
    "obs_max = np.array([ 0.4 , 1.65, 0.95, 0.9, 0.95, 0.95, 1.1 , 0.75, 8  , 3.4, 7  , 19, 25, 22, 25, 32, 26])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "initialise NN\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Hyperparameters for our network\n",
    "input_size = dim\n",
    "n_layer = 3\n",
    "hidden_sizes = [10,10,10]\n",
    "output_size = 1\n",
    "\n",
    "# # Store hyperparameters in string\n",
    "# hyps = str(hidden_sizes[0])\n",
    "# for l in range(1,n_layer):\n",
    "#     hyps = hyps+'_'+str(hidden_sizes[l])\n",
    "\n",
    "# # Build a feed-forward network\n",
    "# model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "#                       nn.Tanh(),\n",
    "#                       nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "#                       nn.Tanh(),\n",
    "#                       nn.Linear(hidden_sizes[1], hidden_sizes[2]),\n",
    "#                       nn.Tanh(),\n",
    "#                       nn.Linear(hidden_sizes[-1], output_size))\n",
    "\n",
    "\n",
    "# # Get weights\n",
    "# weights = []\n",
    "# biases = []\n",
    "    \n",
    "# for l in range(n_layer+1):\n",
    "#     weights.append(np.zeros(model[2*l].weight.shape))\n",
    "#     biases.append(np.zeros(model[2*l].bias.shape))\n",
    "\n",
    "# # Set all weights to 0\n",
    "# for l in range(n_layer+1):\n",
    "#     model[2*l].weight = torch.nn.Parameter(torch.from_numpy(0*weights[l]).float(), requires_grad=True)\n",
    "#     model[2*l].bias = torch.nn.Parameter(torch.from_numpy(0*biases[l]).float(), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "INITIALISE ACTION AND OBSERVATION SPACE\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "actions = []\n",
    "actions_matrix = np.load('HC_actions.npy')\n",
    "N_a = 8\n",
    "for i in range(N_a):\n",
    "    actions.append(actions_matrix[i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "FUNCTIONS - PRIOR\n",
    "\n",
    "\"\"\" \n",
    "\n",
    "    \n",
    "''' Set sigma_w and sigma_b uniformly '''    \n",
    "alpha = 1.5\n",
    "sigma_w_sq = np.ones(n_layer+1)*2\n",
    "sigma_b_sq = np.ones(n_layer+1)*2\n",
    "    \n",
    "    \n",
    "# ''' define scaling/covariance matrix '''\n",
    "# C = [[],[]]\n",
    "# C_root = [[],[]]\n",
    "# C_arr = np.ones(model[0].weight.shape)\n",
    "# for t in range(hidden_sizes[0]):\n",
    "#     for s in range(input_size):\n",
    "#         C_arr[t][s] = sigma_w_sq[0]/np.power(t+1,alpha)\n",
    "# C[0].append(C_arr)\n",
    "# C_root[0].append(C_arr**(1/2))\n",
    "\n",
    "# for l in range(1,n_layer):\n",
    "#     C_arr = np.ones(model[2*l].weight.shape)\n",
    "#     for t in range(hidden_sizes[l]):\n",
    "#         for s in range(hidden_sizes[l-1]):\n",
    "#             C_arr[t][s] = sigma_w_sq[l]/np.power(s+1,alpha)/np.power(t+1,alpha)\n",
    "#     C[0].append(C_arr)\n",
    "#     C_root[0].append(C_arr**(1/2))\n",
    "    \n",
    "# C_arr = np.ones(model[2*n_layer].weight.shape)   \n",
    "# for t in range(output_size):\n",
    "#     for s in range(hidden_sizes[n_layer-1]):\n",
    "#         C_arr[t][s] = sigma_w_sq[n_layer]/np.power(s+1,alpha)\n",
    "# C[0].append(C_arr)   \n",
    "# C_root[0].append(C_arr**(1/2))\n",
    "\n",
    "# C_arr = np.ones(model[0].bias.shape)\n",
    "# for t in range(hidden_sizes[0]):\n",
    "#     C_arr[t] = sigma_b_sq[0]/np.power(t+1,alpha)\n",
    "# C[1].append(C_arr)\n",
    "# C_root[1].append(C_arr**(1/2))\n",
    "\n",
    "# for l in range(1,n_layer):\n",
    "#     C_arr = np.ones(model[2*l].bias.shape)\n",
    "#     for t in range(hidden_sizes[l]):\n",
    "#         C_arr[t] = sigma_b_sq[l]/np.power(t+1,alpha)\n",
    "#     C[1].append(C_arr)\n",
    "#     C_root[1].append(C_arr**(1/2))\n",
    "\n",
    "# C_arr = np.ones(model[2*n_layer].bias.shape)\n",
    "# for t in range(output_size):\n",
    "#     C_arr[t] = sigma_b_sq[n_layer]/np.power(t+1,alpha)\n",
    "# C[1].append(C_arr)\n",
    "# C_root[1].append(C_arr**(1/2))\n",
    "\n",
    "\n",
    "''' prior samples are represented by array of coefficients '''\n",
    "def sample_prior(c=1):\n",
    "    w = []\n",
    "    b = []\n",
    "    \n",
    "    ''' weights '''\n",
    "    w_layer = np.random.randn(model[0].weight.shape[0],model[0].weight.shape[1])*C_root[0][0]\n",
    "    w.append(c*w_layer)\n",
    "    \n",
    "    for l in range(1,n_layer):\n",
    "        w_layer = np.random.randn(model[2*l].weight.shape[0],model[2*l].weight.shape[1])*C_root[0][l]\n",
    "        w.append(c*w_layer)\n",
    "     \n",
    "    w_layer = np.random.randn(model[2*n_layer].weight.shape[0],model[2*n_layer].weight.shape[1])*C_root[0][-1]\n",
    "    w.append(c*w_layer)\n",
    "                \n",
    "    ''' biases '''\n",
    "    b_layer = np.random.randn(model[0].bias.shape[0])*C_root[1][0]\n",
    "    b.append(c*b_layer)\n",
    "    \n",
    "    for l in range(1,n_layer):\n",
    "        b_layer = np.random.randn(model[2*l].bias.shape[0])*C_root[1][l]\n",
    "        b.append(c*b_layer)\n",
    "     \n",
    "    b_layer = np.random.randn(model[2*n_layer].bias.shape[0])*C_root[1][-1]\n",
    "    b.append(c*b_layer)\n",
    "    \n",
    "    return [w,b]\n",
    "\n",
    "\n",
    "''' evaluate the log_prior up to a constant '''\n",
    "def logprior(xi):\n",
    "    log_prior = 0\n",
    "    w = xi[0]\n",
    "    b = xi[1]\n",
    "    \n",
    "    ''' weights '''\n",
    "    for l in range(n_layer+1):\n",
    "        log_prior += -0.5*np.sum(w[l]**2/C[0][l])\n",
    "        \n",
    "    ''' biases '''\n",
    "    for l in range(n_layer+1):\n",
    "        log_prior += -0.5*np.sum(b[l]**2/C[1][l])\n",
    "        \n",
    "    return log_prior\n",
    "\n",
    "\n",
    "def prior_proposal(xi):\n",
    "    log_prior_xi = logprior(xi)\n",
    "    \n",
    "    w = xi[0]\n",
    "    b = xi[1]\n",
    "    \n",
    "    ''' Firstly make a cpoy of the current state '''\n",
    "    w_proposal = []\n",
    "    b_proposal = []\n",
    "    for l in range(n_layer+1):\n",
    "        w_proposal.append(dc(w[l]))\n",
    "        b_proposal.append(dc(b[l]))\n",
    "    \n",
    "    random_l = np.random.randint(n_layer)\n",
    "    \n",
    "    ''' Sample index of function to be swapped '''\n",
    "    random_i = geom.rvs(1/alpha)\n",
    "    while random_i>=model[2*random_l].weight.shape[0]-1:  \n",
    "        random_i = geom.rvs(1/alpha)\n",
    "        \n",
    "    ''' swap f^l_i with f^l_{i+1}'''\n",
    "    w_proposal[random_l][random_i,:] = w[random_l][random_i+1,:]\n",
    "    w_proposal[random_l][random_i+1,:] = w[random_l][random_i,:]\n",
    "    w_proposal[random_l+1][:,random_i] = w[random_l+1][:,random_i+1]\n",
    "    w_proposal[random_l+1][:,random_i+1] = w[random_l+1][:,random_i]\n",
    "    b_proposal[random_l][random_i] = b[random_l][random_i+1]\n",
    "    b_proposal[random_l][random_i+1] = b[random_l][random_i]\n",
    "    \n",
    "    xi_proposal = [w_proposal,b_proposal]\n",
    "    log_prior_proposal = logprior(xi_proposal)\n",
    "    \n",
    "    a = np.exp(log_prior_proposal-log_prior_xi)\n",
    "    if np.random.uniform() < a:\n",
    "        counts[0] += 1\n",
    "        return xi_proposal\n",
    "    else:\n",
    "        counts[1] += 1\n",
    "        return xi\n",
    "    \n",
    "counts = np.zeros(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "FUNCTIONS - LIKELIHOOD \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "''' Function to integrate within likelihood '''\n",
    "def f(x,v_a):\n",
    "    value = norm._pdf((x-v_a[0])/sigma)/sigma\n",
    "    for i in range(1,N_a):\n",
    "        value = value*norm._cdf((x-v_a[i])/sigma)\n",
    "    return value\n",
    "\n",
    "\n",
    "def likelihood(pair):\n",
    "    poss = pair[0:8]\n",
    "    vels = pair[8:17]\n",
    "    action = int(pair[17])\n",
    "    poss = np.insert(poss,0,0)\n",
    "    \n",
    "    ''' Check where all actions would take us '''\n",
    "    x = np.zeros((N_a,dim))\n",
    "    v = np.zeros(N_a)\n",
    "    for j in range(N_a):\n",
    "        env.set_state(poss, vels)\n",
    "        x[j,:] = env.step(actions[j])[0]\n",
    "        v[j] = u(x[j,:]).detach().numpy()[0]\n",
    "        \n",
    "    ''' Sort v such that the first entry is the taken action '''\n",
    "    v_a = np.zeros(N_a)\n",
    "    if action!=0:\n",
    "        v_a[0] = v[action]\n",
    "        v_a[1:action+1] = v[0:action]\n",
    "        v_a[action+1:] = v[action+1:]\n",
    "    else:\n",
    "        v_a = v\n",
    "            \n",
    "    ''' Integrate over pdf of chosen action and cdf of other actions '''\n",
    "    lklhd = quad(f,v_a[0]-3*sigma,v_a[0]+3*sigma,args=v_a,limit=500)[0]\n",
    "    return lklhd\n",
    "\n",
    "def loglikelihood(data):\n",
    "    loglikelihood = 0\n",
    "    for j in range(data.shape[0]):\n",
    "        loglikelihood += np.log(likelihood(data[j,:]))\n",
    "    return loglikelihood\n",
    "\n",
    "''' function which is integrated in likelihood gradient '''\n",
    "def f_grad(t,args):\n",
    "    v_a = args[0]\n",
    "    j = args[1]\n",
    "    if j==0:\n",
    "        value = (t-v_a[0])/(sigma**2)*norm._pdf((t-v_a[0])/sigma)/sigma\n",
    "    else:\n",
    "        value = -norm._pdf((v_a[0]-v_a[j])/(np.sqrt(2)*sigma))/(np.sqrt(2)*sigma)*norm._pdf((t-(v_a[0]+v_a[j])/2)/(sigma/np.sqrt(2)))/(sigma/np.sqrt(2))\n",
    "    for i in range(1,N_a):\n",
    "        if i!=j:\n",
    "            value = value*norm._cdf((t-v_a[i])/sigma)\n",
    "    return value\n",
    "\n",
    "''' partial derivative dl/dv '''\n",
    "def grad_ll(v_a,j):\n",
    "    if j==0:\n",
    "        return quad(f_grad,v_a[0]-3*sigma,v_a[0]+3*sigma,args=[v_a,j],limit=500)[0]\n",
    "    else:\n",
    "        return quad(f_grad,(v_a[0]+v_a[j])/2-3*sigma,(v_a[0]+v_a[j])/2+3*sigma,args=[v_a,j],limit=500)[0]\n",
    "\n",
    "    \n",
    "def diff_ll(data):\n",
    "    ''' One (more efficient) gradient implementation '''\n",
    "    diff_w = []\n",
    "    diff_b = []\n",
    "        \n",
    "    x = np.zeros((N_a,dim))\n",
    "    v = np.zeros(N_a)\n",
    "    x_a = np.zeros((N_a,dim))\n",
    "    v_a = np.zeros(N_a)\n",
    "    grad_a = np.zeros(N_a)\n",
    "    data_state = np.zeros(dim)\n",
    "    \n",
    "    ''' Iterate through all or a subset of the data points, and compute the respective gradients '''\n",
    "    if stochastic_gradients and unadjusted:\n",
    "        range_i = random.sample(range(data.shape[0]),10)\n",
    "    else:\n",
    "        range_i = range(data.shape[0])\n",
    "    for i in range_i:\n",
    "        lh = likelihood(data[i,:])\n",
    "        data_poss = data[i,0:8]\n",
    "        data_vels = data[i,8:17]\n",
    "        data_poss = np.insert(data_poss,0,0)\n",
    "        data_action = int(data[i,-1])\n",
    "        \n",
    "        for j in range(N_a):\n",
    "            env.set_state(data_poss, data_vels)\n",
    "            x[j,:] = env.step(j)[0]\n",
    "            v[j] = u(x[j,:]).detach().numpy()[0]\n",
    "        \n",
    "        # sort v and x such that the first entry is the taken action\n",
    "        if data_action!=0:\n",
    "            v_a[0] = v[data_action]\n",
    "            v_a[1:data_action+1] = v[0:data_action]\n",
    "            v_a[data_action+1:] = v[data_action+1:]\n",
    "            x_a[0,:] = x[data_action,:]\n",
    "            x_a[1:data_action+1,:] = x[0:data_action,:]\n",
    "            x_a[data_action+1:,:] = x[data_action+1:,:]\n",
    "        else:\n",
    "            v_a = v\n",
    "            x_a = x\n",
    "            \n",
    "        for j in range(N_a):\n",
    "            grad_a[j] = grad_ll(v_a,j)/lh\n",
    "        mean_grad = np.mean(grad_a)\n",
    "        grad_a -= mean_grad\n",
    "            \n",
    "        for j in range(N_a):\n",
    "            out = u(x_a[j,:])\n",
    "            out.backward(torch.from_numpy(np.array([grad_a[j]])).float())\n",
    "\n",
    "    for l in range(n_layer+1):\n",
    "        diff_w.append(model[2*l].weight.grad.data.clone().detach().numpy())\n",
    "        diff_b.append(model[2*l].bias.grad.data.clone().detach().numpy())\n",
    "        \n",
    "    return [diff_w, diff_b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading and building expert policy\n",
      "obs (1, 17) (1, 17)\n",
      "loaded and built\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "FUNCTIONS - VALUE FUNCTION AND POLICIES\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "''' u(x), which evaluates the function u at x=(pos,speed) with the currently set xi '''\n",
    "def u(x):\n",
    "    x_transformed = np.zeros(dim)\n",
    "    for i in range(dim):\n",
    "        x_transformed[i] = ((x[i]-obs_min[i])/(obs_max[i]-obs_min[i])-1/2)*2\n",
    "        \n",
    "    value = model(torch.from_numpy(x_transformed).float())\n",
    "    return value\n",
    "    \n",
    "''' Policy from berkeley course '''\n",
    "def policy(obs):\n",
    "    return policy_fn(obs[None,:])\n",
    "print('loading and building expert policy')\n",
    "policy_fn = load_policy.load_policy('/Users/torbensell/Dropbox (Cambridge University)/UNI/CAM/PhD/Programme/BIRL_NN_pCN/homework/hw1/experts/HalfCheetah-v2.pkl')\n",
    "print('loaded and built')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "FUNCTIONS - ANALYTICS\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "''' Progress bar to know how much longer one has to wait '''\n",
    "def progressBar(t,value, t_max, acceptances, bar_length=40):\n",
    "    percent = float(t) / t_max\n",
    "    arrow = '-' * int(round(percent * bar_length)-1) + '>'\n",
    "    spaces = ' ' * (bar_length - len(arrow))\n",
    "    sys.stdout.write(\"\\rIteration: {0}    Acceptance ratio: {1}    Percent: [{2}] {3}%  \".format(value,round(acceptances/value,3),arrow + spaces, int(round(percent * 100))))\n",
    "    sys.stdout.flush()      \n",
    "    \n",
    "''' Plotting a value function '''    \n",
    "def func_plot(xi,name):\n",
    "    x = np.arange(obs_min[0],obs_max[0],0.02)\n",
    "    y = np.arange(obs_min[1],obs_max[1],0.05)\n",
    "    X,Y = np.meshgrid(x,y)\n",
    "    Z = np.zeros(X.shape)\n",
    "    \n",
    "    for l in range(n_layer+1):\n",
    "        model[2*l].weight = torch.nn.Parameter(torch.from_numpy(xi[0][l]).float(), requires_grad=False)\n",
    "        model[2*l].bias = torch.nn.Parameter(torch.from_numpy(xi[1][l]).float(), requires_grad=False)\n",
    "        \n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(X.shape[1]):\n",
    "            Z[i,j] = u((X[i,j],Y[i,j],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0))[0]\n",
    "    fig = plt.figure()\n",
    "    ax = fig.gca(projection='3d')\n",
    "    surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=cm.RdBu,linewidth=0, antialiased=False)\n",
    "    ax.zaxis.set_major_locator(LinearLocator(10))\n",
    "    ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n",
    "    ax.set_xlabel('x-axis')\n",
    "    ax.set_ylabel('y-axis')\n",
    "    ax.set_zlabel('z-axis')\n",
    "    ax.view_init(elev=25, azim=-120)\n",
    "    fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "    fig.savefig(name + '.png', bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "def trajectory_plot(xi,name):\n",
    "    x = np.arange(len(xi))\n",
    "    fig = plt.figure()\n",
    "    plt.plot(x,xi)\n",
    "    fig.savefig(name + '.png', bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "def autocorr(x,lags):\n",
    "    mean=np.mean(x)\n",
    "    var=np.var(x)\n",
    "    xp=x-mean\n",
    "    corr=[1. if l==0 else np.sum(xp[l:]*xp[:-l])/len(x)/var for l in lags]\n",
    "    return np.array(corr)\n",
    "\n",
    "''' Calculate the Effective Sample Size, assumes algorithm already burned in '''\n",
    "def ESS(logposterior,name):\n",
    "    fig, ax = plt.subplots()\n",
    "    N = len(logposterior)\n",
    "    ax.stem(autocorr(logposterior, range(int(N*0.1))),use_line_collection=True) \n",
    "    ESS = N/(1+2*sum(autocorr(logposterior, range(int(N*0.1)))))\n",
    "    print('\\nEffective Sample Size:', round(ESS))\n",
    "    print('Samples required to generate 1 independent sample:', round(N/ESS,2))\n",
    "    fig.savefig(name + '.png', bbox_inches='tight')\n",
    "    plt.close(fig)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Uncertainty Quantification Initialisation\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "x_test = np.load('HC_x_test.npy')\n",
    "v_test = []\n",
    "for i in range(N_a):\n",
    "    v_test.append([])\n",
    "\n",
    "x_test_100 = np.load('HC_x_test_100.npy')\n",
    "a_test_100 = np.load('HC_a_test_100.npy')\n",
    "optimal_choice = 0\n",
    "not_optimal_choice = 0\n",
    "v_all = np.zeros((N_a,100))\n",
    "\n",
    "def test_value_fn():\n",
    "    for i in range(N_a):\n",
    "        v = u(x_test[i,:]).detach().numpy()[0]\n",
    "        v_test[i].append(v)\n",
    "        if i>0:\n",
    "            v_test[i][-1]=v_test[i][-1]-v_test[0][-1]\n",
    "    v_test[0][-1] = 0\n",
    "        \n",
    "    global optimal_choice\n",
    "    global not_optimal_choice\n",
    "    v = np.zeros(N_a)\n",
    "    for j in range(100):\n",
    "        for i in range(N_a):\n",
    "            v[i] = u(x_test_100[i,:,j]).detach().numpy()[0]\n",
    "            v_all[i,j] += v[i]\n",
    "        if a_test_100[j]==np.argmax(v):\n",
    "            optimal_choice += 1\n",
    "        else:\n",
    "            not_optimal_choice += 1\n",
    "\n",
    "        \n",
    "def boxplot_value_fn():\n",
    "    global v_test\n",
    "    global optimal_choice\n",
    "    global not_optimal_choice\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title('UQ of relative value function evaluation')\n",
    "    reward_test = np.load('HC_reward_test.npy')\n",
    "    tags = []\n",
    "    for i in range(N_a):\n",
    "        tags.append(str(round(reward_test[i],2)))\n",
    "    ax.set_xticklabels(tags,rotation=45)\n",
    "    ax.boxplot(v_test)\n",
    "    fig.savefig('figs/HC/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_UQ.pdf', dpi=300)\n",
    "    plt.close(fig)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title('UQ of relative value function evaluation')\n",
    "    ax.boxplot(v_test)\n",
    "    fig.savefig('figs/HC/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_UQ_v2.pdf', dpi=300)\n",
    "    plt.close(fig)\n",
    "    \n",
    "    v_test = []\n",
    "    for i in range(N_a):\n",
    "        v_test.append([])\n",
    "        \n",
    "    print('Samples, Correct choices: ',optimal_choice/(optimal_choice+not_optimal_choice))\n",
    "    print('Samples, Wrong choices:   ',not_optimal_choice/(optimal_choice+not_optimal_choice))\n",
    "    \n",
    "    ''' Mean values '''\n",
    "    optimal_choice = 0\n",
    "    not_optimal_choice = 0\n",
    "    \n",
    "    for j in range(100):\n",
    "        for i in range(N_a): \n",
    "            if a_test_100[j]==np.argmax(v_all[:,j]):\n",
    "                optimal_choice += 1\n",
    "            else:\n",
    "                not_optimal_choice += 1\n",
    "                \n",
    "    print('Mean,    Correct choices: ',optimal_choice/(optimal_choice+not_optimal_choice))\n",
    "    print('Mean,    Wrong choices:   ',not_optimal_choice/(optimal_choice+not_optimal_choice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "FUNCTIONS - MCMC (pCN/pCNL)\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "def acceptance_prop(xi_u, xi_v,data,ll_u,diff_u=False):\n",
    "    accept_prop = -ll_u\n",
    "    ll_v = 0\n",
    "    \n",
    "    if method=='pCNL' or method=='CNL':\n",
    "        for l in range(n_layer+1):\n",
    "            model[2*l].weight = torch.nn.Parameter(torch.from_numpy(xi_v[0][l]).float(), requires_grad=True)\n",
    "            model[2*l].bias = torch.nn.Parameter(torch.from_numpy(xi_v[1][l]).float(), requires_grad=True)\n",
    "    else:\n",
    "        for l in range(n_layer+1):\n",
    "            model[2*l].weight = torch.nn.Parameter(torch.from_numpy(xi_v[0][l]).float(), requires_grad=False)\n",
    "            model[2*l].bias = torch.nn.Parameter(torch.from_numpy(xi_v[1][l]).float(), requires_grad=False)\n",
    "        \n",
    "    for j in range(data.shape[0]):\n",
    "        lh = likelihood(data[j,:])\n",
    "        ll_v += np.log(lh)\n",
    "    accept_prop += ll_v\n",
    "    \n",
    "    ''' unadjusted? '''\n",
    "    if unadjusted:\n",
    "        diff_v = diff_ll(data)\n",
    "        return 1,ll_v,diff_v\n",
    "        \n",
    "    elif method=='pCNL':\n",
    "        diff_v = diff_ll(data)\n",
    "        for l in range(n_layer+1):\n",
    "            accept_prop += -np.sum((xi_v[0][l]-xi_u[0][l])*diff_u[0][l])/2 - delta*np.sum((xi_u[0][l]+xi_v[0][l])*diff_u[0][l])/4 + delta*np.linalg.norm(diff_u[0][l]*C_root[0][l])**2/4\n",
    "            accept_prop += -np.sum((xi_v[1][l]-xi_u[1][l])*diff_u[1][l])/2 - delta*np.sum((xi_u[1][l]+xi_v[1][l])*diff_u[1][l])/4 + delta*np.linalg.norm(diff_u[1][l]*C_root[1][l])**2/4\n",
    "\n",
    "            accept_prop -= -np.sum((xi_u[0][l]-xi_v[0][l])*diff_v[0][l])/2 - delta*np.sum((xi_v[0][l]+xi_u[0][l])*diff_v[0][l])/4 + delta*np.linalg.norm(diff_v[0][l]*C_root[0][l])**2/4  \n",
    "            accept_prop -= -np.sum((xi_u[1][l]-xi_v[1][l])*diff_v[1][l])/2 - delta*np.sum((xi_v[1][l]+xi_u[1][l])*diff_v[1][l])/4 + delta*np.linalg.norm(diff_v[1][l]*C_root[1][l])**2/4      \n",
    "        return min(1, np.exp(accept_prop)),ll_v,diff_v\n",
    "#     elif method=='CNL':\n",
    "#         diff_v = diff_ll(data)\n",
    "#         for l in range(n_layer+1):\n",
    "#             accept_prop += -np.sum((xi_v[0][l]-xi_u[0][l])*diff_u[0][l])/2 - delta*np.sum((xi_u[0][l]+xi_v[0][l])*diff_u[0][l]/C[0][l])/4 + delta*np.linalg.norm(diff_u[0][l])**2/4\n",
    "#             accept_prop += -np.sum((xi_v[1][l]-xi_u[1][l])*diff_u[1][l])/2 - delta*np.sum((xi_u[1][l]+xi_v[1][l])*diff_u[1][l]/C[1][l])/4 + delta*np.linalg.norm(diff_u[1][l])**2/4\n",
    "\n",
    "#             accept_prop -= -np.sum((xi_u[0][l]-xi_v[0][l])*diff_v[0][l])/2 - delta*np.sum((xi_v[0][l]+xi_u[0][l])*diff_v[0][l]/C[0][l])/4 + delta*np.linalg.norm(diff_v[0][l])**2/4  \n",
    "#             accept_prop -= -np.sum((xi_u[1][l]-xi_v[1][l])*diff_v[1][l])/2 - delta*np.sum((xi_v[1][l]+xi_u[1][l])*diff_v[1][l]/C[1][l])/4 + delta*np.linalg.norm(diff_v[1][l])**2/4      \n",
    "#         return min(1, np.exp(accept_prop)),ll_v,diff_v\n",
    "    else:\n",
    "        return min(1, np.exp(accept_prop)),ll_v   \n",
    "    \n",
    "def propose(xi,diff=False):\n",
    "    w = xi[0]\n",
    "    b = xi[1]\n",
    "    \n",
    "    noise = sample_prior()\n",
    "    w_noise = noise[0]\n",
    "    b_noise = noise[1]\n",
    "    \n",
    "    w_proposal = []\n",
    "    b_proposal = []\n",
    "    for l in range(n_layer+1):\n",
    "        w_proposal.append(np.zeros(model[2*l].weight.shape))\n",
    "        b_proposal.append(np.zeros(model[2*l].bias.shape))\n",
    "    \n",
    "    if method=='pCNL':\n",
    "        diff_w = diff[0]\n",
    "        diff_b = diff[1]\n",
    "        for l in range(n_layer+1):\n",
    "            w_proposal[l] = ((2-delta)*w[l] + 2*delta*C[0][l]*diff_w[l] + np.sqrt(8*delta)*w_noise[l])/(2+delta)\n",
    "            b_proposal[l] = ((2-delta)*b[l] + 2*delta*C[1][l]*diff_b[l] + np.sqrt(8*delta)*b_noise[l])/(2+delta)\n",
    "#     elif method=='CNL':\n",
    "#         diff_w = diff[0]\n",
    "#         diff_b = diff[1]\n",
    "#         for l in range(n_layer+1):\n",
    "#             w_proposal[l] = ((2*C[0][l]-delta)*w[l] + 2*delta*C[0][l]*diff_w[l] + np.sqrt(8*delta*C[0][l])*w_noise[l])/(2*C[0][l]+delta)\n",
    "#             b_proposal[l] = ((2*C[1][l]-delta)*b[l] + 2*delta*C[1][l]*diff_b[l] + np.sqrt(8*delta*C[1][l])*b_noise[l])/(2*C[1][l]+delta)\n",
    "    else:\n",
    "        for l in range(n_layer+1):\n",
    "            w_proposal[l] = np.sqrt(1-beta*beta)*w[l]+beta*w_noise[l]\n",
    "            b_proposal[l] = np.sqrt(1-beta*beta)*b[l]+beta*b_noise[l]\n",
    "    return [w_proposal,b_proposal]\n",
    "\n",
    "def MCMC(xi,N_data,data,max_time):   \n",
    "    print('\\nMCMC algorithm ('+method + ', N_data=' + str(N_data) + ', ' + str(max_time) + ' seconds) was started: ' + str(time.ctime()))\n",
    "      \n",
    "    acc_ratio = 0\n",
    "    logposterior = []\n",
    "    logp = []\n",
    "    logl = []\n",
    "    \n",
    "    ''' Set model weights and biases to current iterate '''\n",
    "    if method=='pCNL' or method=='CNL':\n",
    "        for l in range(n_layer+1):\n",
    "            model[2*l].weight = torch.nn.Parameter(torch.from_numpy(xi[0][l]).float(), requires_grad=True)\n",
    "            model[2*l].bias = torch.nn.Parameter(torch.from_numpy(xi[1][l]).float(), requires_grad=True)\n",
    "    else:\n",
    "        for l in range(n_layer+1):\n",
    "            model[2*l].weight = torch.nn.Parameter(torch.from_numpy(xi[0][l]).float(), requires_grad=False)\n",
    "            model[2*l].bias = torch.nn.Parameter(torch.from_numpy(xi[1][l]).float(), requires_grad=False)\n",
    "        \n",
    "    ''' Initialise likelihood and gradient ''' \n",
    "    ll = loglikelihood(data)\n",
    "    print('Initial loglikelihood: ',ll)\n",
    "    if method=='CNL' or method=='pCNL':\n",
    "        diff = diff_ll(data)\n",
    "    \n",
    "    ''' Run MCMC '''\n",
    "    start = time.time() \n",
    "    j = 0\n",
    "    it = 0\n",
    "    while(time.time()-start<max_time):\n",
    "        \n",
    "        ''' Swap functions around to get mode switching '''\n",
    "        for _ in range(hidden_sizes[0]):\n",
    "            xi = prior_proposal(xi)\n",
    "            \n",
    "        ''' Propose and calculate acceptance probability '''\n",
    "        if method=='CNL' or method=='pCNL':\n",
    "            xi_proposal = propose(xi,diff)  \n",
    "            a,ll_proposal,diff_proposal = acceptance_prop(xi,xi_proposal,data,ll,diff)\n",
    "        else:\n",
    "            xi_proposal = propose(xi)  \n",
    "            a,ll_proposal = acceptance_prop(xi,xi_proposal,data,ll)\n",
    "        \n",
    "        ''' Accept or reject proposal '''\n",
    "        uni = np.random.uniform()\n",
    "        if uni < a or unadjusted:\n",
    "            if method=='CNL' or method=='pCNL':\n",
    "                diff = diff_proposal\n",
    "            xi = xi_proposal    \n",
    "            ll = ll_proposal\n",
    "            acc_ratio = acc_ratio + 1\n",
    "\n",
    "        ''' prior, likelihood, and posterior traceplots are appended '''\n",
    "        lp = logprior(xi)\n",
    "        logposterior.append(lp+ll)\n",
    "        logp.append(lp)\n",
    "        logl.append(ll)\n",
    "        \n",
    "        if prior_compare and j%10==0:\n",
    "            ''' store value function evaluations for uncertainty estimates '''\n",
    "            test_value_fn()\n",
    "        elif policy_compare and (time.time()-start)>it*t_max/1000 and it<1000:\n",
    "            ''' store sample for future use '''\n",
    "            for l in range(n_layer+1):\n",
    "                np.save('np_saved/HC/samples_policy_learning/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_w'+str(l)+'_sampleNo'+str(it)+'.npy',xi[0][l])\n",
    "                np.save('np_saved/HC/samples_policy_learning/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_b'+str(l)+'_sampleNo'+str(it)+'.npy',xi[1][l])\n",
    "            it += 1\n",
    "        \n",
    "        if method=='pCNL' or method=='CNL':\n",
    "            progressBar(time.time()-start,j+1,max_time,acc_ratio)\n",
    "        elif (j+1)%100==0:\n",
    "            progressBar(time.time()-start,j+1,max_time,acc_ratio)\n",
    "        j+=1\n",
    "        \n",
    "    progressBar(max_time,j,max_time,acc_ratio)\n",
    "    \n",
    "    acc_ratio = acc_ratio/(j)\n",
    "    print('\\nMCMC algorithm terminated: ' + str(time.ctime()) + '. \\nRuntime = ' + str(time.time()-start))\n",
    "    print('Final loglikelihood: ',ll)\n",
    "    print('Acceptance ratio is ',acc_ratio)\n",
    "\n",
    "    for l in range(n_layer+1):\n",
    "        np.save('np_saved/HC/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_lastSample_w'+str(l)+'.npy',xi[0][l])\n",
    "        np.save('np_saved/HC/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_lastSample_b'+str(l)+'.npy',xi[1][l])\n",
    "        \n",
    "    trajectory_plot(logposterior[1:],'figs/HC/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_logposterior')\n",
    "    trajectory_plot(logp[1:],'figs/HC/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_logprior')\n",
    "    trajectory_plot(logl[1:],'figs/HC/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_loglikelihood')\n",
    "    if prior_compare:\n",
    "        boxplot_value_fn()\n",
    "        \n",
    "    func_plot(xi,'figs/MC/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_lastSample')\n",
    "        \n",
    "    ESS(logposterior,'figs/HC/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_autocorr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN PROGRAMMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MCMC algorithm (pCN, N_data=100, 86400 seconds) was started: Tue Jul 14 23:08:48 2020\n",
      "Initial loglikelihood:  -45.06797553436477\n",
      "Iteration: 225103    Acceptance ratio: 0.294    Percent: [--------------------------------------->] 100%  \n",
      "MCMC algorithm terminated: Wed Jul 15 23:08:48 2020. \n",
      "Runtime = 86400.06720089912\n",
      "Final loglikelihood:  -54.246173549495246\n",
      "Acceptance ratio is  0.2938210508078524\n",
      "Samples, Correct choices:  0.3210226111678735\n",
      "Samples, Wrong choices:    0.6789773888321265\n",
      "Mean,    Correct choices:  0.42\n",
      "Mean,    Wrong choices:    0.58\n",
      "\n",
      "Effective Sample Size: 10.0\n",
      "Samples required to generate 1 independent sample: 21769.06\n",
      "Counts: 294653 accepted prior moves, 22215647 rejected ones.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "MAIN PROGRAMME 1 - compare NN prior to KL prior (large number of parameters)\n",
    "\n",
    "\"\"\"    \n",
    "\n",
    "prior_compare = True\n",
    "policy_compare = False  \n",
    "\n",
    "# set maximal runtime\n",
    "t_max = 3600*24\n",
    "\n",
    "optimal_choice = 0\n",
    "not_optimal_choice = 0\n",
    "v_all = np.zeros((N_a,100))\n",
    "\n",
    "''' Initialise network network, see second block for detailed comments '''\n",
    "input_size = dim\n",
    "n_layer = 3\n",
    "hidden_sizes = [100,100,100]\n",
    "output_size = 1\n",
    "hyps = str(hidden_sizes[0])\n",
    "for i in range(1,n_layer):\n",
    "    hyps = hyps+'_'+str(hidden_sizes[i])\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.Tanh(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.Tanh(),\n",
    "                      nn.Linear(hidden_sizes[1], hidden_sizes[2]),\n",
    "                      nn.Tanh(),\n",
    "                      nn.Linear(hidden_sizes[-1], output_size))\n",
    "weights = []\n",
    "biases = []\n",
    "for l in range(n_layer+1):\n",
    "    weights.append(np.zeros(model[2*l].weight.shape))\n",
    "    biases.append(np.zeros(model[2*l].bias.shape))\n",
    "for l in range(n_layer+1):\n",
    "    model[2*l].weight = torch.nn.Parameter(torch.from_numpy(0*weights[l]).float(), requires_grad=False)\n",
    "    model[2*l].bias = torch.nn.Parameter(torch.from_numpy(0*biases[l]).float(), requires_grad=False)\n",
    "''' Initialise covariance operator, see prior block for detailed comments '''\n",
    "C = [[],[]]\n",
    "C_root = [[],[]]\n",
    "C_arr = np.ones(model[0].weight.shape)\n",
    "for t in range(hidden_sizes[0]):\n",
    "    for s in range(input_size):\n",
    "        C_arr[t][s] = sigma_w_sq[0]/np.power(t+1,alpha)\n",
    "C[0].append(C_arr)\n",
    "C_root[0].append(C_arr**(1/2))\n",
    "for l in range(1,n_layer):\n",
    "    C_arr = np.ones(model[2*l].weight.shape)\n",
    "    for t in range(hidden_sizes[l]):\n",
    "        for s in range(hidden_sizes[l-1]):\n",
    "            C_arr[t][s] = sigma_w_sq[l]/np.power(s+1,alpha)/np.power(t+1,alpha)\n",
    "    C[0].append(C_arr)\n",
    "    C_root[0].append(C_arr**(1/2))\n",
    "C_arr = np.ones(model[2*n_layer].weight.shape)   \n",
    "for t in range(output_size):\n",
    "    for s in range(hidden_sizes[n_layer-1]):\n",
    "        C_arr[t][s] = sigma_w_sq[n_layer]/np.power(s+1,alpha)\n",
    "C[0].append(C_arr)   \n",
    "C_root[0].append(C_arr**(1/2))\n",
    "C_arr = np.ones(model[0].bias.shape)\n",
    "for t in range(hidden_sizes[0]):\n",
    "    C_arr[t] = sigma_b_sq[0]/np.power(t+1,alpha)\n",
    "C[1].append(C_arr)\n",
    "C_root[1].append(C_arr**(1/2))\n",
    "for l in range(1,n_layer):\n",
    "    C_arr = np.ones(model[2*l].bias.shape)\n",
    "    for t in range(hidden_sizes[l]):\n",
    "        C_arr[t] = sigma_b_sq[l]/np.power(t+1,alpha)\n",
    "    C[1].append(C_arr)\n",
    "    C_root[1].append(C_arr**(1/2))\n",
    "C_arr = np.ones(model[2*n_layer].bias.shape)\n",
    "for t in range(output_size):\n",
    "    C_arr[t] = sigma_b_sq[n_layer]/np.power(t+1,alpha)\n",
    "C[1].append(C_arr)\n",
    "C_root[1].append(C_arr**(1/2))\n",
    "\n",
    "env = gym.make('HalfCheetah-v2')\n",
    "env.unwrapped\n",
    "env.reset()\n",
    "    \n",
    "try:\n",
    "    data = np.load('HC_data.npy')\n",
    "except FileNotFoundError:\n",
    "    print('\\nNo data found - create data!\\n')\n",
    "N_data = 100\n",
    "\n",
    "''' run pCN '''\n",
    "method = 'pCN'\n",
    "stochastic_gradients = False\n",
    "unadjusted = False\n",
    "beta = 1/59\n",
    "counts = np.zeros(2)\n",
    "try:\n",
    "    xi = [[],[]]\n",
    "    for l in range(n_layer+1):\n",
    "        xi[0].append(np.load('np_saved/HC/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_lastSample_w'+str(l)+'.npy'))\n",
    "        xi[1].append(np.load('np_saved/HC/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_lastSample_b'+str(l)+'.npy')) \n",
    "except FileNotFoundError:\n",
    "    print('\\nStarting from close to 0')\n",
    "    xi = sample_prior(beta)\n",
    "func_plot(xi,'figs/HC/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_firstSample')\n",
    "MCMC(xi,N_data,data[50:50+N_data,:],t_max) \n",
    "print('Counts: '+str(int(counts[0]))+' accepted prior moves, '+str(int(counts[1]))+' rejected ones.')\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MCMC algorithm (pCN, N_data=100, 86400 seconds) was started: Wed Jul 15 23:09:04 2020\n",
      "Initial loglikelihood:  -33.97670185393364\n",
      "Iteration: 245349    Acceptance ratio: 0.214    Percent: [--------------------------------------->] 100%  \n",
      "MCMC algorithm terminated: Thu Jul 16 23:09:04 2020. \n",
      "Runtime = 86400.02739691734\n",
      "Final loglikelihood:  -41.862402080688135\n",
      "Acceptance ratio is  0.21368744115525232\n",
      "\n",
      "Effective Sample Size: 22.0\n",
      "Samples required to generate 1 independent sample: 11107.07\n",
      "Counts: 605615 accepted prior moves, 1847875 rejected ones.\n",
      "\n",
      "MCMC algorithm (pCNL, N_data=100, 86400 seconds) was started: Thu Jul 16 23:09:20 2020\n",
      "Initial loglikelihood:  -51.575600068520444\n",
      "Iteration: 26384    Acceptance ratio: 0.581    Percent: [--------------------------------------->] 100%  \n",
      "MCMC algorithm terminated: Fri Jul 17 23:09:25 2020. \n",
      "Runtime = 86401.34112596512\n",
      "Final loglikelihood:  -40.47636956370256\n",
      "Acceptance ratio is  0.5810339599757429\n",
      "\n",
      "Effective Sample Size: 7.0\n",
      "Samples required to generate 1 independent sample: 3665.21\n",
      "Counts: 62061 accepted prior moves, 201779 rejected ones.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "MAIN PROGRAMME 2 - LEARN policy, and store samples for future use (small number of parameters)\n",
    "\n",
    "\"\"\"    \n",
    "\n",
    "prior_compare = False\n",
    "policy_compare = True   \n",
    "\n",
    "# set maximal runtime\n",
    "t_max = 3600*24\n",
    "\n",
    "''' Initialise network network, see second block for detailed comments '''\n",
    "input_size = dim\n",
    "n_layer = 3\n",
    "hidden_sizes = [10,10,10]\n",
    "output_size = 1\n",
    "hyps = str(hidden_sizes[0])\n",
    "for i in range(1,n_layer):\n",
    "    hyps = hyps+'_'+str(hidden_sizes[i])\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.Tanh(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.Tanh(),\n",
    "                      nn.Linear(hidden_sizes[1], hidden_sizes[2]),\n",
    "                      nn.Tanh(),\n",
    "                      nn.Linear(hidden_sizes[-1], output_size))\n",
    "weights = []\n",
    "biases = []\n",
    "for l in range(n_layer+1):\n",
    "    weights.append(np.zeros(model[2*l].weight.shape))\n",
    "    biases.append(np.zeros(model[2*l].bias.shape))\n",
    "for l in range(n_layer+1):\n",
    "    model[2*l].weight = torch.nn.Parameter(torch.from_numpy(0*weights[l]).float(), requires_grad=False)\n",
    "    model[2*l].bias = torch.nn.Parameter(torch.from_numpy(0*biases[l]).float(), requires_grad=False)\n",
    "''' Initialise covariance operator, see prior block for detailed comments '''\n",
    "C = [[],[]]\n",
    "C_root = [[],[]]\n",
    "C_arr = np.ones(model[0].weight.shape)\n",
    "for t in range(hidden_sizes[0]):\n",
    "    for s in range(input_size):\n",
    "        C_arr[t][s] = sigma_w_sq[0]/np.power(t+1,alpha)\n",
    "C[0].append(C_arr)\n",
    "C_root[0].append(C_arr**(1/2))\n",
    "for l in range(1,n_layer):\n",
    "    C_arr = np.ones(model[2*l].weight.shape)\n",
    "    for t in range(hidden_sizes[l]):\n",
    "        for s in range(hidden_sizes[l-1]):\n",
    "            C_arr[t][s] = sigma_w_sq[l]/np.power(s+1,alpha)/np.power(t+1,alpha)\n",
    "    C[0].append(C_arr)\n",
    "    C_root[0].append(C_arr**(1/2))\n",
    "C_arr = np.ones(model[2*n_layer].weight.shape)   \n",
    "for t in range(output_size):\n",
    "    for s in range(hidden_sizes[n_layer-1]):\n",
    "        C_arr[t][s] = sigma_w_sq[n_layer]/np.power(s+1,alpha)\n",
    "C[0].append(C_arr)   \n",
    "C_root[0].append(C_arr**(1/2))\n",
    "C_arr = np.ones(model[0].bias.shape)\n",
    "for t in range(hidden_sizes[0]):\n",
    "    C_arr[t] = sigma_b_sq[0]/np.power(t+1,alpha)\n",
    "C[1].append(C_arr)\n",
    "C_root[1].append(C_arr**(1/2))\n",
    "for l in range(1,n_layer):\n",
    "    C_arr = np.ones(model[2*l].bias.shape)\n",
    "    for t in range(hidden_sizes[l]):\n",
    "        C_arr[t] = sigma_b_sq[l]/np.power(t+1,alpha)\n",
    "    C[1].append(C_arr)\n",
    "    C_root[1].append(C_arr**(1/2))\n",
    "C_arr = np.ones(model[2*n_layer].bias.shape)\n",
    "for t in range(output_size):\n",
    "    C_arr[t] = sigma_b_sq[n_layer]/np.power(t+1,alpha)\n",
    "C[1].append(C_arr)\n",
    "C_root[1].append(C_arr**(1/2))\n",
    "\n",
    "env = gym.make('HalfCheetah-v2')\n",
    "env.unwrapped\n",
    "env.reset()\n",
    "\n",
    "try:\n",
    "    data = np.load('HC_data.npy')\n",
    "except FileNotFoundError:\n",
    "    print('\\nNo data found - create data!\\n')\n",
    "N_data = 100\n",
    "\n",
    "    \n",
    "''' run pCN '''\n",
    "method = 'pCN'\n",
    "stochastic_gradients = False\n",
    "unadjusted = False\n",
    "beta = 1/59 #1/60 gave 25.2 % acc\n",
    "counts = np.zeros(2)\n",
    "try:\n",
    "    xi = [[],[]]\n",
    "    for l in range(n_layer+1):\n",
    "        xi[0].append(np.load('np_saved/HC/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_lastSample_w'+str(l)+'.npy'))\n",
    "        xi[1].append(np.load('np_saved/HC/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_lastSample_b'+str(l)+'.npy')) \n",
    "except FileNotFoundError:\n",
    "    print('\\nStarting from close to 0')\n",
    "    xi = sample_prior(beta)\n",
    "func_plot(xi,'figs/HC/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_firstSample')\n",
    "MCMC(xi,N_data,data[50:50+N_data,:],t_max) \n",
    "print('Counts: '+str(int(counts[0]))+' accepted prior moves, '+str(int(counts[1]))+' rejected ones.')\n",
    "    \n",
    "    \n",
    "''' run pCNL ''' # can be modified to CNL by just replacing the method to 'CNL'\n",
    "method = 'pCNL'\n",
    "stochastic_gradients = False # if true then unadjusted needs to be true too\n",
    "unadjusted = False\n",
    "delta = 1/36000\n",
    "counts = np.zeros(2)\n",
    "try:\n",
    "    xi = [[],[]]\n",
    "    for l in range(n_layer+1):\n",
    "        xi[0].append(np.load('np_saved/HC/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_lastSample_w'+str(l)+'.npy'))\n",
    "        xi[1].append(np.load('np_saved/HC/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_lastSample_b'+str(l)+'.npy'))\n",
    "except FileNotFoundError:\n",
    "    print('\\nStarting from close to 0')\n",
    "    xi = sample_prior(delta)\n",
    "func_plot(xi,'figs/HC/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_firstSample')\n",
    "MCMC(xi,N_data,data[50:50+N_data,:],t_max) \n",
    "print('Counts: '+str(int(counts[0]))+' accepted prior moves, '+str(int(counts[1]))+' rejected ones.')\n",
    "\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
