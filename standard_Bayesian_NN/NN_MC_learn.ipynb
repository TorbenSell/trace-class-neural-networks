{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import gym\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "from scipy.integrate import quad\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import geom\n",
    "from torch import nn\n",
    "from copy import deepcopy as dc\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['text.usetex'] = True\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "torch.autograd.set_detect_anomaly(False) #makes code very slow but one can find errors in backward()\n",
    "\n",
    "N_a = 3                             # number of possible actions (actions are -1, 0, and 1)\n",
    "sigma = 0.1                         # noise\n",
    "dim = 2                             # dimensionality of the space (v: R^d --> R)\n",
    "obs_min = np.array([-1.2,-0.07])\n",
    "obs_max = np.array([ 0.6, 0.07])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "initialise NN\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Hyperparameters for our network\n",
    "input_size = dim\n",
    "output_size = 1\n",
    "n_layer = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "FUNCTIONS - PRIOR\n",
    "\n",
    "\"\"\" \n",
    "\n",
    "\n",
    "''' Set sigma_w and sigma_b uniformly '''    \n",
    "alpha = 0\n",
    "\n",
    "sigma_w_sq = np.ones(n_layer+1)*1/3\n",
    "sigma_b_sq = np.ones(n_layer+1)*1/3\n",
    "sigma_w_sq[-1] = 1/3\n",
    "sigma_b_sq[-1] = 1/3\n",
    "    \n",
    "\n",
    "''' prior samples are represented by array of coefficients '''\n",
    "def sample_prior(c=1):\n",
    "    w = []\n",
    "    b = []\n",
    "    \n",
    "    ''' weights '''\n",
    "    w_layer = np.random.randn(model[0].weight.shape[0],model[0].weight.shape[1])*C_root[0][0]\n",
    "    w.append(c*w_layer)\n",
    "    \n",
    "    for l in range(1,n_layer):\n",
    "        w_layer = np.random.randn(model[2*l].weight.shape[0],model[2*l].weight.shape[1])*C_root[0][l]\n",
    "        w.append(c*w_layer)\n",
    "     \n",
    "    w_layer = np.random.randn(model[2*n_layer].weight.shape[0],model[2*n_layer].weight.shape[1])*C_root[0][-1]\n",
    "    w.append(c*w_layer)\n",
    "                \n",
    "    ''' biases '''\n",
    "    b_layer = np.random.randn(model[0].bias.shape[0])*C_root[1][0]\n",
    "    b.append(c*b_layer)\n",
    "    \n",
    "    for l in range(1,n_layer):\n",
    "        b_layer = np.random.randn(model[2*l].bias.shape[0])*C_root[1][l]\n",
    "        b.append(c*b_layer)\n",
    "     \n",
    "    b_layer = np.random.randn(model[2*n_layer].bias.shape[0])*C_root[1][-1]\n",
    "    b.append(c*b_layer)\n",
    "    \n",
    "    return [w,b]\n",
    "\n",
    "\n",
    "''' evaluate the log_prior up to a constant '''\n",
    "def logprior(xi):\n",
    "    log_prior = 0\n",
    "    w = xi[0]\n",
    "    b = xi[1]\n",
    "    \n",
    "    ''' weights '''\n",
    "    for l in range(n_layer+1):\n",
    "        log_prior += -0.5*np.sum(w[l]**2/C[0][l])\n",
    "        \n",
    "    ''' biases '''\n",
    "    for l in range(n_layer+1):\n",
    "        log_prior += -0.5*np.sum(b[l]**2/C[1][l])\n",
    "        \n",
    "    return log_prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "FUNCTIONS - LIKELIHOOD \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "''' Function to integrate within likelihood '''\n",
    "def f(x,v_a):\n",
    "    value = norm._pdf((x-v_a[0])/sigma)/sigma\n",
    "    for i in range(1,N_a):\n",
    "        value = value*norm._cdf((x-v_a[i])/sigma)\n",
    "    return value\n",
    "\n",
    "def likelihood(pair):\n",
    "    ''' Check where agent's action would take us '''\n",
    "    pos_curr = pair[0]\n",
    "    speed_curr = pair[1]\n",
    "    action = int(pair[2])\n",
    "    \n",
    "    data_state = pair[0:dim]\n",
    "    x = np.zeros((N_a,dim))\n",
    "    v = np.zeros(N_a)\n",
    "    for j in range(N_a):\n",
    "        env.state = data_state\n",
    "        x[j,:] = env.step(j)[0]\n",
    "        v[j] = u(x[j,:]).detach().numpy()[0]\n",
    "        \n",
    "    # sort v such that the first entry is the taken action\n",
    "    v_a = np.zeros(N_a)\n",
    "    if action!=0:\n",
    "        v_a[0] = v[action]\n",
    "        v_a[1:action+1] = v[0:action]\n",
    "        v_a[action+1:] = v[action+1:]\n",
    "    else:\n",
    "        v_a = v\n",
    "    \n",
    "    ''' Integrate over pdf of chosen action and cdf of other actions '''\n",
    "    lklhd = quad(f,v_a[0]-3*sigma,v_a[0]+3*sigma,args=v_a,limit=200)[0]\n",
    "    return lklhd\n",
    "\n",
    "def loglikelihood(data):\n",
    "    loglikelihood = 0\n",
    "    for j in range(data.shape[0]):\n",
    "        lh = likelihood(data[j,:])\n",
    "        loglikelihood += np.log(lh)\n",
    "    return loglikelihood\n",
    "\n",
    "''' function which is integrated in likelihood gradient '''\n",
    "def f_grad(t,args):\n",
    "    v_a = args[0]\n",
    "    j = args[1]\n",
    "    if j==0:\n",
    "        value = (t-v_a[0])/(sigma**2)*norm._pdf((t-v_a[0])/sigma)/sigma\n",
    "    else:\n",
    "        value = -norm._pdf((v_a[0]-v_a[j])/(np.sqrt(2)*sigma))/(np.sqrt(2)*sigma)*norm._pdf((t-(v_a[0]+v_a[j])/2)/(sigma/np.sqrt(2)))/(sigma/np.sqrt(2))\n",
    "    for i in range(1,N_a):\n",
    "        if i!=j:\n",
    "            value = value*norm._cdf((t-v_a[i])/sigma)\n",
    "    return value\n",
    "\n",
    "''' partial derivative dl/dv '''\n",
    "def grad_ll(v_a,j):\n",
    "    if j==0:\n",
    "        return quad(f_grad,v_a[0]-3*sigma,v_a[0]+3*sigma,args=[v_a,j],limit=200)[0]\n",
    "    else:\n",
    "        return quad(f_grad,(v_a[0]+v_a[j])/2-3*(sigma/np.sqrt(2)),(v_a[0]+v_a[j])/2+3*(sigma/np.sqrt(2)),args=[v_a,j],limit=200)[0]\n",
    "\n",
    "''' partial derivative dl/dw, dl/db'''\n",
    "def diff_ll(data):\n",
    "    diff_w = []\n",
    "    diff_b = []\n",
    "        \n",
    "    x = np.zeros((N_a,dim))\n",
    "    v = np.zeros(N_a)\n",
    "    x_a = np.zeros((N_a,dim))\n",
    "    v_a = np.zeros(N_a)\n",
    "    grad_a = np.zeros(N_a)\n",
    "    data_state = np.zeros(dim)\n",
    "    \n",
    "    ''' Iterate through all or a subset of the data points, and compute the respective gradients '''\n",
    "    if stochastic_gradients and unadjusted:\n",
    "        range_i = random.sample(range(data.shape[0]),10)\n",
    "    else:\n",
    "        range_i = range(data.shape[0])\n",
    "    for i in range_i:\n",
    "        lh = likelihood(data[i,:])\n",
    "        data_state = data[i,0:dim]\n",
    "        data_action = int(data[i,-1])\n",
    "        \n",
    "        ''' compute locations the actions would take us to and the values of the value function at those points '''\n",
    "        for j in range(N_a):\n",
    "            env.state = data_state\n",
    "            x[j,:] = env.step(j)[0]\n",
    "            v[j] = u(x[j,:]).detach().numpy()[0]\n",
    "        \n",
    "        ''' sort v and x such that the first entry is the taken action '''\n",
    "        if data_action!=0:\n",
    "            v_a[0] = v[data_action]\n",
    "            v_a[1:data_action+1] = v[0:data_action]\n",
    "            v_a[data_action+1:] = v[data_action+1:]\n",
    "            x_a[0,:] = x[data_action,:]\n",
    "            x_a[1:data_action+1,:] = x[0:data_action,:]\n",
    "            x_a[data_action+1:,:] = x[data_action+1:,:]\n",
    "        else:\n",
    "            v_a = v\n",
    "            x_a = x\n",
    "            \n",
    "        ''' Calculate gradient at the v_i the action i would give us '''\n",
    "        for j in range(N_a):\n",
    "            grad_a[j] = grad_ll(v_a,j)/lh\n",
    "        mean_grad = np.mean(grad_a)\n",
    "        grad_a -= mean_grad\n",
    "            \n",
    "        for j in range(N_a):\n",
    "            out = u(x_a[j,:])\n",
    "            out.backward(torch.from_numpy(np.array([grad_a[j]])).float())\n",
    "\n",
    "    for l in range(n_layer+1):\n",
    "        diff_w.append(model[2*l].weight.grad.data.clone().detach().numpy())\n",
    "        diff_b.append(model[2*l].bias.grad.data.clone().detach().numpy())\n",
    "        \n",
    "    return [diff_w, diff_b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "FUNCTIONS - VALUE FUNCTION AND POLICIES\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "''' u(x), which evaluates the function u at x=(pos,speed) with the currently set xi '''\n",
    "def u(x):\n",
    "    x_transformed = np.zeros(dim)\n",
    "    for i in range(dim):\n",
    "        x_transformed[i] = ((x[i]-obs_min[i])/(obs_max[i]-obs_min[i])-1/2)*2\n",
    "        \n",
    "    value = model(torch.from_numpy(x_transformed).float())\n",
    "    return value\n",
    "\n",
    "''' Policy from \"Reinforcement Learning: Theory and {Python} Implementation\" '''\n",
    "def policy(position,velocity):\n",
    "        lb = min(-0.09 * (position + 0.25) ** 2 + 0.03,\n",
    "                0.3 * (position + 0.9) ** 4 - 0.008)\n",
    "        ub = -0.07 * (position + 0.38) ** 2 + 0.07\n",
    "        if lb < velocity < ub:\n",
    "            action = 2 # push right\n",
    "        else:\n",
    "            action = 0 # push left\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "FUNCTIONS - ANALYTICS\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "''' Progress bar to know how much longer one has to wait '''\n",
    "def progressBar(t,value, t_max, acceptances, bar_length=40):\n",
    "    percent = float(t) / t_max\n",
    "    arrow = '-' * int(round(percent * bar_length)-1) + '>'\n",
    "    spaces = ' ' * (bar_length - len(arrow))\n",
    "    sys.stdout.write(\"\\rIteration: {0}    Acceptance ratio: {1}    Percent: [{2}] {3}%  \".format(value,round(acceptances/value,3),arrow + spaces, int(round(percent * 100))))\n",
    "    sys.stdout.flush()    \n",
    "        \n",
    "''' Plotting a value function '''    \n",
    "def func_plot(xi,name):\n",
    "    x = np.arange(-1.2,0.6,0.02)\n",
    "    y = np.arange(-0.07,0.07,0.002)\n",
    "    X,Y = np.meshgrid(x,y)\n",
    "    Z = np.zeros(X.shape)\n",
    "    \n",
    "    for l in range(n_layer+1):\n",
    "        model[2*l].weight = torch.nn.Parameter(torch.from_numpy(xi[0][l]).float(), requires_grad=False)\n",
    "        model[2*l].bias = torch.nn.Parameter(torch.from_numpy(xi[1][l]).float(), requires_grad=False)\n",
    "        \n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(X.shape[1]):\n",
    "            Z[i,j] = u((X[i,j],Y[i,j]))[0]\n",
    "            \n",
    "    fig = plt.figure()\n",
    "    ax = fig.gca(projection='3d')\n",
    "    surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=cm.RdBu,linewidth=0, antialiased=False)\n",
    "    ax.zaxis.set_major_locator(LinearLocator(10))\n",
    "    ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n",
    "    ax.set_xlabel('x-axis')\n",
    "    ax.set_ylabel('y-axis')\n",
    "    ax.set_zlabel('z-axis')\n",
    "    ax.view_init(elev=25, azim=-120)\n",
    "    fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "    fig.savefig(name + '.png', bbox_inches='tight',format='png')\n",
    "    plt.close(fig)\n",
    "    \n",
    "''' Plot a trajectory '''  \n",
    "def trajectory_plot(xi,name):\n",
    "    x = np.arange(len(xi))\n",
    "    fig = plt.figure()\n",
    "    plt.plot(x,xi)\n",
    "    fig.savefig(name + '.png', bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "''' Compute the autocorrelations '''    \n",
    "def autocorr(x,lags):\n",
    "    mean=np.mean(x)\n",
    "    var=np.var(x)\n",
    "    xp=x-mean\n",
    "    corr=[1. if l==0 else np.sum(xp[l:]*xp[:-l])/len(x)/var for l in lags]\n",
    "    return np.array(corr)\n",
    "\n",
    "''' Calculate the Effective Sample Size, assumes algorithm already burned in '''\n",
    "def ESS(logposterior,name):\n",
    "    fig, ax = plt.subplots()\n",
    "    N = len(logposterior)\n",
    "    ax.stem(autocorr(logposterior, range(int(N*0.1))),use_line_collection=True) \n",
    "    ESS = N/(1+2*sum(autocorr(logposterior, range(int(N*0.1)))))\n",
    "    print('Effective Sample Size:', round(ESS))\n",
    "    print('Samples required to generate 1 independent sample:', round(N/ESS,2))\n",
    "    fig.savefig(name + '.png', bbox_inches='tight')\n",
    "    plt.close(fig)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Uncertainty Quantification Initialisation\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "x_test = np.load('MC_x_test.npy')\n",
    "a_test = np.asarray(np.load('MC_a_test.npy'),dtype=int)\n",
    "    \n",
    "v_test = [[],[],[],[],[]]\n",
    "for j in range(5):\n",
    "    for i in range(N_a):\n",
    "        v_test[j].append([])\n",
    "\n",
    "def test_value_fn():\n",
    "    for j in range(5):\n",
    "        ''' Evaluate value function at test points '''\n",
    "        for i in range(N_a):\n",
    "            v = u(x_test[i,:,j]).detach().numpy()[0]\n",
    "            v_test[j][i].append(v)\n",
    "        ''' substract value at optimal test point for normalisation purposes'''\n",
    "        for i in range(N_a):\n",
    "            if i!=a_test[j]:\n",
    "                v_test[j][i][-1]=v_test[j][i][-1]-v_test[j][a_test[j]][-1]\n",
    "        v_test[j][a_test[j]][-1] = 0\n",
    "        \n",
    "def boxplot_value_fn():\n",
    "    global v_test\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title('UQ of relative value function evaluation')\n",
    "    ax.boxplot(v_test[0], positions = [0,1,2])\n",
    "    ax.boxplot(v_test[1], positions = [4,5,6])\n",
    "    ax.boxplot(v_test[2], positions = [8,9,10])\n",
    "    ax.boxplot(v_test[3], positions = [12,13,14])\n",
    "    ax.boxplot(v_test[4], positions = [16,17,18])\n",
    "    ax.set_xticklabels(['L','0','R','L','0','R','L','0','R','L','0','R','L','0','R'])\n",
    "\n",
    "    fig.savefig('figs/MC/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_UQ.pdf', dpi=300)\n",
    "    plt.close(fig) \n",
    "    \n",
    "    v_test = [[],[],[],[],[]]\n",
    "    for j in range(5):\n",
    "        for i in range(N_a):\n",
    "            v_test[j].append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "FUNCTIONS - MCMC (pCN/pCNL)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def acceptance_prop(xi_u, xi_v,data,ll_u,diff_u=False):\n",
    "    accept_prop = -ll_u\n",
    "    ll_v = 0\n",
    "    \n",
    "    ''' both pCN and pCNL '''\n",
    "    for l in range(n_layer+1):\n",
    "        model[2*l].weight = torch.nn.Parameter(torch.from_numpy(xi_v[0][l]).float(), requires_grad=True)\n",
    "        model[2*l].bias = torch.nn.Parameter(torch.from_numpy(xi_v[1][l]).float(), requires_grad=True)\n",
    "        \n",
    "    for j in range(data.shape[0]):\n",
    "        lh = likelihood(data[j,:])\n",
    "        ll_v += np.log(lh)\n",
    "    accept_prop += ll_v\n",
    "    \n",
    "    ''' unadjusted? '''\n",
    "    if unadjusted:\n",
    "        diff_v = diff_ll(data)\n",
    "        return 1,ll_v,diff_v\n",
    "    \n",
    "    elif method=='pCNL':\n",
    "        diff_v = diff_ll(data)\n",
    "        for l in range(n_layer+1):\n",
    "            accept_prop += -np.sum((xi_v[0][l]-xi_u[0][l])*diff_u[0][l])/2 - delta*np.sum((xi_u[0][l]+xi_v[0][l])*diff_u[0][l])/4 + delta*np.linalg.norm(diff_u[0][l]*C_root[0][l])**2/4\n",
    "            accept_prop += -np.sum((xi_v[1][l]-xi_u[1][l])*diff_u[1][l])/2 - delta*np.sum((xi_u[1][l]+xi_v[1][l])*diff_u[1][l])/4 + delta*np.linalg.norm(diff_u[1][l]*C_root[1][l])**2/4\n",
    "\n",
    "            accept_prop -= -np.sum((xi_u[0][l]-xi_v[0][l])*diff_v[0][l])/2 - delta*np.sum((xi_v[0][l]+xi_u[0][l])*diff_v[0][l])/4 + delta*np.linalg.norm(diff_v[0][l]*C_root[0][l])**2/4  \n",
    "            accept_prop -= -np.sum((xi_u[1][l]-xi_v[1][l])*diff_v[1][l])/2 - delta*np.sum((xi_v[1][l]+xi_u[1][l])*diff_v[1][l])/4 + delta*np.linalg.norm(diff_v[1][l]*C_root[1][l])**2/4      \n",
    "        return min(1, np.exp(accept_prop)),ll_v,diff_v\n",
    "#     elif method=='CNL':\n",
    "#         diff_v = diff_ll(data)\n",
    "#         for l in range(n_layer+1):\n",
    "#             accept_prop += -np.sum((xi_v[0][l]-xi_u[0][l])*diff_u[0][l])/2 - delta*np.sum((xi_u[0][l]+xi_v[0][l])*diff_u[0][l]/C[0][l])/4 + delta*np.linalg.norm(diff_u[0][l])**2/4\n",
    "#             accept_prop += -np.sum((xi_v[1][l]-xi_u[1][l])*diff_u[1][l])/2 - delta*np.sum((xi_u[1][l]+xi_v[1][l])*diff_u[1][l]/C[1][l])/4 + delta*np.linalg.norm(diff_u[1][l])**2/4\n",
    "\n",
    "#             accept_prop -= -np.sum((xi_u[0][l]-xi_v[0][l])*diff_v[0][l])/2 - delta*np.sum((xi_v[0][l]+xi_u[0][l])*diff_v[0][l]/C[0][l])/4 + delta*np.linalg.norm(diff_v[0][l])**2/4  \n",
    "#             accept_prop -= -np.sum((xi_u[1][l]-xi_v[1][l])*diff_v[1][l])/2 - delta*np.sum((xi_v[1][l]+xi_u[1][l])*diff_v[1][l]/C[1][l])/4 + delta*np.linalg.norm(diff_v[1][l])**2/4      \n",
    "#         return min(1, np.exp(accept_prop)),ll_v,diff_v\n",
    "    else:\n",
    "        return min(1, np.exp(accept_prop)),ll_v\n",
    "    \n",
    "def propose(xi,diff=False):\n",
    "    w = xi[0]\n",
    "    b = xi[1]\n",
    "    \n",
    "    noise = sample_prior()\n",
    "    w_noise = noise[0]\n",
    "    b_noise = noise[1]\n",
    "    \n",
    "    w_proposal = []\n",
    "    b_proposal = []\n",
    "    for l in range(n_layer+1):\n",
    "        w_proposal.append(np.zeros(model[2*l].weight.shape))\n",
    "        b_proposal.append(np.zeros(model[2*l].bias.shape))\n",
    "    \n",
    "    if method=='pCNL':\n",
    "        diff_w = diff[0]\n",
    "        diff_b = diff[1]\n",
    "        for l in range(n_layer+1):\n",
    "            w_proposal[l] = ((2-delta)*w[l] + 2*delta*C[0][l]*diff_w[l] + np.sqrt(8*delta)*w_noise[l])/(2+delta)\n",
    "            b_proposal[l] = ((2-delta)*b[l] + 2*delta*C[1][l]*diff_b[l] + np.sqrt(8*delta)*b_noise[l])/(2+delta)\n",
    "#     elif method=='CNL':\n",
    "#         diff_w = diff[0]\n",
    "#         diff_b = diff[1]\n",
    "#         for l in range(n_layer+1):\n",
    "#             w_proposal[l] = ((2*C[0][l]-delta)*w[l] + 2*delta*C[0][l]*diff_w[l] + np.sqrt(8*delta*C[0][l])*w_noise[l])/(2*C[0][l]+delta)\n",
    "#             b_proposal[l] = ((2*C[1][l]-delta)*b[l] + 2*delta*C[1][l]*diff_b[l] + np.sqrt(8*delta*C[1][l])*b_noise[l])/(2*C[1][l]+delta)\n",
    "    else:\n",
    "        for l in range(n_layer+1):\n",
    "            w_proposal[l] = np.sqrt(1-beta*beta)*w[l]+beta*w_noise[l]\n",
    "            b_proposal[l] = np.sqrt(1-beta*beta)*b[l]+beta*b_noise[l]\n",
    "    return [w_proposal,b_proposal]\n",
    "\n",
    "def MCMC(xi,N_data,data,max_time):   \n",
    "    print('\\nMCMC algorithm ('+method + ', N_data=' + str(N_data) + ', ' + str(max_time) + ' seconds) was started: ' + str(time.ctime()))\n",
    "      \n",
    "    acc_ratio = 0\n",
    "    logposterior = []\n",
    "    logp = []\n",
    "    logl = []\n",
    "    \n",
    "    ''' Set model weights and biases to current iterate '''\n",
    "    for l in range(n_layer+1):\n",
    "        model[2*l].weight = torch.nn.Parameter(torch.from_numpy(xi[0][l]).float(), requires_grad=True)\n",
    "        model[2*l].bias = torch.nn.Parameter(torch.from_numpy(xi[1][l]).float(), requires_grad=True)\n",
    "        \n",
    "    ''' Initialise likelihood and gradient '''    \n",
    "    ll = loglikelihood(data)\n",
    "    print('Initial loglikelihood: ',ll)\n",
    "    if method=='CNL' or method=='pCNL':\n",
    "        diff = diff_ll(data)\n",
    "        func_plot(diff,'figs/MC/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_diff')\n",
    "        \n",
    "    ''' Run MCMC '''\n",
    "    start = time.time() \n",
    "    j = 0\n",
    "    it = 0\n",
    "    while(time.time()-start<max_time):\n",
    "        \n",
    "        ''' Swap functions around to get mode switching, not applicable for standard BNN '''\n",
    "        \n",
    "        ''' Propose and calculate acceptance probability '''\n",
    "        if method=='pCNL' or method=='CNL':\n",
    "            xi_proposal = propose(xi,diff)  \n",
    "            a,ll_proposal,diff_proposal = acceptance_prop(xi,xi_proposal,data,ll,diff)\n",
    "        else:\n",
    "            xi_proposal = propose(xi)  \n",
    "            a,ll_proposal = acceptance_prop(xi,xi_proposal,data,ll)\n",
    "        \n",
    "        ''' Accept or reject proposal '''\n",
    "        uni = np.random.uniform()\n",
    "        if uni < a or unadjusted:\n",
    "            if method=='pCNL' or method=='CNL':\n",
    "                diff = diff_proposal\n",
    "            xi = xi_proposal    \n",
    "            ll = ll_proposal\n",
    "            acc_ratio = acc_ratio + 1\n",
    "\n",
    "        ''' prior, likelihood, and posterior traceplots are appended '''\n",
    "        lp = logprior(xi)\n",
    "        logposterior.append(lp+ll)\n",
    "        logp.append(lp)\n",
    "        logl.append(ll)\n",
    "        \n",
    "        if prior_compare and j%10==0:\n",
    "            ''' store value function evaluations for uncertainty estimates '''\n",
    "            test_value_fn()\n",
    "        elif policy_compare and (time.time()-start)>it*t_max/1000 and it<1000:\n",
    "            ''' store sample for future use '''\n",
    "            for l in range(n_layer+1):\n",
    "                np.save('np_saved/MC/samples_policy_learning/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_w'+str(l)+'_sampleNo'+str(it)+'.npy',xi[0][l])\n",
    "                np.save('np_saved/MC/samples_policy_learning/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_b'+str(l)+'_sampleNo'+str(it)+'.npy',xi[1][l])\n",
    "            it += 1\n",
    "        \n",
    "        if (j+1)%100==0:\n",
    "            progressBar(time.time()-start,j+1,max_time,acc_ratio)\n",
    "        j+=1\n",
    "        \n",
    "    progressBar(max_time,j,max_time,acc_ratio)\n",
    "    \n",
    "    acc_ratio = acc_ratio/(j)\n",
    "    print('\\nMCMC algorithm terminated: ' + str(time.ctime()) + '. \\nRuntime = ' + str(time.time()-start) + '\\nSteps: ' + str(j))\n",
    "    print('Final loglikelihood: ',ll)\n",
    "    print('Acceptance ratio is ',acc_ratio)\n",
    "    if dim_robust_test:\n",
    "        global all_acceptances\n",
    "        all_acceptances.append(acc_ratio)\n",
    "    \n",
    "    trajectory_plot(logposterior[1:],'figs/MC/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_logposterior')\n",
    "    trajectory_plot(logp[1:],'figs/MC/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_logprior')\n",
    "    trajectory_plot(logl[1:],'figs/MC/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_loglikelihood')\n",
    "    if prior_compare:\n",
    "        boxplot_value_fn()\n",
    "    for l in range(n_layer+1):\n",
    "        np.save('np_saved/MC/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_lastSample_w'+str(l)+'.npy',xi[0][l])\n",
    "        np.save('np_saved/MC/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_lastSample_b'+str(l)+'.npy',xi[1][l])\n",
    "    func_plot(xi,'figs/MC/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_lastSample')\n",
    "    \n",
    "    ESS(logposterior,'figs/MC/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_autocorr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN PROGRAMMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MCMC algorithm (pCN, N_data=50, 36000 seconds) was started: Thu Jul  8 19:10:42 2021\n",
      "Initial loglikelihood:  -17.84586236853932\n",
      "Iteration: 565200    Acceptance ratio: 0.26    Percent: [--------------------------------------->] 100%  \n",
      "MCMC algorithm terminated: Fri Jul  9 05:10:43 2021. \n",
      "Runtime = 36000.01379084587\n",
      "Steps: 565200\n",
      "Final loglikelihood:  -10.902652760540295\n",
      "Acceptance ratio is  0.25993276716206654\n",
      "Effective Sample Size: 109.0\n",
      "Samples required to generate 1 independent sample: 5181.7\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "MAIN PROGRAMME 1 - compare NN prior to KL prior (large number of parameters)\n",
    "\n",
    "\"\"\"    \n",
    "\n",
    "prior_compare = True\n",
    "policy_compare = False\n",
    "dim_robust_test = False\n",
    "\n",
    "# set maximal runtime\n",
    "t_max = 3600*10\n",
    "\n",
    "''' Initialise network network, see second block for detailed comments '''\n",
    "input_size = dim\n",
    "output_size = 1\n",
    "n_layer = 3\n",
    "hidden_sizes = [100,100,100]\n",
    "hyps = str(hidden_sizes[0])\n",
    "for i in range(1,n_layer):\n",
    "    hyps = hyps+'_'+str(hidden_sizes[i])\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.Tanh(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.Tanh(),\n",
    "                      nn.Linear(hidden_sizes[1], hidden_sizes[2]),\n",
    "                      nn.Tanh(),\n",
    "                      nn.Linear(hidden_sizes[-1], output_size))\n",
    "weights = []\n",
    "biases = []\n",
    "for l in range(n_layer+1):\n",
    "    weights.append(np.zeros(model[2*l].weight.shape))\n",
    "    biases.append(np.zeros(model[2*l].bias.shape))\n",
    "for l in range(n_layer+1):\n",
    "    model[2*l].weight = torch.nn.Parameter(torch.from_numpy(0*weights[l]).float(), requires_grad=False)\n",
    "    model[2*l].bias = torch.nn.Parameter(torch.from_numpy(0*biases[l]).float(), requires_grad=False)\n",
    "''' Initialise covariance operator, see prior block for detailed comments '''\n",
    "C = [[],[]]\n",
    "C_root = [[],[]]\n",
    "C_arr = np.ones(model[0].weight.shape)\n",
    "for t in range(hidden_sizes[0]):\n",
    "    for s in range(input_size):\n",
    "        C_arr[t][s] = sigma_w_sq[0]/np.power(t+1,alpha)\n",
    "C[0].append(C_arr)\n",
    "C_root[0].append(C_arr**(1/2))\n",
    "for l in range(1,n_layer):\n",
    "    C_arr = np.ones(model[2*l].weight.shape)\n",
    "    for t in range(hidden_sizes[l]):\n",
    "        for s in range(hidden_sizes[l-1]):\n",
    "            C_arr[t][s] = sigma_w_sq[l]/np.power(s+1,alpha)/np.power(t+1,alpha)\n",
    "    C[0].append(C_arr)\n",
    "    C_root[0].append(C_arr**(1/2))\n",
    "C_arr = np.ones(model[2*n_layer].weight.shape)   \n",
    "for t in range(output_size):\n",
    "    for s in range(hidden_sizes[n_layer-1]):\n",
    "        C_arr[t][s] = sigma_w_sq[n_layer]/np.power(s+1,alpha)\n",
    "C[0].append(C_arr)   \n",
    "C_root[0].append(C_arr**(1/2))\n",
    "C_arr = np.ones(model[0].bias.shape)\n",
    "for t in range(hidden_sizes[0]):\n",
    "    C_arr[t] = sigma_b_sq[0]/np.power(t+1,alpha)\n",
    "C[1].append(C_arr)\n",
    "C_root[1].append(C_arr**(1/2))\n",
    "for l in range(1,n_layer):\n",
    "    C_arr = np.ones(model[2*l].bias.shape)\n",
    "    for t in range(hidden_sizes[l]):\n",
    "        C_arr[t] = sigma_b_sq[l]/np.power(t+1,alpha)\n",
    "    C[1].append(C_arr)\n",
    "    C_root[1].append(C_arr**(1/2))\n",
    "C_arr = np.ones(model[2*n_layer].bias.shape)\n",
    "for t in range(output_size):\n",
    "    C_arr[t] = sigma_b_sq[n_layer]/np.power(t+1,alpha)\n",
    "C[1].append(C_arr)\n",
    "C_root[1].append(C_arr**(1/2))\n",
    "\n",
    "# Sample from the prior to see what a sample looks like\n",
    "xi = sample_prior()\n",
    "func_plot(xi,'figs/MC/NN_'+hyps+'_a_prior_sample')\n",
    "\n",
    "# Create environment \n",
    "env = gym.make('MountainCar-v0')\n",
    "env = env.unwrapped\n",
    "data = np.load('MC_data.npy')\n",
    "N_data = 50\n",
    "\n",
    "''' run pCN '''\n",
    "method = 'pCN'\n",
    "stochastic_gradients = False\n",
    "unadjusted = False\n",
    "# beta =  1/7 # for 10 on hidden layer (if all w = b = 1/3), and for 100 on hidden layer (if all w = 1/10, b = 1/30)\n",
    "beta =  1/31  # for 100 on hidden layer (if all w = b = 1/3)\n",
    "try:\n",
    "    xi = [[],[]]\n",
    "    for l in range(n_layer+1):\n",
    "        xi[0].append(np.load('np_saved/MC/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_lastSample_w'+str(l)+'.npy'))\n",
    "        xi[1].append(np.load('np_saved/MC/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_lastSample_b'+str(l)+'.npy'))\n",
    "except FileNotFoundError:\n",
    "    print('Starting from close to 0')\n",
    "    xi = sample_prior(0.1)\n",
    "MCMC(xi,N_data,data[0:N_data,:],t_max) \n",
    "                                          \n",
    "env.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MCMC algorithm (pCNL, N_data=50, 36000 seconds) was started: Wed Jul  7 17:31:19 2021\n",
      "Initial loglikelihood:  -29.19219649598323\n",
      "Iteration: 106151    Acceptance ratio: 0.72    Percent: [--------------------------------------->] 100%  \n",
      "MCMC algorithm terminated: Thu Jul  8 03:31:22 2021. \n",
      "Runtime = 36000.15491604805\n",
      "Steps: 106151\n",
      "Final loglikelihood:  -29.4802415896125\n",
      "Acceptance ratio is  0.7195787133423143\n",
      "Effective Sample Size: 312.0\n",
      "Samples required to generate 1 independent sample: 340.23\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "MAIN PROGRAMME 2 - LEARN policy, and store samples for future use (small number of parameters)\n",
    "\n",
    "\"\"\"    \n",
    "\n",
    "prior_compare = False\n",
    "policy_compare = True\n",
    "dim_robust_test = False\n",
    "\n",
    "# set maximal runtime\n",
    "t_max = 3600*10\n",
    "\n",
    "''' Initialise network network, see second block for detailed comments '''\n",
    "input_size = dim\n",
    "output_size = 1\n",
    "n_layer = 3\n",
    "hidden_sizes = [10,10,10]\n",
    "hyps = str(hidden_sizes[0])\n",
    "for i in range(1,n_layer):\n",
    "    hyps = hyps+'_'+str(hidden_sizes[i])\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.Tanh(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.Tanh(),\n",
    "                      nn.Linear(hidden_sizes[1], hidden_sizes[2]),\n",
    "                      nn.Tanh(),\n",
    "                      nn.Linear(hidden_sizes[-1], output_size))\n",
    "weights = []\n",
    "biases = []\n",
    "for l in range(n_layer+1):\n",
    "    weights.append(np.zeros(model[2*l].weight.shape))\n",
    "    biases.append(np.zeros(model[2*l].bias.shape))\n",
    "for l in range(n_layer+1):\n",
    "    model[2*l].weight = torch.nn.Parameter(torch.from_numpy(0*weights[l]).float(), requires_grad=False)\n",
    "    model[2*l].bias = torch.nn.Parameter(torch.from_numpy(0*biases[l]).float(), requires_grad=False)\n",
    "''' Initialise covariance operator, see prior block for detailed comments '''\n",
    "C = [[],[]]\n",
    "C_root = [[],[]]\n",
    "C_arr = np.ones(model[0].weight.shape)\n",
    "for t in range(hidden_sizes[0]):\n",
    "    for s in range(input_size):\n",
    "        C_arr[t][s] = sigma_w_sq[0]/np.power(t+1,alpha)\n",
    "C[0].append(C_arr)\n",
    "C_root[0].append(C_arr**(1/2))\n",
    "for l in range(1,n_layer):\n",
    "    C_arr = np.ones(model[2*l].weight.shape)\n",
    "    for t in range(hidden_sizes[l]):\n",
    "        for s in range(hidden_sizes[l-1]):\n",
    "            C_arr[t][s] = sigma_w_sq[l]/np.power(s+1,alpha)/np.power(t+1,alpha)\n",
    "    C[0].append(C_arr)\n",
    "    C_root[0].append(C_arr**(1/2))\n",
    "C_arr = np.ones(model[2*n_layer].weight.shape)   \n",
    "for t in range(output_size):\n",
    "    for s in range(hidden_sizes[n_layer-1]):\n",
    "        C_arr[t][s] = sigma_w_sq[n_layer]/np.power(s+1,alpha)\n",
    "C[0].append(C_arr)   \n",
    "C_root[0].append(C_arr**(1/2))\n",
    "C_arr = np.ones(model[0].bias.shape)\n",
    "for t in range(hidden_sizes[0]):\n",
    "    C_arr[t] = sigma_b_sq[0]/np.power(t+1,alpha)\n",
    "C[1].append(C_arr)\n",
    "C_root[1].append(C_arr**(1/2))\n",
    "for l in range(1,n_layer):\n",
    "    C_arr = np.ones(model[2*l].bias.shape)\n",
    "    for t in range(hidden_sizes[l]):\n",
    "        C_arr[t] = sigma_b_sq[l]/np.power(t+1,alpha)\n",
    "    C[1].append(C_arr)\n",
    "    C_root[1].append(C_arr**(1/2))\n",
    "C_arr = np.ones(model[2*n_layer].bias.shape)\n",
    "for t in range(output_size):\n",
    "    C_arr[t] = sigma_b_sq[n_layer]/np.power(t+1,alpha)\n",
    "C[1].append(C_arr)\n",
    "C_root[1].append(C_arr**(1/2))\n",
    "\n",
    "# Sample from the prior to see what a sample looks like\n",
    "xi = sample_prior()\n",
    "func_plot(xi,'figs/MC/NN_'+hyps+'_a_prior_sample')\n",
    "\n",
    "# Create environment \n",
    "env = gym.make('MountainCar-v0')\n",
    "env = env.unwrapped\n",
    "data = np.load('MC_data.npy')\n",
    "N_data = 50 \n",
    "\n",
    "''' run pCN '''\n",
    "method = 'pCN'\n",
    "stochastic_gradients = False\n",
    "unadjusted = False\n",
    "beta =  1/7\n",
    "try:\n",
    "    xi = [[],[]]\n",
    "    for l in range(n_layer+1):\n",
    "        xi[0].append(np.load('np_saved/MC/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_lastSample_w'+str(l)+'.npy'))\n",
    "        xi[1].append(np.load('np_saved/MC/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_lastSample_b'+str(l)+'.npy'))\n",
    "except FileNotFoundError:\n",
    "    print('Starting from close to 0')\n",
    "    xi = sample_prior(0.1)\n",
    "MCMC(xi,N_data,data[0:N_data,:],t_max) \n",
    "\n",
    "''' run pCNL ''' # can be modified to CNL by just replacing the method to 'CNL'\n",
    "method = 'pCNL'\n",
    "stochastic_gradients = False # if true then unadjusted needs to be true too\n",
    "unadjusted = False\n",
    "delta = 1/400\n",
    "try:\n",
    "    xi = [[],[]]\n",
    "    for l in range(n_layer+1):\n",
    "        xi[0].append(np.load('np_saved/MC/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_lastSample_w'+str(l)+'.npy'))\n",
    "        xi[1].append(np.load('np_saved/MC/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_lastSample_b'+str(l)+'.npy'))\n",
    "except FileNotFoundError:\n",
    "    print('\\nStarting from close to 0')\n",
    "    xi = sample_prior(0.1)\n",
    "MCMC(xi,N_data,data[0:N_data,:],t_max) \n",
    "                                          \n",
    "env.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MCMC algorithm (pCN, N_data=50, 18000 seconds) was started: Fri Apr  8 22:47:27 2022\n",
      "Initial loglikelihood:  -35.19923810573533\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-d967504f1cf6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nStarting from close to 0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mxi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_prior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mMCMC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mN_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Number of layers: '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m', number of nodes per layer: '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-695e7683a304>\u001b[0m in \u001b[0;36mMCMC\u001b[0;34m(xi, N_data, data, max_time)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mxi_proposal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpropose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mll_proposal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macceptance_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxi_proposal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;34m''' Accept or reject proposal '''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-695e7683a304>\u001b[0m in \u001b[0;36macceptance_prop\u001b[0;34m(xi_u, xi_v, data, ll_u, diff_u)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mlh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlikelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mll_v\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0maccept_prop\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mll_v\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-b337162556c8>\u001b[0m in \u001b[0;36mlikelihood\u001b[0;34m(pair)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;34m''' Integrate over pdf of chosen action and cdf of other actions '''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mlklhd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv_a\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv_a\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mv_a\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlklhd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/NN_gym/lib/python3.6/site-packages/scipy/integrate/quadpack.py\u001b[0m in \u001b[0;36mquad\u001b[0;34m(func, a, b, args, full_output, epsabs, epsrel, limit, points, weight, wvar, wopts, maxp1, limlst)\u001b[0m\n\u001b[1;32m    350\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         retval = _quad(func, a, b, args, full_output, epsabs, epsrel, limit,\n\u001b[0;32m--> 352\u001b[0;31m                        points)\n\u001b[0m\u001b[1;32m    353\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpoints\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/NN_gym/lib/python3.6/site-packages/scipy/integrate/quadpack.py\u001b[0m in \u001b[0;36m_quad\u001b[0;34m(func, a, b, args, full_output, epsabs, epsrel, limit, points)\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpoints\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minfbounds\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_quadpack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qagse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfull_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepsabs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepsrel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_quadpack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qagie\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbound\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minfbounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfull_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepsabs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepsrel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-b337162556c8>\u001b[0m in \u001b[0;36mf\u001b[0;34m(x, v_a)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m''' Function to integrate within likelihood '''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mv_a\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mv_a\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "MAIN PROGRAMME 3 - check dimension-robustness of the proposed Neural Network prior\n",
    "\n",
    "\"\"\"    \n",
    "\n",
    "prior_compare = False\n",
    "policy_compare = False\n",
    "dim_robust_test = True\n",
    "\n",
    "# set maximal runtime\n",
    "t_max = 3600*5\n",
    "\n",
    "all_acceptances = []\n",
    "\n",
    "for hidden_size in [10,20,30,40,50,60,70,80,90,100]:\n",
    "    ''' Initialise network network, see second block for detailed comments '''\n",
    "    input_size = dim\n",
    "    output_size = 1\n",
    "    n_layer = 3\n",
    "    hidden_sizes = [hidden_size,hidden_size,hidden_size]\n",
    "    hyps = str(hidden_sizes[0])\n",
    "    for i in range(1,n_layer):\n",
    "        hyps = hyps+'_'+str(hidden_sizes[i])\n",
    "    model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                          nn.Tanh(),\n",
    "                          nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                          nn.Tanh(),\n",
    "                          nn.Linear(hidden_sizes[1], hidden_sizes[2]),\n",
    "                          nn.Tanh(),\n",
    "                          nn.Linear(hidden_sizes[-1], output_size))\n",
    "    weights = []\n",
    "    biases = []\n",
    "    for l in range(n_layer+1):\n",
    "        weights.append(np.zeros(model[2*l].weight.shape))\n",
    "        biases.append(np.zeros(model[2*l].bias.shape))\n",
    "    for l in range(n_layer+1):\n",
    "        model[2*l].weight = torch.nn.Parameter(torch.from_numpy(0*weights[l]).float(), requires_grad=False)\n",
    "        model[2*l].bias = torch.nn.Parameter(torch.from_numpy(0*biases[l]).float(), requires_grad=False)\n",
    "    ''' Initialise covariance operator, see prior block for detailed comments '''\n",
    "    # Recall that alpha = 0 here\n",
    "    C = [[],[]]\n",
    "    C_root = [[],[]]\n",
    "    C_arr = np.ones(model[0].weight.shape)\n",
    "    for t in range(hidden_sizes[0]):\n",
    "        for s in range(input_size):\n",
    "            C_arr[t][s] = sigma_w_sq[0]/np.power(t+1,alpha)\n",
    "    C[0].append(C_arr)\n",
    "    C_root[0].append(C_arr**(1/2))\n",
    "    for l in range(1,n_layer):\n",
    "        C_arr = np.ones(model[2*l].weight.shape)\n",
    "        for t in range(hidden_sizes[l]):\n",
    "            for s in range(hidden_sizes[l-1]):\n",
    "                C_arr[t][s] = sigma_w_sq[l]/np.power(s+1,alpha)/np.power(t+1,alpha)\n",
    "        C[0].append(C_arr)\n",
    "        C_root[0].append(C_arr**(1/2))\n",
    "    C_arr = np.ones(model[2*n_layer].weight.shape)   \n",
    "    for t in range(output_size):\n",
    "        for s in range(hidden_sizes[n_layer-1]):\n",
    "            C_arr[t][s] = sigma_w_sq[n_layer]/np.power(s+1,alpha)\n",
    "    C[0].append(C_arr)   \n",
    "    C_root[0].append(C_arr**(1/2))\n",
    "    C_arr = np.ones(model[0].bias.shape)\n",
    "    for t in range(hidden_sizes[0]):\n",
    "        C_arr[t] = sigma_b_sq[0]/np.power(t+1,alpha)\n",
    "    C[1].append(C_arr)\n",
    "    C_root[1].append(C_arr**(1/2))\n",
    "    for l in range(1,n_layer):\n",
    "        C_arr = np.ones(model[2*l].bias.shape)\n",
    "        for t in range(hidden_sizes[l]):\n",
    "            C_arr[t] = sigma_b_sq[l]/np.power(t+1,alpha)\n",
    "        C[1].append(C_arr)\n",
    "        C_root[1].append(C_arr**(1/2))\n",
    "    C_arr = np.ones(model[2*n_layer].bias.shape)\n",
    "    for t in range(output_size):\n",
    "        C_arr[t] = sigma_b_sq[n_layer]/np.power(t+1,alpha)\n",
    "    C[1].append(C_arr)\n",
    "    C_root[1].append(C_arr**(1/2))\n",
    "\n",
    "    # Sample from the prior to see what a sample looks like\n",
    "    xi = sample_prior()\n",
    "    func_plot(xi,'figs/MC/NN_'+hyps+'_a_prior_sample')\n",
    "\n",
    "    # Create environment \n",
    "    env = gym.make('MountainCar-v0')\n",
    "    env = env.unwrapped\n",
    "\n",
    "    data = np.load('MC_data.npy')\n",
    "\n",
    "    for N_data in [50]: \n",
    "\n",
    "        ''' run pCN '''\n",
    "        method = 'pCN'\n",
    "        stochastic_gradients = False\n",
    "        unadjusted = False\n",
    "        beta =  1/7\n",
    "        try:\n",
    "            xi = [[],[]]\n",
    "            for l in range(n_layer+1):\n",
    "                xi[0].append(np.load('np_saved/MC/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_lastSample_w'+str(l)+'.npy'))\n",
    "                xi[1].append(np.load('np_saved/MC/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_lastSample_b'+str(l)+'.npy'))\n",
    "        except FileNotFoundError:\n",
    "            print('\\nStarting from close to 0')\n",
    "            xi = sample_prior(0.1)\n",
    "        MCMC(xi,N_data,data[0:N_data,:],t_max) \n",
    "        print('Number of layers: '+str(int(n_layer))+', number of nodes per layer: '+str(int(hidden_size)))\n",
    "\n",
    "    env.close()   \n",
    "\n",
    "np.save('MC_dim_independence_acceptances.npy',all_acceptances)\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "ax.set_title('Acceptance ratio by layer width')\n",
    "ax.plot([10,20,30,40,50,60,70,80,90,100],all_acceptances)\n",
    "\n",
    "fig.savefig('figs/NN_dim_independence.pdf', dpi=300)\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Respective acceptances and total number of parameters for the different layer widths:\n",
      "10:   26.85%     261\n",
      "20:   15.62%     921\n",
      "30:   9.09%     1981\n",
      "40:   5.09%     3441\n",
      "50:   2.73%     5301\n",
      "60:   1.51%     7561\n",
      "70:   0.86%     10221\n",
      "80:   0.5%     13281\n",
      "90:   0.36%     16741\n",
      "100:  0.18%     20601\n"
     ]
    }
   ],
   "source": [
    "all_acceptances = np.load('MC_dim_independence_acceptances.npy')\n",
    "print('\\nRespective acceptances and total number of parameters for the different layer widths:')\n",
    "for N_l in [10,20,30,40,50,60,70,80,90,100]:\n",
    "    number_w = N_l+2*(N_l**2)+2*N_l\n",
    "    number_b = 1+3*N_l\n",
    "    if N_l==100:\n",
    "        print(str(N_l)+':  '+str(round(all_acceptances[int(N_l/10-1)]*100,2))+'%'+'     '+str(number_w+number_b))\n",
    "    else:\n",
    "        print(str(N_l)+':   '+str(round(all_acceptances[int(N_l/10-1)]*100,2))+'%'+'     '+str(number_w+number_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MCMC algorithm (pCN, N_data=50, 18000 seconds) was started: Fri Apr  8 23:04:43 2022\n",
      "Initial loglikelihood:  -35.19923810573533\n",
      "Iteration: 279211    Acceptance ratio: 0.212    Percent: [--------------------------------------->] 100%  \n",
      "MCMC algorithm terminated: Sat Apr  9 04:04:43 2022. \n",
      "Runtime = 18000.008841991425\n",
      "Steps: 279211\n",
      "Final loglikelihood:  -18.447714281226723\n",
      "Acceptance ratio is  0.21237701953003285\n",
      "Effective Sample Size: 306\n",
      "Samples required to generate 1 independent sample: 913.45\n",
      "Number of layers: 3, number of nodes per layer: 10\n",
      "\n",
      "MCMC algorithm (pCN, N_data=50, 18000 seconds) was started: Sat Apr  9 04:05:08 2022\n",
      "Initial loglikelihood:  -48.952184777819724\n",
      "Iteration: 277155    Acceptance ratio: 0.15    Percent: [--------------------------------------->] 100%  \n",
      "MCMC algorithm terminated: Sat Apr  9 09:05:08 2022. \n",
      "Runtime = 18000.009399175644\n",
      "Steps: 277155\n",
      "Final loglikelihood:  -22.85819067415917\n",
      "Acceptance ratio is  0.15008930021107322\n",
      "Effective Sample Size: 292\n",
      "Samples required to generate 1 independent sample: 950.35\n",
      "Number of layers: 3, number of nodes per layer: 20\n",
      "\n",
      "MCMC algorithm (pCN, N_data=50, 18000 seconds) was started: Sat Apr  9 09:05:42 2022\n",
      "Initial loglikelihood:  -55.98598593480188\n",
      "Iteration: 281028    Acceptance ratio: 0.109    Percent: [--------------------------------------->] 100%  \n",
      "MCMC algorithm terminated: Sat Apr  9 14:05:42 2022. \n",
      "Runtime = 18000.013796806335\n",
      "Steps: 281028\n",
      "Final loglikelihood:  -18.665513749090476\n",
      "Acceptance ratio is  0.10920620009394082\n",
      "Effective Sample Size: 286\n",
      "Samples required to generate 1 independent sample: 983.53\n",
      "Number of layers: 3, number of nodes per layer: 30\n",
      "\n",
      "MCMC algorithm (pCN, N_data=50, 18000 seconds) was started: Sat Apr  9 14:06:06 2022\n",
      "Initial loglikelihood:  -53.842078012176565\n",
      "Iteration: 280708    Acceptance ratio: 0.085    Percent: [--------------------------------------->] 100%  \n",
      "MCMC algorithm terminated: Sat Apr  9 19:06:07 2022. \n",
      "Runtime = 18000.04546380043\n",
      "Steps: 280708\n",
      "Final loglikelihood:  -16.682475963099595\n",
      "Acceptance ratio is  0.08522022884990808\n",
      "Effective Sample Size: 251\n",
      "Samples required to generate 1 independent sample: 1116.15\n",
      "Number of layers: 3, number of nodes per layer: 40\n",
      "\n",
      "MCMC algorithm (pCN, N_data=50, 18000 seconds) was started: Sat Apr  9 19:06:31 2022\n",
      "Initial loglikelihood:  -54.81233749674324\n",
      "Iteration: 279812    Acceptance ratio: 0.068    Percent: [--------------------------------------->] 100%  \n",
      "MCMC algorithm terminated: Sun Apr 10 00:06:31 2022. \n",
      "Runtime = 18000.03283596039\n",
      "Steps: 279812\n",
      "Final loglikelihood:  -21.52967346364702\n",
      "Acceptance ratio is  0.06808499992852343\n",
      "Effective Sample Size: 110\n",
      "Samples required to generate 1 independent sample: 2535.82\n",
      "Number of layers: 3, number of nodes per layer: 50\n",
      "\n",
      "MCMC algorithm (pCN, N_data=50, 18000 seconds) was started: Sun Apr 10 00:07:00 2022\n",
      "Initial loglikelihood:  -54.93458048192917\n",
      "Iteration: 276959    Acceptance ratio: 0.055    Percent: [--------------------------------------->] 100%  \n",
      "MCMC algorithm terminated: Sun Apr 10 05:07:00 2022. \n",
      "Runtime = 18000.022491693497\n",
      "Steps: 276959\n",
      "Final loglikelihood:  -16.241243662687378\n",
      "Acceptance ratio is  0.05466874158268913\n",
      "Effective Sample Size: 75\n",
      "Samples required to generate 1 independent sample: 3679.49\n",
      "Number of layers: 3, number of nodes per layer: 60\n",
      "\n",
      "MCMC algorithm (pCN, N_data=50, 18000 seconds) was started: Sun Apr 10 05:07:23 2022\n",
      "Initial loglikelihood:  -55.3201120303235\n",
      "Iteration: 274954    Acceptance ratio: 0.042    Percent: [--------------------------------------->] 100%  \n",
      "MCMC algorithm terminated: Sun Apr 10 10:07:23 2022. \n",
      "Runtime = 18000.036712884903\n",
      "Steps: 274954\n",
      "Final loglikelihood:  -14.627603163507047\n",
      "Acceptance ratio is  0.04249074390625341\n",
      "Effective Sample Size: 59\n",
      "Samples required to generate 1 independent sample: 4699.37\n",
      "Number of layers: 3, number of nodes per layer: 70\n",
      "\n",
      "MCMC algorithm (pCN, N_data=50, 18000 seconds) was started: Sun Apr 10 10:07:45 2022\n",
      "Initial loglikelihood:  -56.13833366174094\n",
      "Iteration: 275870    Acceptance ratio: 0.039    Percent: [--------------------------------------->] 100%  \n",
      "MCMC algorithm terminated: Sun Apr 10 15:07:45 2022. \n",
      "Runtime = 18000.02604007721\n",
      "Steps: 275870\n",
      "Final loglikelihood:  -18.099635351448367\n",
      "Acceptance ratio is  0.03911262551201653\n",
      "Effective Sample Size: 75\n",
      "Samples required to generate 1 independent sample: 3693.53\n",
      "Number of layers: 3, number of nodes per layer: 80\n",
      "\n",
      "MCMC algorithm (pCN, N_data=50, 18000 seconds) was started: Sun Apr 10 15:08:10 2022\n",
      "Initial loglikelihood:  -55.854608004424634\n",
      "Iteration: 272419    Acceptance ratio: 0.03    Percent: [--------------------------------------->] 100%  \n",
      "MCMC algorithm terminated: Sun Apr 10 20:08:10 2022. \n",
      "Runtime = 18000.046525001526\n",
      "Steps: 272419\n",
      "Final loglikelihood:  -14.24575888796187\n",
      "Acceptance ratio is  0.02967854665056402\n",
      "Effective Sample Size: 40\n",
      "Samples required to generate 1 independent sample: 6829.63\n",
      "Number of layers: 3, number of nodes per layer: 90\n",
      "\n",
      "MCMC algorithm (pCN, N_data=50, 18000 seconds) was started: Sun Apr 10 20:08:33 2022\n",
      "Initial loglikelihood:  -54.85171739622235\n",
      "Iteration: 264298    Acceptance ratio: 0.022    Percent: [--------------------------------------->] 100%  \n",
      "MCMC algorithm terminated: Mon Apr 11 01:08:33 2022. \n",
      "Runtime = 18000.01804113388\n",
      "Steps: 264298\n",
      "Final loglikelihood:  -21.73100823993074\n",
      "Acceptance ratio is  0.022251398043117994\n",
      "Effective Sample Size: 35\n",
      "Samples required to generate 1 independent sample: 7611.07\n",
      "Number of layers: 3, number of nodes per layer: 100\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "MAIN PROGRAMME 4 - check dimension-robustness of the proposed Neural Network prior, 1/N scaling\n",
    "\n",
    "\"\"\"    \n",
    "\n",
    "prior_compare = False\n",
    "policy_compare = False\n",
    "dim_robust_test = True\n",
    "\n",
    "# set maximal runtime\n",
    "t_max = 3600*5\n",
    "\n",
    "all_acceptances = []\n",
    "\n",
    "for hidden_size in [10,20,30,40,50,60,70,80,90,100]:\n",
    "    ''' Initialise network network, see second block for detailed comments '''\n",
    "    input_size = dim\n",
    "    output_size = 1\n",
    "    n_layer = 3\n",
    "    hidden_sizes = [hidden_size,hidden_size,hidden_size]\n",
    "    hyps = str(hidden_sizes[0])\n",
    "    for i in range(1,n_layer):\n",
    "        hyps = hyps+'_'+str(hidden_sizes[i])\n",
    "    model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                          nn.Tanh(),\n",
    "                          nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                          nn.Tanh(),\n",
    "                          nn.Linear(hidden_sizes[1], hidden_sizes[2]),\n",
    "                          nn.Tanh(),\n",
    "                          nn.Linear(hidden_sizes[-1], output_size))\n",
    "    weights = []\n",
    "    biases = []\n",
    "    for l in range(n_layer+1):\n",
    "        weights.append(np.zeros(model[2*l].weight.shape))\n",
    "        biases.append(np.zeros(model[2*l].bias.shape))\n",
    "    for l in range(n_layer+1):\n",
    "        model[2*l].weight = torch.nn.Parameter(torch.from_numpy(0*weights[l]).float(), requires_grad=False)\n",
    "        model[2*l].bias = torch.nn.Parameter(torch.from_numpy(0*biases[l]).float(), requires_grad=False)\n",
    "    ''' Initialise covariance operator, see prior block for detailed comments '''\n",
    "    C = [[],[]]\n",
    "    C_root = [[],[]]\n",
    "    C_arr = np.ones(model[0].weight.shape)\n",
    "    for t in range(hidden_sizes[0]):\n",
    "        for s in range(input_size):\n",
    "            C_arr[t][s] = sigma_w_sq[0]/np.sqrt(dim)\n",
    "    C[0].append(C_arr)\n",
    "    C_root[0].append(C_arr**(1/2))\n",
    "    for l in range(1,n_layer):\n",
    "        C_arr = np.ones(model[2*l].weight.shape)\n",
    "        for t in range(hidden_sizes[l]):\n",
    "            for s in range(hidden_sizes[l-1]):\n",
    "                C_arr[t][s] = sigma_w_sq[l]*10/np.sqrt(hidden_size)\n",
    "        C[0].append(C_arr)\n",
    "        C_root[0].append(C_arr**(1/2))\n",
    "    C_arr = np.ones(model[2*n_layer].weight.shape)   \n",
    "    for t in range(output_size):\n",
    "        for s in range(hidden_sizes[n_layer-1]):\n",
    "            C_arr[t][s] = sigma_w_sq[n_layer]*10/np.sqrt(hidden_size)\n",
    "    C[0].append(C_arr)   \n",
    "    C_root[0].append(C_arr**(1/2))\n",
    "    C_arr = np.ones(model[0].bias.shape)\n",
    "    for t in range(hidden_sizes[0]):\n",
    "        C_arr[t] = sigma_b_sq[0]\n",
    "    C[1].append(C_arr)\n",
    "    C_root[1].append(C_arr**(1/2))\n",
    "    for l in range(1,n_layer):\n",
    "        C_arr = np.ones(model[2*l].bias.shape)\n",
    "        for t in range(hidden_sizes[l]):\n",
    "            C_arr[t] = sigma_b_sq[l]\n",
    "        C[1].append(C_arr)\n",
    "        C_root[1].append(C_arr**(1/2))\n",
    "    C_arr = np.ones(model[2*n_layer].bias.shape)\n",
    "    for t in range(output_size):\n",
    "        C_arr[t] = sigma_b_sq[n_layer]\n",
    "    C[1].append(C_arr)\n",
    "    C_root[1].append(C_arr**(1/2))\n",
    "\n",
    "    # Sample from the prior to see what a sample looks like\n",
    "    xi = sample_prior()\n",
    "    func_plot(xi,'figs/MC/NN_'+hyps+'_a_prior_sample2')\n",
    "\n",
    "    # Create environment \n",
    "    env = gym.make('MountainCar-v0')\n",
    "    env = env.unwrapped\n",
    "\n",
    "    data = np.load('MC_data.npy')\n",
    "\n",
    "    for N_data in [50]: \n",
    "\n",
    "        ''' run pCN '''\n",
    "        method = 'pCN'\n",
    "        stochastic_gradients = False\n",
    "        unadjusted = False\n",
    "        beta =  1/12\n",
    "        try:\n",
    "            xi = [[],[]]\n",
    "            for l in range(n_layer+1):\n",
    "                xi[0].append(np.load('np_saved/MC/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_lastSample_w'+str(l)+'.npy'))\n",
    "                xi[1].append(np.load('np_saved/MC/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_lastSample_b'+str(l)+'.npy'))\n",
    "        except FileNotFoundError:\n",
    "            print('\\nStarting from close to 0')\n",
    "            xi = sample_prior(0.1)\n",
    "        MCMC(xi,N_data,data[0:N_data,:],t_max) \n",
    "        print('Number of layers: '+str(int(n_layer))+', number of nodes per layer: '+str(int(hidden_size)))\n",
    "\n",
    "    env.close()   \n",
    "\n",
    "np.save('MC_dim_independence_acceptances2.npy',all_acceptances)\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "ax.set_title('Acceptance ratio by layer width')\n",
    "ax.plot([10,20,30,40,50,60,70,80,90,100],all_acceptances)\n",
    "\n",
    "fig.savefig('figs/NN_dim_independence2.pdf', dpi=300)\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Respective acceptances and total number of parameters for the different layer widths:\n",
      "10:   21.24%     261\n",
      "20:   15.01%     921\n",
      "30:   10.92%     1981\n",
      "40:   8.52%     3441\n",
      "50:   6.81%     5301\n",
      "60:   5.47%     7561\n",
      "70:   4.25%     10221\n",
      "80:   3.91%     13281\n",
      "90:   2.97%     16741\n",
      "100:  2.23%     20601\n"
     ]
    }
   ],
   "source": [
    "all_acceptances = np.load('MC_dim_independence_acceptances2.npy')\n",
    "print('\\nRespective acceptances and total number of parameters for the different layer widths:')\n",
    "for N_l in [10,20,30,40,50,60,70,80,90,100]:\n",
    "    number_w = N_l+2*(N_l**2)+2*N_l\n",
    "    number_b = 1+3*N_l\n",
    "    if N_l==100:\n",
    "        print(str(N_l)+':  '+str(round(all_acceptances[int(N_l/10-1)]*100,2))+'%'+'     '+str(number_w+number_b))\n",
    "    else:\n",
    "        print(str(N_l)+':   '+str(round(all_acceptances[int(N_l/10-1)]*100,2))+'%'+'     '+str(number_w+number_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
