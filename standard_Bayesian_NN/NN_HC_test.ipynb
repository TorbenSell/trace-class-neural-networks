{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import homework.hw1.load_policy as load_policy\n",
    "import homework.hw1.tf_util as tf_util\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "\n",
    "sigma = 0.1\n",
    "\n",
    "dim = 17\n",
    "dim_act = 6\n",
    "obs_min = np.array([-0.25,-0.8 ,-0.7 ,-0.7,-0.65,-0.75,-0.95,-0.6 ,-1.5,-3.1,-7.1,-20,-24,-27,-27,-30,-20])\n",
    "obs_max = np.array([ 0.4 , 1.65, 0.95, 0.9, 0.95, 0.95, 1.1 , 0.75, 8  , 3.4, 7  , 19, 25, 22, 25, 32, 26])\n",
    "\n",
    "act_norm_init = np.array([2.3,2.1, 1.85,1.8 , 1.5,2.0, 2.1,2.65, 2.3,2.5, 1.9,2.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "INITIALISE ACTION AND OBSERVATION SPACE\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "actions = []\n",
    "actions_matrix = np.load('HC_actions.npy')\n",
    "N_a = 8\n",
    "for i in range(N_a):\n",
    "    actions.append(actions_matrix[i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "NEEDED FUNCTIONS\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "''' u(x), which evaluates the function u at x=(pos,speed) with the currently set xi '''\n",
    "def u(x):\n",
    "    x_transformed = np.zeros(dim)\n",
    "    for i in range(dim):\n",
    "        x_transformed[i] = ((x[i]-obs_min[i])/(obs_max[i]-obs_min[i])-1/2)*2\n",
    "    value = model_list[it](torch.from_numpy(x_transformed).float())\n",
    "    return value\n",
    "    \n",
    "def policy(obs):\n",
    "    return policy_fn(obs[None,:])\n",
    "print('loading and building expert policy')\n",
    "policy_fn = load_policy.load_policy('/Users/torbensell/Dropbox/UNI/CAM/PhD/Programme/BIRL_BNN_pCN/homework/hw1/experts/HalfCheetah-v2.pkl')\n",
    "print('loaded and built')\n",
    "\n",
    "''' Progress bar to know how much longer one has to wait '''\n",
    "def progressBar(value, endvalue, bar_length=40):\n",
    "    percent = float(value) / endvalue\n",
    "    arrow = '-' * int(round(percent * bar_length)-1) + '>'\n",
    "    spaces = ' ' * (bar_length - len(arrow))\n",
    "    sys.stdout.write(\"\\rPercent: [{0}] {1}%\".format(arrow + spaces, int(round(percent * 100))))\n",
    "    sys.stdout.flush() \n",
    "    \n",
    "def discrete_action(action_vector):\n",
    "    if np.random.uniform()<0.01:\n",
    "        ''' With 1% probability, return a random action '''\n",
    "        rand = np.random.randint(0,N_a)\n",
    "        return rand,actions[rand]\n",
    "    else:\n",
    "        distances = np.zeros(N_a)\n",
    "        for i in range(N_a):\n",
    "            distances[i] = np.linalg.norm(action_vector-actions[i])\n",
    "        action = np.argmin(distances) # index of action\n",
    "        return action,actions[action]\n",
    "    \n",
    "def discrete_action_no_noise(action_vector):\n",
    "    distances = np.zeros(N_a)\n",
    "    for i in range(N_a):\n",
    "        distances[i] = np.linalg.norm(action_vector-actions[i])\n",
    "    action = np.argmin(distances) # index of action\n",
    "    return action,actions[action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "TEST 1 (pCN) - check if policy learned behaviour \n",
    "\n",
    "\"\"\"  \n",
    "\n",
    "distances = [[],[]] # stores distances\n",
    "\n",
    "\"\"\" initialise NNs - pCN \"\"\"\n",
    "\n",
    "method = 'pCN'\n",
    "N_data = 100\n",
    "\n",
    "model_list = []\n",
    "\n",
    "# Hyperparameters for our network\n",
    "input_size = dim\n",
    "n_layer = 3\n",
    "hidden_sizes = [10,10,10]\n",
    "output_size = 1\n",
    "\n",
    "# Store hyperparameters in string\n",
    "hyps = str(hidden_sizes[0])\n",
    "for i in range(1,n_layer):\n",
    "    hyps = hyps+'_'+str(hidden_sizes[i])\n",
    "\n",
    "for it in range(1000):\n",
    "    # Build a feed-forward network\n",
    "    model_list.append(nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                          nn.Tanh(),\n",
    "                          nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                          nn.Tanh(),\n",
    "                          nn.Linear(hidden_sizes[1], hidden_sizes[2]),\n",
    "                          nn.Tanh(),\n",
    "                          nn.Linear(hidden_sizes[-1], output_size)))\n",
    "\n",
    "    # Get weights\n",
    "    weights = []\n",
    "    biases = []\n",
    "    for l in range(n_layer+1):\n",
    "        weights.append(np.load('np_saved/HC/samples_policy_learning/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_w'+str(l)+'_sampleNo'+str(it)+'.npy'))\n",
    "        biases.append(np.load('np_saved/HC/samples_policy_learning/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_b'+str(l)+'_sampleNo'+str(it)+'.npy'))\n",
    "        \n",
    "    # Set all weights to 0\n",
    "    for l in range(n_layer+1):\n",
    "        model_list[it][2*l].weight = torch.nn.Parameter(torch.from_numpy(weights[l]).float(), requires_grad=False)\n",
    "        model_list[it][2*l].bias = torch.nn.Parameter(torch.from_numpy(biases[l]).float(), requires_grad=False)\n",
    "\n",
    "        \n",
    "\"\"\" run tests - pCN \"\"\" \n",
    "        \n",
    "v_with_noise = np.zeros(N_a)\n",
    "\n",
    "print('Test started: ' + str(time.ctime()))\n",
    "with tf.Session():\n",
    "    tf_util.initialize()   \n",
    "    env = gym.make('HalfCheetah-v2')\n",
    "    env.unwrapped\n",
    "    \n",
    "    for _ in range(10):\n",
    "        for N_data in [100]:\n",
    "            obs = env.reset()\n",
    "            errors = 0  # to keep track of how many moves would be the same if using true policy\n",
    "            corrects = 0\n",
    "\n",
    "            ''' Store values taken to reproduce behaviour '''\n",
    "            poss = []\n",
    "            vels = []\n",
    "            for k in range(100):\n",
    "                progressBar(k+1,100)\n",
    "\n",
    "                poss.append(obs[0:8])\n",
    "                vels.append(obs[8:17])\n",
    "                poss[k] = np.insert(poss[k],0, env.sim.data.qpos[0] )\n",
    "\n",
    "                ''' See where different actions would take us, evaluate mean of value function samples there '''\n",
    "                v = np.zeros(N_a)\n",
    "                for it in range(1000): # Iterate through different neural networks\n",
    "                    for i in range(N_a):\n",
    "                        x = env.step(actions[i])[0]\n",
    "                        v[i] += u(x).detach().numpy()[0]\n",
    "                        env.set_state(poss[k], vels[k])\n",
    "                v = v/1000 # to get mean of value function evaluations\n",
    "\n",
    "                ''' Pick action which maximises the mean value function (plus noise) at the new location '''\n",
    "                v_with_noise = v+sigma*np.random.normal(np.zeros(N_a),np.ones(N_a)) \n",
    "                a_argmax_with_noise = np.argmax(v_with_noise)\n",
    "                \n",
    "                ''' Check if action was optimal according to true policy; move according to learned behaviour  '''\n",
    "                if a_argmax_with_noise != discrete_action_no_noise(policy(obs))[0]:\n",
    "                    errors+=1\n",
    "                else:\n",
    "                    corrects+=1\n",
    "                obs = env.step(actions[a_argmax_with_noise])[0] \n",
    "\n",
    "            print('\\nErrorrate = ',(errors/(errors+corrects)))\n",
    "#             input(\"Press Enter to start visualisation...\")\n",
    "\n",
    "            ''' Visualise the cheetah '''\n",
    "            env.set_state(poss[0], vels[0])\n",
    "            start = env.sim.data.qpos[0]       \n",
    "            for k in range(100):\n",
    "#                 env.render()\n",
    "                env.set_state(poss[k], vels[k])\n",
    "\n",
    "            print('Distance covered by Cheetah = ' + str(env.sim.data.qpos[0]-start) + '\\n')\n",
    "            distances[0].append(env.sim.data.qpos[0]-start)\n",
    "    \n",
    "print('pCN done, '+ str(time.ctime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "TEST 1 (pCNL) - check if policy learned behaviour \n",
    "\n",
    "\"\"\"  \n",
    "\n",
    "\n",
    "\"\"\" initialise NNs - pCNL \"\"\"\n",
    "\n",
    "method = 'pCNL'\n",
    "N_data = 100\n",
    "\n",
    "model_list = []\n",
    "\n",
    "# Hyperparameters for our network\n",
    "input_size = dim\n",
    "n_layer = 3\n",
    "hidden_sizes = [10,10,10]\n",
    "output_size = 1\n",
    "\n",
    "# Store hyperparameters in string\n",
    "hyps = str(hidden_sizes[0])\n",
    "for i in range(1,n_layer):\n",
    "    hyps = hyps+'_'+str(hidden_sizes[i])\n",
    "\n",
    "for it in range(1000):\n",
    "    # Build a feed-forward network\n",
    "    model_list.append(nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                          nn.Tanh(),\n",
    "                          nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                          nn.Tanh(),\n",
    "                          nn.Linear(hidden_sizes[1], hidden_sizes[2]),\n",
    "                          nn.Tanh(),\n",
    "                          nn.Linear(hidden_sizes[-1], output_size)))\n",
    "\n",
    "    # Get weights\n",
    "    weights = []\n",
    "    biases = []\n",
    "    for l in range(n_layer+1):\n",
    "        weights.append(np.load('np_saved/HC/samples_policy_learning/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_w'+str(l)+'_sampleNo'+str(it)+'.npy'))\n",
    "        biases.append(np.load('np_saved/HC/samples_policy_learning/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_b'+str(l)+'_sampleNo'+str(it)+'.npy'))\n",
    "        \n",
    "    # Set all weights to 0\n",
    "    for l in range(n_layer+1):\n",
    "        model_list[it][2*l].weight = torch.nn.Parameter(torch.from_numpy(weights[l]).float(), requires_grad=False)\n",
    "        model_list[it][2*l].bias = torch.nn.Parameter(torch.from_numpy(biases[l]).float(), requires_grad=False)\n",
    "\n",
    "        \n",
    "\"\"\" run tests - pCNL \"\"\" \n",
    "        \n",
    "v_with_noise = np.zeros(N_a)\n",
    "\n",
    "print('Test started: ' + str(time.ctime()))\n",
    "with tf.Session():\n",
    "    tf_util.initialize()   \n",
    "    env = gym.make('HalfCheetah-v2')\n",
    "    env.unwrapped\n",
    "    \n",
    "    for _ in range(10):\n",
    "        for N_data in [100]:\n",
    "            obs = env.reset()\n",
    "            errors = 0  # to keep track of how many moves would be the same if using true policy\n",
    "            corrects = 0\n",
    "\n",
    "            ''' Store values taken to reproduce behaviour '''\n",
    "            poss = []\n",
    "            vels = []\n",
    "            for k in range(100):\n",
    "                progressBar(k+1,100)\n",
    "\n",
    "                poss.append(obs[0:8])\n",
    "                vels.append(obs[8:17])\n",
    "                poss[k] = np.insert(poss[k],0, env.sim.data.qpos[0] )\n",
    "\n",
    "                ''' See where different actions would take us, evaluate mean of value function samples there '''\n",
    "                v = np.zeros(N_a)\n",
    "                for it in range(1000): # Iterate through different neural networks\n",
    "                    for i in range(N_a):\n",
    "                        x = env.step(actions[i])[0]\n",
    "                        v[i] += u(x).detach().numpy()[0]\n",
    "                        env.set_state(poss[k], vels[k])\n",
    "                v = v/1000 # to get mean of value function evaluations\n",
    "\n",
    "                ''' Pick action which maximises the mean value function (plus noise) at the new location '''\n",
    "                v_with_noise = v+sigma*np.random.normal(np.zeros(N_a),np.ones(N_a)) \n",
    "                a_argmax_with_noise = np.argmax(v_with_noise)\n",
    "                \n",
    "                ''' Check if action was optimal according to true policy; move according to learned behaviour  '''\n",
    "                if a_argmax_with_noise != discrete_action_no_noise(policy(obs))[0]:\n",
    "                    errors+=1\n",
    "                else:\n",
    "                    corrects+=1\n",
    "                obs = env.step(actions[a_argmax_with_noise])[0] \n",
    "\n",
    "            print('\\nErrorrate = ',(errors/(errors+corrects)))\n",
    "#             input(\"Press Enter to start visualisation...\")\n",
    "\n",
    "            ''' Visualise the cheetah '''\n",
    "            env.set_state(poss[0], vels[0])\n",
    "            start = env.sim.data.qpos[0]       \n",
    "            for k in range(100):\n",
    "#                 env.render()\n",
    "                env.set_state(poss[k], vels[k])\n",
    "\n",
    "            print('Distance covered by Cheetah = ' + str(env.sim.data.qpos[0]-start) + '\\n')   \n",
    "            distances[1].append(env.sim.data.qpos[0]-start)\n",
    "            \n",
    "print('pCNL done, '+ str(time.ctime()))\n",
    "np.save('BNN_dist.npy',distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "TEST 2 - check how often mean of posterior function predicts the correct value\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" initialise NNs - pCN \"\"\"\n",
    "\n",
    "method = 'pCN'\n",
    "N_data = 100\n",
    "\n",
    "model_list = []\n",
    "\n",
    "# Hyperparameters for our network\n",
    "input_size = dim\n",
    "n_layer = 3\n",
    "hidden_sizes = [10,10,10]\n",
    "output_size = 1\n",
    "\n",
    "# Store hyperparameters in string\n",
    "hyps = str(hidden_sizes[0])\n",
    "for i in range(1,n_layer):\n",
    "    hyps = hyps+'_'+str(hidden_sizes[i])\n",
    "\n",
    "for it in range(1000):\n",
    "    # Build a feed-forward network\n",
    "    model_list.append(nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                          nn.Tanh(),\n",
    "                          nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                          nn.Tanh(),\n",
    "                          nn.Linear(hidden_sizes[1], hidden_sizes[2]),\n",
    "                          nn.Tanh(),\n",
    "                          nn.Linear(hidden_sizes[-1], output_size)))\n",
    "\n",
    "    # Get weights\n",
    "    weights = []\n",
    "    biases = []\n",
    "    for l in range(n_layer+1):\n",
    "        weights.append(np.load('np_saved/HC/samples_policy_learning/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_w'+str(l)+'_sampleNo'+str(it)+'.npy'))\n",
    "        biases.append(np.load('np_saved/HC/samples_policy_learning/NN_'+hyps+'_'+method+'_NData'+str(N_data)+'_b'+str(l)+'_sampleNo'+str(it)+'.npy'))\n",
    "        \n",
    "    # Set all weights to 0\n",
    "    for l in range(n_layer+1):\n",
    "        model_list[it][2*l].weight = torch.nn.Parameter(torch.from_numpy(weights[l]).float(), requires_grad=False)\n",
    "        model_list[it][2*l].bias = torch.nn.Parameter(torch.from_numpy(biases[l]).float(), requires_grad=False)\n",
    "\n",
    "\n",
    "\"\"\" run tests \"\"\"\n",
    "\n",
    "x_test_100 = np.load('HC_x_test_100.npy')\n",
    "a_test_100 = np.load('HC_a_test_100.npy')\n",
    "optimal_choice = 0\n",
    "not_optimal_choice = 0\n",
    "\n",
    "start = time.time()\n",
    "for j in range(100):\n",
    "    v = np.zeros(N_a)\n",
    "    \n",
    "    for it in range(1000):\n",
    "        for i in range(N_a):\n",
    "            v[i] += u(x_test_100[i,:,j]).detach().numpy()[0]\n",
    "            \n",
    "    if a_test_100[j]==np.argmax(v):\n",
    "        optimal_choice += 1\n",
    "    else:\n",
    "        not_optimal_choice += 1\n",
    "            \n",
    "print('Correct choices: ',optimal_choice/(optimal_choice+not_optimal_choice))\n",
    "print('Wrong choices: ',not_optimal_choice/(optimal_choice+not_optimal_choice))\n",
    "print('Time: ',time.time()-start)\n",
    "print(str(time.ctime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session():\n",
    "    tf_util.initialize()   \n",
    "    env = gym.make('HalfCheetah-v2')\n",
    "    env.unwrapped\n",
    "    \n",
    "    for _ in range(10):\n",
    "#         input(\"Press Enter to start visualisation...\")\n",
    "        obs = env.reset()\n",
    "        start = env.sim.data.qpos[0]    \n",
    "        for k in range(100):\n",
    "#             env.render()\n",
    "            obs = env.step(discrete_action_no_noise(policy(obs))[1])[0] \n",
    "        print('Distance covered by Cheetah = ' + str(env.sim.data.qpos[0]-start) + '\\n')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
