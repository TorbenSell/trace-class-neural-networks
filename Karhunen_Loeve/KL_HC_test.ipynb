{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import homework.hw1.load_policy as load_policy\n",
    "import homework.hw1.tf_util as tf_util\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import numba\n",
    "import time\n",
    "\n",
    "sigma = 0.1\n",
    "\n",
    "dim = 17\n",
    "dim_act = 6\n",
    "obs_min = np.array([-0.25,-0.8 ,-0.7 ,-0.7,-0.65,-0.75,-0.95,-0.6 ,-1.5,-3.1,-7.1,-20,-24,-27,-27,-30,-20])\n",
    "obs_max = np.array([ 0.4 , 1.65, 0.95, 0.9, 0.95, 0.95, 1.1 , 0.75, 8  , 3.4, 7  , 19, 25, 22, 25, 32, 26])\n",
    "\n",
    "act_norm_init = np.array([2.3,2.1, 1.85,1.8 , 1.5,2.0, 2.1,2.65, 2.3,2.5, 1.9,2.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "INITIALISE ACTION AND OBSERVATION SPACE\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "actions = []\n",
    "actions_matrix = np.load('HC_actions.npy')\n",
    "N_a = 8\n",
    "for i in range(N_a):\n",
    "    actions.append(actions_matrix[i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "NEEDED FUNCTIONS\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "''' u(xi,x), which evaluates the function u with coefficients xi at x, fast version '''\n",
    "@numba.njit()\n",
    "def u(xi,x):\n",
    "    u_sum = 0\n",
    "    for j in range(dim):\n",
    "        \n",
    "        ''' 1D terms '''\n",
    "        j_eval = 2*np.pi*(x[j]-obs_min[j])/(obs_max[j]-obs_min[j])\n",
    "        for i in range(1,N_trunc):\n",
    "            c_j = 2/(obs_max[j]-obs_min[j])\n",
    "            u_sum += c_j*xi[2*i-2,j,0]*np.cos(i*j_eval)\n",
    "            u_sum += c_j*xi[2*i-1,j,0]*np.sin(i*j_eval)\n",
    "            \n",
    "        ''' 2D terms '''   \n",
    "        for k in range(dim):\n",
    "            if k>j:\n",
    "                j_eval = 2*np.pi*(x[j]-obs_min[j])/(obs_max[j]-obs_min[j])\n",
    "                k_eval = 2*np.pi*(x[k]-obs_min[k])/(obs_max[k]-obs_min[k])\n",
    "                for i_1 in range(1,N_trunc):\n",
    "                    for i_2 in range(1,N_trunc):\n",
    "                        c_jk = 4/(obs_max[j]-obs_min[j])/(obs_max[k]-obs_min[k])\n",
    "                        u_sum += c_jk*xi[4*(i_1+N_trunc*i_2)-4,j,k]*np.cos(i_1*j_eval)*np.cos(i_2*k_eval)\n",
    "                        u_sum += c_jk*xi[4*(i_1+N_trunc*i_2)-3,j,k]*np.cos(i_1*j_eval)*np.sin(i_2*k_eval)\n",
    "                        u_sum += c_jk*xi[4*(i_1+N_trunc*i_2)-2,j,k]*np.sin(i_1*j_eval)*np.cos(i_2*k_eval)\n",
    "                        u_sum += c_jk*xi[4*(i_1+N_trunc*i_2)-1,j,k]*np.sin(i_1*j_eval)*np.sin(i_2*k_eval)\n",
    "    return u_sum\n",
    "    \n",
    "def policy(obs):\n",
    "    return policy_fn(obs[None,:])\n",
    "print('loading and building expert policy')\n",
    "policy_fn = load_policy.load_policy('/Users/torbensell/Dropbox (Cambridge University)/UNI/CAM/PhD/Programme/BIRL_NN_pCN/homework/hw1/experts/HalfCheetah-v2.pkl')\n",
    "print('loaded and built')\n",
    "\n",
    "''' Progress bar to know how much longer one has to wait '''\n",
    "def progressBar(value, endvalue, bar_length=40):\n",
    "    percent = float(value) / endvalue\n",
    "    arrow = '-' * int(round(percent * bar_length)-1) + '>'\n",
    "    spaces = ' ' * (bar_length - len(arrow))\n",
    "    sys.stdout.write(\"\\rPercent: [{0}] {1}%\".format(arrow + spaces, int(round(percent * 100))))\n",
    "    sys.stdout.flush() \n",
    "    \n",
    "def discrete_action(action_vector):\n",
    "    if np.random.uniform()<0.01:\n",
    "        ''' With 1% probability, return a random action '''\n",
    "        rand = np.random.randint(0,N_a)\n",
    "        return rand,actions[rand]\n",
    "    else:\n",
    "        distances = np.zeros(N_a)\n",
    "        for i in range(N_a):\n",
    "            distances[i] = np.linalg.norm(action_vector-actions[i])\n",
    "        action = np.argmin(distances) # index of action\n",
    "        return action,actions[action]\n",
    "    \n",
    "def discrete_action_no_noise(action_vector):\n",
    "    distances = np.zeros(N_a)\n",
    "    for i in range(N_a):\n",
    "        distances[i] = np.linalg.norm(action_vector-actions[i])\n",
    "    action = np.argmin(distances) # index of action\n",
    "    return action,actions[action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "TEST 1 (pCN) - check if policy learned behaviour \n",
    "\n",
    "\"\"\"  \n",
    "\n",
    "\n",
    "distances = [[],[],[]] # stores distances\n",
    "\n",
    "\"\"\" run optimal policy to store distances using optimal policy \"\"\"\n",
    "print('Test started: ' + str(time.ctime()))\n",
    "with tf.Session():\n",
    "    tf_util.initialize()   \n",
    "    env = gym.make('HalfCheetah-v2')\n",
    "    env.unwrapped\n",
    "    \n",
    "    for _ in range(10):\n",
    "#         input(\"Press Enter to start visualisation...\")\n",
    "        obs = env.reset()\n",
    "        start = env.sim.data.qpos[0]    \n",
    "        for k in range(100):\n",
    "#             env.render()\n",
    "            obs = env.step(discrete_action_no_noise(policy(obs))[1])[0] \n",
    "        print('Distance covered by Cheetah = ' + str(env.sim.data.qpos[0]-start) + '\\n')    \n",
    "        distances[0].append(env.sim.data.qpos[0]-start)\n",
    "print('optimal policy done')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" load samples - pCN \"\"\"\n",
    "\n",
    "method = 'pCN'\n",
    "N_data = 100\n",
    "N_trunc = 5\n",
    "\n",
    "xi = []\n",
    "for it in range(1000):\n",
    "    xi.append(np.load('np_saved/HC/samples_policy_learning/KL_'+str(N_trunc)+'_'+method+'_NData'+str(N_data)+'_sampleNo'+str(it)+'.npy'))\n",
    "                    \n",
    "        \n",
    "\"\"\" run tests - pCN \"\"\" \n",
    "\n",
    "v_a = np.zeros(N_a)\n",
    "v_a_with_noise = np.zeros(N_a)\n",
    "    \n",
    "print('\\n\\nTest started: ' + str(time.ctime()))\n",
    "with tf.Session():\n",
    "    tf_util.initialize()   \n",
    "    env = gym.make('HalfCheetah-v2')\n",
    "    env.unwrapped\n",
    "    \n",
    "    for _ in range(10):\n",
    "        for N_data in [100]:\n",
    "            obs = env.reset()\n",
    "            errors = 0  # to keep track of how many moves would be the same if using true policy\n",
    "            corrects = 0\n",
    "            \n",
    "            ''' Store values taken to reproduce behaviour '''\n",
    "            poss = []\n",
    "            vels = []\n",
    "            for k in range(100):\n",
    "                progressBar(k+1,100)\n",
    "\n",
    "                poss.append(obs[0:8])\n",
    "                vels.append(obs[8:17])\n",
    "                poss[k] = np.insert(poss[k],0, env.sim.data.qpos[0] )\n",
    "                \n",
    "                ''' See where different actions would take us, evaluate value function there '''\n",
    "                v = np.zeros(N_a)\n",
    "                for it in range(1000):\n",
    "                    for i in range(N_a):\n",
    "                        x = env.step(actions[i])[0]\n",
    "                        v[i] += u(xi[it],x)\n",
    "                        env.set_state(poss[k], vels[k])\n",
    "                        \n",
    "                v = v/1000 # to get mean of value function evaluations\n",
    "                        \n",
    "                ''' Pick action which maximises value function (plus noise) at the new location '''\n",
    "                v_with_noise = v+sigma*np.random.normal(np.zeros(N_a),np.ones(N_a)) \n",
    "                a_argmax_with_noise = np.argmax(v_with_noise)\n",
    "\n",
    "                ''' Check if action was optimal according to true policy; move according to learned behaviour  '''\n",
    "                if a_argmax_with_noise != discrete_action_no_noise(policy(obs))[0]:\n",
    "                    errors+=1\n",
    "                else:\n",
    "                    corrects+=1\n",
    "                obs = env.step(actions[a_argmax_with_noise])[0] \n",
    "\n",
    "            print('\\nErrorrate = ',(errors/(errors+corrects)))\n",
    "#             input(\"Press Enter to start visualisation...\")\n",
    "\n",
    "            ''' Visualise the cheetah '''\n",
    "            env.set_state(poss[0], vels[0])\n",
    "            start = env.sim.data.qpos[0]       \n",
    "            for k in range(100):\n",
    "#                 env.render()\n",
    "                env.set_state(poss[k], vels[k])\n",
    "\n",
    "            print('\\nDistance covered by Cheetah = ' + str(env.sim.data.qpos[0]-start))    \n",
    "            distances[1].append(env.sim.data.qpos[0]-start)\n",
    "        \n",
    "print('\\npCN done, '+ str(time.ctime()))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" load samples - pCNL \"\"\"\n",
    "\n",
    "method = 'pCNL'\n",
    "N_data = 100\n",
    "N_trunc = 5\n",
    "\n",
    "xi = []\n",
    "for it in range(1000):\n",
    "    xi.append(np.load('np_saved/HC/samples_policy_learning/KL_'+str(N_trunc)+'_'+method+'_NData'+str(N_data)+'_sampleNo'+str(it)+'.npy'))\n",
    "                    \n",
    "        \n",
    "\"\"\" run tests - pCNL \"\"\" \n",
    "\n",
    "v_a = np.zeros(N_a)\n",
    "v_a_with_noise = np.zeros(N_a)\n",
    "    \n",
    "print('\\n\\nTest started: ' + str(time.ctime()))\n",
    "with tf.Session():\n",
    "    tf_util.initialize()   \n",
    "    env = gym.make('HalfCheetah-v2')\n",
    "    env.unwrapped\n",
    "    \n",
    "    for _ in range(10):\n",
    "        for N_data in [100]:\n",
    "            obs = env.reset()\n",
    "            errors = 0  # to keep track of how many moves would be the same if using true policy\n",
    "            corrects = 0\n",
    "            \n",
    "            ''' Store values taken to reproduce behaviour '''\n",
    "            poss = []\n",
    "            vels = []\n",
    "            for k in range(100):\n",
    "                progressBar(k+1,100)\n",
    "\n",
    "                poss.append(obs[0:8])\n",
    "                vels.append(obs[8:17])\n",
    "                poss[k] = np.insert(poss[k],0, env.sim.data.qpos[0] )\n",
    "                \n",
    "                ''' See where different actions would take us, evaluate value function there '''\n",
    "                v = np.zeros(N_a)\n",
    "                for it in range(1000):\n",
    "                    for i in range(N_a):\n",
    "                        x = env.step(actions[i])[0]\n",
    "                        v[i] += u(xi[it],x)\n",
    "                        env.set_state(poss[k], vels[k])\n",
    "                        \n",
    "                v = v/1000 # to get mean of value function evaluations\n",
    "                        \n",
    "                ''' Pick action which maximises value function (plus noise) at the new location '''\n",
    "                v_with_noise = v+sigma*np.random.normal(np.zeros(N_a),np.ones(N_a)) \n",
    "                a_argmax_with_noise = np.argmax(v_with_noise)\n",
    "\n",
    "                ''' Check if action was optimal according to true policy; move according to learned behaviour  '''\n",
    "                if a_argmax_with_noise != discrete_action_no_noise(policy(obs))[0]:\n",
    "                    errors+=1\n",
    "                else:\n",
    "                    corrects+=1\n",
    "                obs = env.step(actions[a_argmax_with_noise])[0] \n",
    "\n",
    "            print('\\nErrorrate = ',(errors/(errors+corrects)))\n",
    "#             input(\"Press Enter to start visualisation...\")\n",
    "\n",
    "            ''' Visualise the cheetah '''\n",
    "            env.set_state(poss[0], vels[0])\n",
    "            start = env.sim.data.qpos[0]       \n",
    "            for k in range(100):\n",
    "#                 env.render()\n",
    "                env.set_state(poss[k], vels[k])\n",
    "\n",
    "            print('\\nDistance covered by Cheetah = ' + str(env.sim.data.qpos[0]-start))    \n",
    "            distances[2].append(env.sim.data.qpos[0]-start)\n",
    "        \n",
    "print('\\npCNL done, '+ str(time.ctime()))\n",
    "\n",
    "\n",
    "np.save('KL_dist.npy',distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "TEST 2 - check how often mean of posterior function predicts the correct value\n",
    "\n",
    "\"\"\"  \n",
    "\n",
    "\n",
    "\"\"\" load samples - pCN \"\"\"\n",
    "\n",
    "method = 'pCN'\n",
    "N_data = 100  \n",
    "N_trunc = 5\n",
    "\n",
    "xi = []\n",
    "for it in range(1000):\n",
    "    xi.append(np.load('np_saved/HC/samples_policy_learning/KL_'+str(N_trunc)+'_'+method+'_NData'+str(N_data)+'_sampleNo'+str(it)+'.npy'))\n",
    "\n",
    "\n",
    "\"\"\" run tests \"\"\"\n",
    "\n",
    "x_test_100 = np.load('HC_x_test_100.npy')\n",
    "a_test_100 = np.load('HC_a_test_100.npy')\n",
    "optimal_choice = 0\n",
    "not_optimal_choice = 0\n",
    "\n",
    "start = time.time()\n",
    "for j in range(100):\n",
    "    v = np.zeros(N_a)\n",
    "    \n",
    "    for it in range(1000):\n",
    "        for i in range(N_a):\n",
    "            v[i] += u(xi[it],x_test_100[i,:,j])\n",
    "            \n",
    "    if a_test_100[j]==np.argmax(v):\n",
    "        optimal_choice += 1\n",
    "    else:\n",
    "        not_optimal_choice += 1\n",
    "            \n",
    "print('Correct choices: ',optimal_choice/(optimal_choice+not_optimal_choice))\n",
    "print('Wrong choices: ',not_optimal_choice/(optimal_choice+not_optimal_choice))\n",
    "print('Time: ',time.time()-start)\n",
    "print(str(time.ctime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
