{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import gym\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numba\n",
    "import sys\n",
    "import time\n",
    "\n",
    "N_a = 3                             # number of possible actions (actions are -1, 0, and 1)\n",
    "sigma = 0.1                         # noise\n",
    "dim = 2                             # dimensionality of the space (v: R^d --> R)\n",
    "pCNL = False\n",
    "obs_min = np.array([-1.2,-0.07])\n",
    "obs_max = np.array([ 0.6, 0.07])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "FUNCTIONS - VALUE FUNCTION AND POLICIES\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "''' Progress bar to know how much longer one has to wait '''\n",
    "def progressBar(value, endvalue, bar_length=40):\n",
    "    percent = float(value) / endvalue\n",
    "    arrow = '-' * int(round(percent * bar_length)-1) + '>'\n",
    "    spaces = ' ' * (bar_length - len(arrow))\n",
    "    sys.stdout.write(\"\\rPercent: [{0}] {1}%\".format(arrow + spaces, int(round(percent * 100))))\n",
    "    sys.stdout.flush() \n",
    "    \n",
    "''' u(xi,x), which evaluates the function u with coefficients xi at x=(pos,speed) --- new, complicated, but fast version '''\n",
    "@numba.njit()\n",
    "def u(xi,x):\n",
    "    u_sum = 0\n",
    "    for j in range(dim):\n",
    "        \n",
    "        ''' 1D terms '''\n",
    "        j_eval = 2*np.pi*(x[j]-obs_min[j])/(obs_max[j]-obs_min[j])\n",
    "        for i in range(1,N_trunc):\n",
    "            c_j = 2/(obs_max[j]-obs_min[j])\n",
    "            u_sum += c_j*xi[2*i-2,j,0]*np.cos(i*j_eval)\n",
    "            u_sum += c_j*xi[2*i-1,j,0]*np.sin(i*j_eval)\n",
    "            \n",
    "        ''' 2D terms '''   \n",
    "        for k in range(dim):\n",
    "            if k>j:\n",
    "                j_eval = 2*np.pi*(x[j]-obs_min[j])/(obs_max[j]-obs_min[j])\n",
    "                k_eval = 2*np.pi*(x[k]-obs_min[k])/(obs_max[k]-obs_min[k])\n",
    "                for i_1 in range(1,N_trunc):\n",
    "                    for i_2 in range(1,N_trunc):\n",
    "                        c_jk = 4/(obs_max[j]-obs_min[j])/(obs_max[k]-obs_min[k])\n",
    "                        u_sum += c_jk*xi[4*(i_1+N_trunc*i_2)-4,j,k]*np.cos(i_1*j_eval)*np.cos(i_2*k_eval)\n",
    "                        u_sum += c_jk*xi[4*(i_1+N_trunc*i_2)-3,j,k]*np.cos(i_1*j_eval)*np.sin(i_2*k_eval)\n",
    "                        u_sum += c_jk*xi[4*(i_1+N_trunc*i_2)-2,j,k]*np.sin(i_1*j_eval)*np.cos(i_2*k_eval)\n",
    "                        u_sum += c_jk*xi[4*(i_1+N_trunc*i_2)-1,j,k]*np.sin(i_1*j_eval)*np.sin(i_2*k_eval)\n",
    "    return u_sum\n",
    "''' Policy from \"Reinforcement Learning: Theory and {Python} Implementation\" '''\n",
    "def policy(position,velocity):\n",
    "        lb = min(-0.09 * (position + 0.25) ** 2 + 0.03,\n",
    "                0.3 * (position + 0.9) ** 4 - 0.008)\n",
    "        ub = -0.07 * (position + 0.38) ** 2 + 0.07\n",
    "        if lb < velocity < ub:\n",
    "            action = 2 # push right\n",
    "        else:\n",
    "            action = 0 # push left\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN PROGRAMME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "MAIN PROGRAMME - SHOW LEARNED BEHAVIOUR\n",
    "\n",
    "\"\"\"   \n",
    "   \n",
    "env = gym.make('MountainCar-v0')\n",
    "env = env.unwrapped\n",
    "\n",
    "failures = np.zeros(3)\n",
    "steps = [[],[],[]]\n",
    "\n",
    "\n",
    "'''\n",
    "Behaviour following the optimal policy\n",
    "'''\n",
    "for _ in range(100):\n",
    "    pos_curr,speed_curr = env.reset()\n",
    "    for k in range(200):\n",
    "        pos_curr,speed_curr = env.step(policy(pos_curr,speed_curr))[0] \n",
    "        \n",
    "        if pos_curr>0.5:\n",
    "            steps[0].append(k)\n",
    "            break\n",
    "    if k==199:\n",
    "        failures[0]+=1\n",
    "print('Optimal policy done.')\n",
    "s = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" load samples - pCN \"\"\"\n",
    "\n",
    "method = 'pCN'\n",
    "N_data = 50\n",
    "N_trunc = 7\n",
    "\n",
    "xi = []\n",
    "for it in range(1000):\n",
    "    xi.append(np.load('np_saved/MC/samples_policy_learning/KL_'+str(N_trunc)+'_'+method+'_NData'+str(N_data)+'_sampleNo'+str(it)+'.npy'))\n",
    "      \n",
    "\n",
    "\"\"\" run tests - pCN \"\"\"\n",
    "\n",
    "v = np.zeros(N_a)\n",
    "v_with_noise = np.zeros(N_a)\n",
    "\n",
    "errors = 0\n",
    "corrects = 0\n",
    "\n",
    "print('Test started: ' + str(time.ctime()))\n",
    "for _ in range(100):\n",
    "    pos_curr,speed_curr = env.reset()\n",
    "\n",
    "    for k in range(200):\n",
    "        if (k+1)%50==0:\n",
    "            progressBar(k+1,200)\n",
    "\n",
    "        ''' See where different actions would take us, evaluate mean of value function samples there '''\n",
    "        v = np.zeros(N_a)\n",
    "        for it in range(1000): # Iterate through different neural networks\n",
    "            for i in range(N_a):\n",
    "                x = env.step(i)[0]\n",
    "                v[i] += u(xi[it],x)\n",
    "                env.state = [pos_curr,speed_curr]\n",
    "        v = v/1000 # to get mean of value function evaluations\n",
    "\n",
    "        ''' Pick action which maximises the mean value function (plus noise) at the new location '''\n",
    "        v_with_noise = v+np.random.normal(np.zeros(N_a),sigma*np.ones(N_a)) \n",
    "        a_argmax_with_noise = np.argmax(v_with_noise)\n",
    "\n",
    "        ''' Check whether learned action is the same as true action would be '''\n",
    "        if a_argmax_with_noise != policy(pos_curr,speed_curr):\n",
    "            errors+=1\n",
    "        else:\n",
    "            corrects+=1\n",
    "\n",
    "        pos_curr,speed_curr = env.step(a_argmax_with_noise)[0]\n",
    "\n",
    "        if pos_curr>0.5:\n",
    "            steps[s].append(k)\n",
    "            break\n",
    "    if k==199:\n",
    "        failures[s]+=1\n",
    "    progressBar(200,200)\n",
    "    print('\\n')\n",
    "\n",
    "print('Errorrate = ',(errors/(errors+corrects))) # Only makes sense if following learned policy\n",
    "s+=1\n",
    "print('pCN done, '+ str(time.ctime()))\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "\"\"\" load samples - pCNL \"\"\"\n",
    "\n",
    "method = 'pCNL'\n",
    "N_data = 50\n",
    "N_trunc = 7\n",
    "\n",
    "xi = []\n",
    "for it in range(1000):\n",
    "    xi.append(np.load('np_saved/MC/samples_policy_learning/KL_'+str(N_trunc)+'_'+method+'_NData'+str(N_data)+'_sampleNo'+str(it)+'.npy'))\n",
    "      \n",
    "\n",
    "\"\"\" run tests - pCNL \"\"\"\n",
    "\n",
    "v = np.zeros(N_a)\n",
    "v_with_noise = np.zeros(N_a)\n",
    "\n",
    "errors = 0\n",
    "corrects = 0\n",
    "\n",
    "print('\\n\\nTest started: ' + str(time.ctime()))\n",
    "for _ in range(100):\n",
    "    pos_curr,speed_curr = env.reset()\n",
    "\n",
    "    for k in range(200):\n",
    "        if (k+1)%50==0:\n",
    "            progressBar(k+1,200)\n",
    "\n",
    "        ''' See where different actions would take us, evaluate mean of value function samples there '''\n",
    "        v = np.zeros(N_a)\n",
    "        for it in range(1000): # Iterate through different neural networks\n",
    "            for i in range(N_a):\n",
    "                x = env.step(i)[0]\n",
    "                v[i] += u(xi[it],x)\n",
    "                env.state = [pos_curr,speed_curr]\n",
    "        v = v/1000 # to get mean of value function evaluations\n",
    "\n",
    "        ''' Pick action which maximises the mean value function (plus noise) at the new location '''\n",
    "        v_with_noise = v+np.random.normal(np.zeros(N_a),sigma*np.ones(N_a)) \n",
    "        a_argmax_with_noise = np.argmax(v_with_noise)\n",
    "\n",
    "        ''' Check whether learned action is the same as true action would be '''\n",
    "        if a_argmax_with_noise != policy(pos_curr,speed_curr):\n",
    "            errors+=1\n",
    "        else:\n",
    "            corrects+=1\n",
    "\n",
    "        pos_curr,speed_curr = env.step(a_argmax_with_noise)[0]\n",
    "\n",
    "        if pos_curr>0.5:\n",
    "            steps[s].append(k)\n",
    "            break\n",
    "    if k==199:\n",
    "        failures[s]+=1\n",
    "    progressBar(200,200)\n",
    "    print('\\n')\n",
    "\n",
    "print('Errorrate = ',(errors/(errors+corrects))) # Only makes sense if following learned policy\n",
    "s+=1\n",
    "print('pCNL done, '+ str(time.ctime()))\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1, ax1 = plt.subplots()\n",
    "ax1.set_title('Runtimes for different policies')\n",
    "ax1.boxplot(steps)\n",
    "plt.show()\n",
    "\n",
    "print(failures)\n",
    "\n",
    "np.save('KL_steps.npy',np.array(steps))\n",
    "np.save('KL_failures.npy',failures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
